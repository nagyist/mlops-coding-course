{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"MLOps Coding Course","text":"<p> Learn how to create, develop, and maintain a state-of-the-art MLOps code base. </p> <p>Welcome to our comprehensive MLOps Coding Course, designed to integrate robust software development practices with cutting-edge data science techniques. This course is tailored for both beginners who are just starting their journey and experienced professionals looking to enhance their skills in managing and executing AI and machine learning projects using Python. Here, you will learn through a hands-on approach that emphasizes real-world applications and efficient project execution.</p> <ul> <li>Donation Link: https://donate.stripe.com/4gw8xT9oVbCc98s7ss</li> <li>GitHub Repository: https://github.com/MLOps-Courses/mlops-coding-course</li> <li>MLOps Coding Assistant: https://mlops-coding-assistant.fmind.dev/</li> </ul>"},{"location":"index.html#chapter-0-overview","title":"Chapter 0: Overview","text":"<p>In the introductory chapter, we introduce the foundational concepts of MLOps and outline the structure and goals of the course. This chapter sets the stage for what you will learn and provides an overview of the key components and skills you will develop as you progress through the course.</p>"},{"location":"index.html#chapter-1-initializing","title":"Chapter 1: Initializing","text":"<p>The first chapter focuses on the initialization phase, which is crucial for setting up a robust development environment. Here, we guide you through configuring tools and environments essential for Python-based MLOps projects. This foundational setup ensures a smooth workflow and minimizes delays related to technical setups.</p>"},{"location":"index.html#chapter-2-prototyping","title":"Chapter 2: Prototyping","text":"<p>This chapter dives into the prototyping phase, a critical stage where various approaches are tested to identify the most effective solutions, usually with notebooks. We cover essential tools and practices that improve the efficiency of this process, helping you understand the problem and experiment with different models before finalizing the project architecture.</p>"},{"location":"index.html#chapter-3-productionizing","title":"Chapter 3: Productionizing","text":"<p>Learn to refine your Python codebase for better maintainability, scalability, and efficiency in Chapter 3. This includes transitioning from notebooks to structured Python packages, understanding different programming paradigms, and optimizing your development environment. These practices are crucial for enhancing code quality and collaboration.</p>"},{"location":"index.html#chapter-4-validating","title":"Chapter 4: Validating","text":"<p>Chapter 4 emphasizes the importance of code validation in the MLOps landscape. We explore key practices such as typing and debugging that ensure the robustness of ML pipelines, facilitate collaboration, and streamline deployment processes, all of which are essential for creating scalable and efficient systems.</p>"},{"location":"index.html#chapter-5-refining","title":"Chapter 5: Refining","text":"<p>In this chapter, we delve into refining MLOps projects to enhance their efficiency, reliability, and scalability. We discuss methodologies that streamline the development pipeline from code formulation to deployment, focusing on maintaining high code quality and automating repetitive tasks.</p>"},{"location":"index.html#chapter-6-sharing","title":"Chapter 6: Sharing","text":"<p>The chapter focuses on sharing and distributing MLOps projects. We explore tools and practices that enhance collaboration, promote reuse, and facilitate the scaling of machine learning solutions. You will learn how to effectively organize, document, and disseminate your projects to make them more accessible and beneficial to others.</p>"},{"location":"index.html#chapter-7-observability","title":"Chapter 7: Observability","text":"<p>This chapter dives into the essential aspects of observability in MLOps, equipping you with the knowledge and strategies to gain comprehensive insights into the performance, behavior, and health of your deployed models and infrastructure. You'll learn how to ensure reproducibility, implement monitoring and alerting systems, track data and model lineage, manage costs and KPIs, understand model explainability, and monitor infrastructure performance.</p>"},{"location":"index.html#lets-journey-together","title":"Let's journey together!","text":"<p>We are excited to have you join us on this journey to mastering MLOps. By the end of this course, you will be well-equipped to manage and execute ML projects with a high degree of professionalism and skill. Let\u2019s get started on transforming your data science capabilities with effective MLOps practices!</p>"},{"location":"0.%20Overview/index.html","title":"0. Overview","text":"<p>Welcome to the introductory chapter of our course, where we delve into the integration of software development practices with the dynamic field of data science. This course is designed to empower you with the knowledge and skills necessary to manage and execute artificial intelligence (AI) and machine learning (ML) projects effectively using advanced Python techniques. Whether you're a beginner looking to get started or an experienced professional aiming to enhance your capabilities, this course has something to offer.</p> <p>In this overview, we'll guide you through the various components that make up this comprehensive learning experience:</p> <ul> <li>0.0. Course: Unveils the course's mission to integrate software development disciplines with data science. Our objective is to equip learners with the confidence to embark on AI/ML projects using sophisticated Python methodologies.</li> <li>0.1. Projects: Provides insight into the default project included within the course, while encouraging learners to incorporate their projects for a tailored learning experience.</li> <li>0.2. Datasets: Offers a comprehensive look at various types of datasets, their importance throughout the AI/ML project lifecycle, and guidance on selecting the most suitable datasets.</li> <li>0.3. Platforms: Discusses how to choose an MLOps platform that best fits organizational needs, highlighting the course's agnostic approach to specific platforms.</li> <li>0.4. Mentoring: Details the mentoring services provided by the course creators, emphasizing the benefits of personalized advice and support.</li> <li>0.5. Assistants: Introduces the specialized online assistant tailored for this course, including tips on leveraging it effectively.</li> <li>0.6. Resources: Clarifies the extra resources available to enhance the course content and explains how participants can contribute to the course's open-source materials.</li> </ul>"},{"location":"0.%20Overview/0.0.%20Course.html","title":"0.0. Course","text":""},{"location":"0.%20Overview/0.0.%20Course.html#what-will-this-course-teach-you","title":"What will this course teach you?","text":"<p>Welcome to the MLOps Coding Course designed to bring your AI/ML programming level from basic notebooks to production-grade codebase. Throughout this journey, you will learn:</p> <ul> <li>How to build and deploy production ready software artifacts.</li> <li>Transitioning from prototyping in notebooks to structured Python packages.</li> <li>Enhancing code reliability and maintainability through linting and testing.</li> <li>Streamlining repetitive tasks using automation, locally and through CI/CD pipelines.</li> <li>Adopting best practices to develop versatile and resilient AI/ML codebases.</li> </ul>"},{"location":"0.%20Overview/0.0.%20Course.html#why-enroll-in-this-course","title":"Why enroll in this course?","text":"<p>The intersection of AI and ML with software applications is becoming increasingly complex, requiring management of models, datasets, and code. This course aims to bridge the knowledge gap between software engineers and data scientists, empowering you to efficiently navigate and manage AI/ML projects.</p> <p>A key focus is the shift from using notebooks for production, which often lack rigorous software development practices, to a structured codebase. This transition is crucial for tackling production challenges, encouraging better collaboration, and advancing your MLOps capabilities.</p>"},{"location":"0.%20Overview/0.0.%20Course.html#is-there-a-fee-for-this-course","title":"Is there a fee for this course?","text":"<p>We offer this course at no cost, under the Creative Commons Attribution 4.0 International license. This means you can adapt, share, and even use the content for commercial purposes, provided you attribute the original authors.</p> <p>Additionally, for those seeking a deeper understanding, we provide extra support options, including personal mentoring sessions and access to online assistance.</p>"},{"location":"0.%20Overview/0.0.%20Course.html#what-should-you-know-before-starting","title":"What should you know before starting?","text":"<p>To get the most out of this course, you should have:</p> <ol> <li>A good understanding of Python including loops, conditionals, functions, and classes.</li> <li>Familiarity with terminal commands for software installation, following README guides, and launching applications.</li> <li>Basic knowledge in data science, including data exploration, feature engineering, model training and tuning, and performance evaluation.</li> </ol>"},{"location":"0.%20Overview/0.0.%20Course.html#what-skills-will-you-acquire","title":"What skills will you acquire?","text":"<p>The course is divided into six in-depth chapters, each focusing on different facets of coding and project management skills:</p> <ol> <li>Initializing: Go through the necessary tools and platforms for your development environment.</li> <li>Prototyping: Start with notebooks to dive into data science projects and pinpoint viable solutions.</li> <li>Productionizing: Transform your prototype into a neatly organized Python package, complete with scripts, configurations, and documentation.</li> <li>Validating: Adopt practices like typing, linting, testing, and logging to refine code quality.</li> <li>Refining: Leverage advanced software development techniques and tools to polish your project.</li> <li>Sharing: Foster a productive team environment for effective contributions and communication.</li> <li>Observability: Implement tools and practices for monitoring your data, models, and infrastructure.</li> </ol>"},{"location":"0.%20Overview/0.0.%20Course.html#whats-beyond-the-scope-of-this-course","title":"What's beyond the scope of this course?","text":"<p>While this course provides a solid basis in managing AI/ML code bases, it does not enter into the specificity of the different MLOps platforms like SageMaker, Vertex AI, Azure ML, or Databricks. Vendor courses already cover these end-to-end platforms in length. Instead, this course focuses on core principles and practices that are universally applicable, whether you're working on-premise, cloud-based, or in a hybrid setting.</p>"},{"location":"0.%20Overview/0.0.%20Course.html#how-much-time-do-you-need-to-complete-this-course","title":"How much time do you need to complete this course?","text":"<p>The time required to complete this course varies based on your prior experience and familiarity with the covered tools and practices. If you're already comfortable with tools like Git or VS Code, you may progress faster. The course philosophy encourages incremental improvement following the \"make it done, make it right, make it fast\" mantra, encouraging you to begin with a functional project version and steadily refine it for better quality and efficiency.</p>"},{"location":"0.%20Overview/0.1.%20Projects.html","title":"0.1. Projects","text":""},{"location":"0.%20Overview/0.1.%20Projects.html#what-is-the-default-learning-project","title":"What is the default learning project?","text":"<p>The default project of this course involves a forecasting task using the Bike Sharing Demand dataset. The objective is to predict the number of bike rentals based on variables like date and time, weather conditions, and past rental data. A reference implementation is provided to fallback on if needed.</p> <p>Forecasting is a critical skill with wide-ranging applications in academia and industry, utilizing diverse machine learning techniques. This project introduces challenges such as managing data subsets to prevent data leakage, where future information could wrongly influence past predictions. Through tackling this project, you'll gain hands-on experience in structuring MLOps projects effectively, offering a solid foundation for your learning journey.</p>"},{"location":"0.%20Overview/0.1.%20Projects.html#is-it-possible-to-select-a-personal-project-instead","title":"Is it possible to select a personal project instead?","text":"<p>Absolutely! We encourage you to dive into a project that resonates with you personally. This could be a project you're currently working on professionally, or a passion project you're eager to develop further. Opting for your own project allows you to apply improvements directly within a familiar domain, streamlining the learning process by removing the need to acquaint yourself with a new project's nuances.</p>"},{"location":"0.%20Overview/0.1.%20Projects.html#how-to-find-project-ideas","title":"How to find project ideas?","text":"<p>Looking for inspiration? There are several online platforms offering data science challenges, complete with datasets and clearly defined objectives:</p> <ul> <li>Kaggle: A hub for data scientists worldwide, Kaggle provides the tools and community support needed to pursue your data science aspirations.</li> <li>DrivenData: Hosts competitions where data scientists can address significant societal challenges through innovative predictive modeling.</li> <li>DataCamp: Offers real-world data science competitions, allowing participants to hone their skills, win accolades, and present their solutions.</li> </ul>"},{"location":"0.%20Overview/0.1.%20Projects.html#can-you-work-on-a-large-language-model-llm-project","title":"Can you work on a Large Language Model (LLM) project?","text":"<p>Working on projects centered around Large Language Models (LLM) and Generative AI does hold similarities with predictive ML projects, particularly in the areas of model management and code structuring. However, LLM projects also present distinct challenges. Evaluating LLMs can be more intricate, sometimes necessitating the use of external LLMs for thorough testing. Additionally, the training and fine-tuning of LLMs typically demand specific hardware, like high-memory GPUs, and adhere to different methodologies compared to conventional ML tasks.</p> <p>Therefore, we recommend starting with a predictive ML project to get acquainted with fundamental MLOps practices. These core skills will then be easier to adapt and apply to LLM projects, easing the progression to these more specialized areas.</p>"},{"location":"0.%20Overview/0.2.%20Datasets.html","title":"0.2. Datasets","text":""},{"location":"0.%20Overview/0.2.%20Datasets.html#what-is-a-dataset","title":"What is a dataset?","text":"<p>A dataset is an organized collection of data, serving as the cornerstone for any AI or Machine Learning (ML) initiative. The structure of a dataset might vary, yet its pivotal role in shaping the scope, capabilities, and challenges of a project remains undisputed. Data preparation, involving substantial cleaning and exploration of raw data, often consumes the lion's share of a Machine Learning engineer's efforts, reinforcing to the adage that 80% of the work pertains to data processing, leaving only 20% for modeling. Yet, this preparation phase is crucial, setting the stage for the subsequent modeling efforts.</p> <p>The impact of a dataset's quality and size on the outcomes of a model is profound, frequently surpassing the effects of model adjustments. This impact is embedded into the \"Garbage in, garbage out\" concept related to data science projects.</p>"},{"location":"0.%20Overview/0.2.%20Datasets.html#when-is-the-dataset-used","title":"When is the dataset used?","text":"<p>Datasets are integral throughout the AI/ML project lifecycle, playing pivotal roles in various stages:</p> <ul> <li>Exploration: This phase involves delving into the dataset to find insights, study variable relationships, and discern patterns that could influence future predictions.</li> <li>Data Processing: At this stage, the focus is on crafting features that encapsulate the predictive essence of the data and on partitioning the dataset effectively to gear up for the modeling phase.</li> <li>Model Tuning: Here, the objective is to refine the model's hyperparameters through strategies like cross-validation to bolster the model's generalization capability.</li> <li>Model Evaluation: This final step entails evaluating the model's performance on unseen data and identifying areas for potential improvement.</li> </ul>"},{"location":"0.%20Overview/0.2.%20Datasets.html#what-are-the-types-of-datasets","title":"What are the types of datasets?","text":"<p>Datasets are typically classified into three categories: structured, unstructured, and semi-structured, each with distinctive features:</p>"},{"location":"0.%20Overview/0.2.%20Datasets.html#structured-data","title":"Structured Data","text":"<p>This category includes data that adhere to a clear schema, simplifying organization and analysis.</p> <ul> <li>Tabular Data: Characterized by its rows-and-columns format, where each column holds data of a uniform type. This format is prevalent in CSV files and databases.</li> <li>Time Series Data: Comprises sequential data points collected at consistent time intervals. The sequential order is crucial, as alterations can significantly impact the dataset's interpretation, making it vital for forecasting in sectors like finance and energy.</li> <li>Geospatial Data: Relates to specific geographical locations or areas, critical for spatial analyses and often managed with Geographic Information Systems (GIS).</li> <li>Graph Data: Consists of nodes (or vertices) and edges (or connections), representing entities and their interrelations. This data type is crucial for modeling complex networks and systems.</li> </ul>"},{"location":"0.%20Overview/0.2.%20Datasets.html#unstructured-data","title":"Unstructured Data","text":"<p>Unstructured data lacks a predefined format, which poses challenges in its processing and analysis.</p> <ul> <li>Text: Ranges from short messages to extensive documents, marked by the complexity and diversity of language.</li> <li>Multimedia: Includes images, audio, and video files, known for their high dimensionality and the complexities involved in extracting meaningful insights.</li> </ul>"},{"location":"0.%20Overview/0.2.%20Datasets.html#semi-structured-data","title":"Semi-Structured Data","text":"<p>Positioned between structured and unstructured data, semi-structured data does not follow a strict schema but includes markers or tags to aid in identifying specific elements, making it more manageable. JSON and XML are notable examples.</p>"},{"location":"0.%20Overview/0.2.%20Datasets.html#which-dataset-should-you-use","title":"Which dataset should you use?","text":"<p>The decision on which dataset to use often boils down to a balance between familiarity and exploration of new data. A simple rule of thumb is to opt for the dataset you are most acquainted with. Regardless of the diversity in data types and applications, the core principles of MLOps are applicable across various domains. Therefore, starting with a well-understood dataset allows you to concentrate on honing your MLOps skills rather than untangling the complexities of an unfamiliar dataset.</p> <p>As mentioned in the previous section, the course uses the Bike Sharing Demand dataset dataset by default. You are free to use any other datasets, either for personal or professional purposes.</p>"},{"location":"0.%20Overview/0.3.%20Platforms.html","title":"0.3. Platforms","text":""},{"location":"0.%20Overview/0.3.%20Platforms.html#what-is-an-mlops-platform","title":"What is an MLOps platform?","text":"<p>An MLOps platform is a comprehensive toolkit designed to facilitate the deployment, management, and operational efficiency of AI and Machine Learning (ML) projects in production environments. Essential components of an MLOps platform typically can encompass:</p> CI/CD and automated ML pipeline (source). <ul> <li>Storage Systems: These are solutions like Amazon S3 or Google Cloud Storage that provide a secure space for storing datasets, models, and other essential artifacts.</li> <li>Compute Engines: Services such as Kubernetes or Databricks deliver the necessary computational power for training models and executing predictions.</li> <li>Orchestrators: Automation tools, including Apache Airflow, Metaflow, or Prefect, that streamline and manage workflows and data pipelines efficiently.</li> <li>Model Registries: Platforms such as MLflow, Neptune.ai, or Weights and Biases that offer functionalities for tracking, versioning, and managing models.</li> </ul> <p>The complexity and scale of an MLOps platform can greatly differ based on an organization's specific needs and the requirements of individual projects. While smaller teams might lean towards open-source options like MLflow for model lifecycle management and Airflow for orchestration due to their cost-effectiveness and versatility, larger enterprises may prefer comprehensive, fully-managed solutions such as Databricks or AWS SageMaker to accommodate extensive AI/ML deployments.</p>"},{"location":"0.%20Overview/0.3.%20Platforms.html#which-mlops-platform-is-the-best","title":"Which MLOps platform is the best?","text":"<p>Determining the \"best\" MLOps platform hinges on the unique requirements, infrastructure, and technical expertise of your organization. The ideal choice is one that seamlessly integrates with your existing technology stack and optimally supports your AI/ML workflows. Consider the following steps to guide your selection:</p> <ol> <li>Stakeholder Engagement: Engage with key stakeholders across data science, IT operations, and software architecture to define project requirements and objectives.</li> <li>Goal Alignment: Specify your objectives and the degree of platform sophistication needed to fulfill them within your project timelines.</li> <li>Pilot Testing: Conduct pilot projects to evaluate the platform\u2019s alignment with your business needs and its capacity to satisfy user expectations.</li> </ol> <p>The decision is often influenced by various factors, such as the organization's familiarity with certain technologies (like Kubernetes), budgetary limitations, and the preference for flexibility versus fully-managed services.</p>"},{"location":"0.%20Overview/0.3.%20Platforms.html#why-is-this-course-not-tied-to-a-specific-mlops-platform","title":"Why is this course not tied to a specific MLOps platform?","text":"<p>Market offerings frequently highlight the ease and simplicity of their MLOps platforms, sometimes at the expense of acknowledging the complexities of crafting a robust AI/ML codebase grounded in software engineering best practices. Although each platform brings its unique advantages and experiences, the foundational skills in MLOps coding are universally applicable, cutting across the specificities of individual platforms.</p>"},{"location":"0.%20Overview/0.3.%20Platforms.html#is-an-mlops-platform-required-for-this-course","title":"Is an MLOps platform required for this course?","text":"<p>Intentionally designed to be platform-agnostic, this course empowers you to apply its principles within any technological ecosystem you choose. Whether you're working with specific environment management systems, leveraging various libraries, or adapting to the workflow methodologies of tools like GitLab or Azure DevOps, the course material is versatile and can be tailored to align with your organization's preferences.</p>"},{"location":"0.%20Overview/0.3.%20Platforms.html#how-does-this-course-prepare-you-for-using-an-mlops-platform","title":"How does this course prepare you for using an MLOps platform?","text":"<p>Given the variety of artifacts MLOps platforms support, from Jupyter notebooks to Python packages, adopting Python packages is advocated for its robustness and maintainability. This course provides you with the essential skills to seamlessly integrate such packages within the Python ecosystem. It covers the use of testing tools like pytest and coverage, and package management via repositories like PyPI or Docker Hub. Armed with this knowledge, you can confidently tackle other pivotal aspects of your projects, such as data and model management and orchestration, knowing your codebase is solid and dependable.</p>"},{"location":"0.%20Overview/0.3.%20Platforms.html#platform-additional-resources","title":"Platform additional resources","text":"<ul> <li>The ultimate list of internal ML platforms</li> </ul>"},{"location":"0.%20Overview/0.4.%20Mentoring.html","title":"0.4. Mentoring","text":""},{"location":"0.%20Overview/0.4.%20Mentoring.html#can-you-receive-mentoring-during-this-course","title":"Can you receive mentoring during this course?","text":"<p>Mentoring forms a fundamental aspect of this course, provided directly by its authors, M\u00e9d\u00e9ric HURIER and Matthieu Jimenez. These mentors come equipped with extensive experience in the educational field. The mentoring program is flexible, catering to both individual learners and groups. You can tailor the frequency of mentorship sessions to match your preferences, from weekly engagements to several sessions per month.</p>"},{"location":"0.%20Overview/0.4.%20Mentoring.html#what-are-the-benefits-of-having-a-mentor","title":"What are the benefits of having a mentor?","text":"<p>The inclusion of mentorship dramatically enhances the educational journey, contributing to over half of the learning efficacy as observed. Mentorship deepens your engagement with the course materials, encourages critical examination of the methodologies presented, facilitates a more thorough exploration of the concepts, and tailors the learning path to meet your specific needs and context.</p> <p>Mentors also play a pivotal role in motivation. The discipline and persistence required to navigate through the course, especially its more demanding segments, can be daunting. Having a mentor provides the support and encouragement necessary to navigate these challenges more smoothly.</p>"},{"location":"0.%20Overview/0.4.%20Mentoring.html#how-much-does-mentoring-cost","title":"How much does mentoring cost?","text":"<p>The pricing for mentorship varies, depending on the size of the participant group and the duration of the sessions. To obtain a personalized quote, it\u2019s best to contact the course creators directly. If you are an individual learner, you can also opt for one-on-one mentoring sessions, which are priced differently.</p>"},{"location":"0.%20Overview/0.4.%20Mentoring.html#is-company-training-available","title":"Is company training available?","text":"<p>Company-specific training sessions are offered, designed to meet the unique needs of your organization. For a tailored training proposal, please reach out through the course\u2019s main contact channel.</p>"},{"location":"0.%20Overview/0.4.%20Mentoring.html#can-you-offer-mentoring-services-using-this-course","title":"Can you offer mentoring services using this course?","text":"<p>You are more than welcome to leverage this course as a foundation for offering your own mentoring services. The course is designed with open-source principles in mind, aiming for broad accessibility.</p> <p>However, if you opt to use this course material in your mentoring services, it\u2019s imperative to properly acknowledge M\u00e9d\u00e9ric HURIER and Matthieu Jimenez as the original authors. Such attribution not only honors the intellectual property rights of the creators but also preserves the educational integrity of the course.</p>"},{"location":"0.%20Overview/0.5.%20Assistants.html","title":"0.5. Assistants","text":""},{"location":"0.%20Overview/0.5.%20Assistants.html#what-is-the-course-assistant","title":"What is the course assistant?","text":"<p>Included in this course is a premium MLOps Coding Assistant, specifically configured to support your learning journey by answering queries and tailoring the course content to your needs. This assistant functions similarly to ChatGPT but is uniquely customized to engage with the topics and materials of this course.</p>"},{"location":"0.%20Overview/0.5.%20Assistants.html#how-does-this-assistant-work","title":"How does this assistant work?","text":"<p>The assistant utilizes a Large Language Model (LLM) in a Retrieval-Augmented Generation (RAG) architecture to offer real-time support and customized feedback. Upon receiving a query or request for assistance, it applies sophisticated natural language understanding to interpret your input accurately. It then consults its comprehensive database to fetch relevant information or examples, aiming to deliver the most precise and useful responses.</p> <p>This tool is consistently updated to reflect the latest developments in AI and machine learning, ensuring it remains an effective resource throughout your MLOps educational journey. Whether you need further explanation on a topic, examples of MLOps applications, or feedback on your code, this assistant is prepared to assist.</p>"},{"location":"0.%20Overview/0.5.%20Assistants.html#how-much-does-the-assistant-cost","title":"How much does the assistant cost?","text":"<p>The assistant is a complimentary feature that costs $5 per month. You can access it by contacting the course creators. This fee grants you access to the assistant for one month, enabling you to leverage its support whenever you require assistance or guidance during the course.</p>"},{"location":"0.%20Overview/0.5.%20Assistants.html#how-should-you-use-the-assistant","title":"How should you use the assistant?","text":"<p>The assistant is available for a wide range of course-related inquiries. Consider using it in the following ways:</p> <ul> <li>Demonstrating Examples: Request detailed examples, such as a pipeline example for PyTorch, to deepen your comprehension of key concepts.</li> <li>Feature Creation Guidance: Seek advice on developing features for your dataset, utilizing the assistant's insights to refine your project.</li> <li>Understanding Best Practices: Query about the benefits of implementing unit tests or other MLOps best practices to enhance your learning.</li> </ul> <p>The assistant can also perform code reviews, providing critiques and recommendations to elevate the caliber and effectiveness of your work.</p>"},{"location":"0.%20Overview/0.5.%20Assistants.html#can-you-fully-trust-the-assistant","title":"Can you fully trust the assistant?","text":"<p>While the assistant aims to offer accurate and relevant responses, it's essential to critically evaluate its outputs. It is estimated that about 90% of the information it generates will be accurate, but there is still a margin for potential errors or contextually unsuitable suggestions due to the limitations of generative AI technologies.</p> <p>For those looking for a more dependable source of information, or to complement AI's convenience with expert human insight, the human mentoring service is an excellent alternative. This option extends the assistant's support by providing personalized feedback and expert insights from seasoned professionals in the field.</p>"},{"location":"0.%20Overview/0.6.%20Resources.html","title":"0.6. Resources","text":""},{"location":"0.%20Overview/0.6.%20Resources.html#is-there-additional-project-resources","title":"Is there additional project resources?","text":"<p>This course is supplemented with a variety of resources aimed at enriching your MLOps learning journey. A key resource is the MLOps Python Package, designed to exemplify how to structure an MLOps codebase efficiently. This package incorporates the dataset featured in the course, providing a holistic view of what your end project could resemble.</p> <p>Another important resource is the Cookiecutter MLOps Package which generalizes the concepts provided in the MLOps Python Package and this course, allowing you to quickly create a new MLOps project with the same structure.</p> <p>For those seeking to deepen their knowledge in specific areas, the course creators have also contributed insights through personal blog posts. These articles explore subjects like setting up Visual Studio Code for MLOps activities or employing Pydantic for robust data validation. Such resources offer additional insights and actionable advice to enhance your understanding and skills.</p>"},{"location":"0.%20Overview/0.6.%20Resources.html#can-you-suggest-a-new-project-resource","title":"Can you suggest a new project resource?","text":"<p>The course adheres to an open-source ethos, warmly welcoming contributions that augment the educational value for all participants. Whether you've discovered an essential tool, library, or piece of literature that aligns with the course's aims, or you've developed your resources inspired by your coursework and are keen to share them, your contributions are greatly valued.</p> <p>To propose a new project resource, please forward your suggestions to the course's repository and create an issue on GitHub. Your contributions not only aid your peers in their learning process but also play a crucial role in the continuous improvement and updating of the course materials to mirror the evolving landscape of MLOps best practices and innovations.</p>"},{"location":"0.%20Overview/0.6.%20Resources.html#additional-vendor-resources","title":"Additional vendor resources","text":"<ul> <li>MLOps Python Package</li> <li>LLMOps Python Package</li> <li>Cookiecutter MLOps Package</li> <li>MLOps: Continuous delivery and automation pipelines in machine learning</li> <li>The Big Book of MLOps</li> <li>Practitioners guide to MLOps: A framework for continuous delivery and automation of machine learning</li> <li>AWS Machine Learning Lens</li> </ul>"},{"location":"1.%20Initializing/index.html","title":"1. Initializing","text":"<p>The initialization phase is crucial in setting the stage for efficient and streamlined development, particularly for projects centered around Python and MLOps. This chapter aims to guide you through establishing a robust development setup, ensuring that every necessary tool and environment is correctly configured from the get-go. By following these foundational steps, you'll create a solid base for your project, enabling smooth progress and reducing the likelihood of delays caused by environment-related issues.</p> <ul> <li>1.0. System: This section ensures your system is adequately prepared, outlining the essential prerequisites for installing and effectively running the necessary development tools.</li> <li>1.1. Python: Here, we introduce how to set up Python\u2014the core programming language for our projects. We'll focus on version management and creating isolated environments for each project to avoid conflicts and dependency issues.</li> <li>1.2. uv: We explore <code>uv</code>, an extremely fast Python tool written in Rust. Uv can install Python versions, manage virtual environments, and handle dependencies, making it a versatile tool for MLOps projects.</li> <li>1.3. uv (project): This part delves into using <code>uv</code> for project packaging. It simplifies the process of defining, installing, and updating project metadata and dependencies with ease.</li> <li>1.4. git: Focuses on <code>git</code>, the cornerstone version control system integral to GitHub. You'll learn how to initiate and manage repositories effectively, a critical skill for collaborative development.</li> <li>1.5. GitHub: Discusses how to leverage GitHub for project hosting, version control, and collaboration. It's a pivotal component in modern development workflows, facilitating teamwork and project management.</li> <li>1.6. VS Code: Highlights the setup of Visual Studio Code (VS Code), showing how to adapt this versatile editor into an integrated development environment (IDE) customized for Python and MLOps projects.</li> <li>1.7. pyenv (ARCHIVE): Explores <code>pyenv</code>, a Python version management tool that simplifies handling multiple Python versions on a single machine. It ensures each project uses the correct Python version, preventing conflicts and compatibility issues.</li> <li>1.8. Poetry (ARCHIVE): Introduces <code>Poetry</code>, a Python dependency management tool that simplifies package installation and project configuration. You'll learn how to create, manage, and publish Python packages using Poetry.</li> </ul>"},{"location":"1.%20Initializing/1.0.%20System.html","title":"1.0. System","text":""},{"location":"1.%20Initializing/1.0.%20System.html#what-system-is-recommended-for-this-course","title":"What system is recommended for this course?","text":"<p>This course is tailored to be accessible on a wide range of operating systems including Linux, Chromebook, macOS, and Windows. Although there are no stringent hardware prerequisites, having a computer with enough CPU and RAM specifications is crucial for processing datasets efficiently. This ensures that you can fully engage with the course activities, regardless of your system preferences or the devices you have at your disposal.</p>"},{"location":"1.%20Initializing/1.0.%20System.html#can-you-use-jupyterlab-or-google-colab-for-this-course","title":"Can you use JupyterLab or Google Colab for this course?","text":"<p>While JupyterLab is an acceptable environment for this course, it's worth noting that we've optimized the course content for Visual Studio Code (VS Code). VS Code offers a broader set of features that are specifically designed to enhance your learning experience in this course. Although Jupyter notebooks and Google Colab can be suitable for initial stages, like the Prototyping chapter, you may find that later sections of the course require functionalities, such as terminal access and file system navigation, that are more efficiently executed in VS Code.</p>"},{"location":"1.%20Initializing/1.0.%20System.html#are-additional-software-installations-required","title":"Are additional software installations required?","text":"<p>Engaging with this course material necessitates installing several key software packages, including Python, uv, git, and VS Code. These tools form the backbone of your development workflow:</p> <ul> <li>Python is indispensable for all course-related coding activities.</li> <li>uv offers an efficient way to manage Python package dependencies.</li> <li>Git is crucial for version control and collaboration.</li> <li>VS Code is recommended for its integrated development environment (IDE) capabilities, although alternatives may be used based on personal preference or specific needs.</li> </ul> <p>Detailed instructions for installing these software packages are provided in their respective course chapters.</p>"},{"location":"1.%20Initializing/1.0.%20System.html#what-are-the-specific-hardware-requirements-for-mlops-projects","title":"What are the specific hardware requirements for MLOps projects?","text":"<p>MLOps projects vary significantly in their complexity and demands on hardware, from simple tabular data analyses to complex machine learning models like transformers:</p> <ul> <li>Tabular Data Projects: Projects utilizing libraries like scikit-learn or XGBoost typically don't require specialized hardware, though an optional GPU could enhance performance for certain tasks.</li> <li>Multimedia Data Projects: For projects involving TensorFlow or PyTorch for processing images or video data, access to at least one GPU is beneficial for faster processing.</li> <li>Large Dataset Projects: Advanced projects that employ transformers or require extensive parallel processing may need multiple GPUs, possibly distributed across several machines for optimal performance.</li> </ul> <p>It's often best to start with a straightforward setup, such as developing models on a local machine with sample data, before scaling to more complex arrangements like cloud-based resources for deployment and broader testing. Cloud platforms also enable running multiple experiments simultaneously, which can expedite the development process.</p>"},{"location":"1.%20Initializing/1.0.%20System.html#is-it-possible-to-use-cloud-based-systems","title":"Is it possible to use cloud-based systems?","text":"<p>This course supports both local and cloud-based development environments, including options like GitHub Codespaces and Cloud Workstation. Cloud platforms offer considerable benefits, such as standardized development environments for easier team collaboration and enhanced security measures for your data. Nonetheless, it's crucial to understand any specific setup requirements and to manage resources effectively, especially when navigating the limitations of free tiers or usage quotas on these services.</p>"},{"location":"1.%20Initializing/1.0.%20System.html#system-additional-resources","title":"System additional resources","text":"<ul> <li>GitHub Codespaces</li> <li>Google Cloud Workstations</li> <li>MLOps Landscape in 2024: Top Tools and Platforms</li> </ul>"},{"location":"1.%20Initializing/1.1.%20Python.html","title":"1.1. Python","text":""},{"location":"1.%20Initializing/1.1.%20Python.html#what-is-python","title":"What is Python?","text":"<p>Python is a dynamic, high-level programming language known for its ease of learning and readability, making it a favorite among developers across various disciplines, including web development, automation, data science, and machine learning. It stands out for its simplicity and the vast ecosystem of third-party packages, allowing developers to build applications quickly and efficiently. Given its popularity, reflected in rankings such as the Tiobe Index and IEEE Spectrum Annual Ranking, Python is a staple in the programming world. For beginners and seasoned developers alike, Python offers a balance of readability and power, supported by a rich standard library and an extensive array of packages for diverse application needs.</p> Python(source)"},{"location":"1.%20Initializing/1.1.%20Python.html#why-is-python-preferred-for-aiml-projects","title":"Why is Python preferred for AI/ML projects?","text":"<p>Python's preeminence in AI and machine learning is attributed to its comprehensive selection of libraries and frameworks tailored for these fields, such as Pandas for data handling, Scikit-Learn for machine learning algorithms, and PyTorch and TensorFlow for advanced deep learning projects. Its user-friendly syntax supports rapid prototyping and iterative development, essential in the AI/ML workflow. Additionally, Python's interoperability with high-performance languages like C and C++ enables developers to optimize computational efficiency without sacrificing development speed or ease of use, making it the go-to language for AI/ML endeavors.</p>"},{"location":"1.%20Initializing/1.1.%20Python.html#is-python-a-good-language-for-mlops","title":"Is Python a good language for MLOps?","text":"<p>Python excels in the MLOps domain, offering a blend of simplicity for algorithm development and the robustness required for operational workflows. The key to maximizing Python's benefits in MLOps lies in adopting best practices for code quality and maintainability. This course covers strategies for effective Python code structuring and validation, ensuring that Python's flexibility and extensive toolkit can be leveraged effectively within MLOps pipelines.</p>"},{"location":"1.%20Initializing/1.1.%20Python.html#can-you-use-other-languages-for-aiml","title":"Can you use other languages for AI/ML?","text":"<p>While Python dominates the AI/ML landscape, other languages like R or Julia also provide capabilities for statistical analysis and machine learning. Each of these languages brings unique strengths, whether in performance, syntax, or domain specificity. Transitioning to or incorporating these languages in AI/ML projects is possible but requires careful consideration of their ecosystems and how they fit into the broader project goals.</p>"},{"location":"1.%20Initializing/1.1.%20Python.html#which-python-version-should-you-use","title":"Which Python version should you use?","text":"<p>For new projects, the latest Python version is recommended to take advantage of current features and improvements. However, the choice may be influenced by the compatibility needs of significant libraries or production environment constraints. It's vital to avoid unsupported Python versions to ensure your projects remain secure and efficient. Regularly checking the official Python website for version updates and support status is a good practice.</p>"},{"location":"1.%20Initializing/1.1.%20Python.html#how-to-install-python-for-this-course","title":"How to install Python for this course?","text":"<p><code>pyenv</code> is recommended for managing Python installations during development, offering the flexibility to switch between Python versions on a per-project basis. This approach is preferable to using system package managers or Anaconda, which may limit version flexibility. In production, using the system's default Python version can help avoid compatibility issues. For containerized deployments, you have the freedom to specify any Python version, aligning your development and production environments closely.</p>"},{"location":"1.%20Initializing/1.1.%20Python.html#python-additional-resources","title":"Python additional resources","text":"<ul> <li>Python Website</li> <li>Planet Python</li> <li>Learn Python</li> <li>Reddit Python</li> <li>Real Python Tutorials</li> <li>Learn Python in Y minutes</li> <li>Best of Python</li> <li>Best of Python ML</li> <li>Awesome Python</li> </ul>"},{"location":"1.%20Initializing/1.2.%20uv.html","title":"1.2. uv","text":""},{"location":"1.%20Initializing/1.2.%20uv.html#what-is-uv","title":"What is uv?","text":"<p>uv is an extremely fast Python package installer and resolver, written in Rust, designed to be a drop-in replacement for <code>pip</code>, <code>pipx</code>, <code>venv</code>, and <code>pyenv</code>. Created by Astral, the same team behind the high-performance linter Ruff, <code>uv</code> aims to significantly speed up and simplify Python project management. It offers a unified toolchain that handles virtual environments, dependency resolution, package installation, and more, all while providing exceptional performance.</p>"},{"location":"1.%20Initializing/1.2.%20uv.html#why-should-you-use-uv","title":"Why should you use uv?","text":"<p><code>uv</code> offers several compelling advantages for Python developers, especially in the context of MLOps:</p> <ul> <li>Performance: <code>uv</code> is incredibly fast, often outperforming <code>pip</code>, <code>venv</code>, and <code>pyenv</code> by a significant margin. This speed translates to faster project setup, quicker dependency resolution, and reduced wait times during development and deployment.</li> <li>Unified Toolchain: <code>uv</code> replaces multiple tools, simplifying your development workflow. It can manage virtual environments, install packages, and resolve dependencies, all within a single command-line interface.</li> <li>Drop-in Replacement: <code>uv</code> is designed to be a drop-in replacement for common Python tools. This means you can often substitute <code>uv</code> commands for <code>pip</code>, <code>venv</code>, or <code>pipx</code> commands without altering your existing workflows significantly.</li> <li>Active Development: Backed by Astral, <code>uv</code> is under active development with a focus on performance, reliability, and ease of use.</li> <li>Cross-Platform Compatibility: <code>uv</code> works seamlessly across Linux, macOS, and Windows, ensuring a consistent experience regardless of your operating system.</li> <li>Caching: <code>uv</code> implements aggressive caching mechanisms to avoid redundant work, further speeding up operations like package installation and environment setup.</li> </ul>"},{"location":"1.%20Initializing/1.2.%20uv.html#how-to-install-uv","title":"How to install uv?","text":"<p>Installing <code>uv</code> is straightforward. The recommended method is to use the official installation script, which automatically detects your operating system and installs the appropriate version:</p> <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre> <p>Alternatively, you can install <code>uv</code> using <code>pip</code> or <code>pipx</code>:</p> <pre><code>pip install uv\n# or\npipx install uv\n</code></pre> <p>Once installed, verify the installation by checking the version:</p> <pre><code>uv --version\n</code></pre>"},{"location":"1.%20Initializing/1.2.%20uv.html#how-to-use-uv-as-a-drop-in-replacement-for-pip","title":"How to use uv as a drop-in replacement for pip?","text":"<p><code>uv</code> can be used as a direct replacement for many common <code>pip</code> commands. Here's how:</p> <ul> <li>Installing packages:</li> </ul> <pre><code>uv pip install requests numpy pandas\n</code></pre> <ul> <li>Uninstalling packages:</li> </ul> <pre><code>uv pip uninstall requests\n</code></pre> <ul> <li>Listing installed packages:</li> </ul> <pre><code>uv pip freeze\n</code></pre> <ul> <li>Updating packages:</li> </ul> <pre><code>uv pip install --upgrade requests\n</code></pre> <ul> <li>Installing packages from a <code>requirements.txt</code> file:</li> </ul> <pre><code>uv pip install -r requirements.txt\n</code></pre>"},{"location":"1.%20Initializing/1.2.%20uv.html#how-to-use-uv-as-a-drop-in-replacement-for-venv","title":"How to use uv as a drop-in replacement for venv?","text":"<p><code>uv</code> can also replace <code>venv</code> for creating and managing virtual environments:</p> <ul> <li>Creating a virtual environment:</li> </ul> <pre><code>uv venv\n</code></pre> <p>This command creates a new virtual environment in the <code>.venv</code> directory by default. You can customize the location using the <code>--python</code> flag to specify a Python interpreter path.</p> <ul> <li>Activating the virtual environment:</li> </ul> <p>The activation process depends on your shell. For example, on bash:</p> <pre><code>source .venv/bin/activate\n</code></pre> <ul> <li>Listing available Python interpreters:</li> </ul> <pre><code>uv venv --python\n</code></pre>"},{"location":"1.%20Initializing/1.2.%20uv.html#how-to-use-uv-as-a-drop-in-replacement-for-pipx","title":"How to use uv as a drop-in replacement for pipx?","text":"<p><code>uv</code> can also replace <code>pipx</code> for installing and managing globally available Python tools:</p> <ul> <li>Installing a tool globally:</li> </ul> <pre><code>uv tool install ruff\n</code></pre> <ul> <li>Listing globally installed tools:</li> </ul> <pre><code>uv tool list\n</code></pre> <ul> <li>Running a tool without installing it:</li> </ul> <pre><code>uv tool run ruff --version\n</code></pre>"},{"location":"1.%20Initializing/1.2.%20uv.html#how-to-install-a-python-version-with-uv","title":"How to install a Python version with uv?","text":"<p><code>uv</code> can also be used to install specific Python versions, similar to <code>pyenv</code>. This is particularly useful when you need to test your code against different Python environments or when a project requires a specific Python version that is not your system's default.</p> <ul> <li>Installing a specific Python version:</li> </ul> <pre><code>uv python install 3.12\n</code></pre> <p>This command downloads and installs Python 3.12. You can then use this version to create virtual environments or run scripts.</p> <ul> <li>Listing available Python versions:</li> </ul> <pre><code>uv python list\n</code></pre> <ul> <li>Listing installed Python versions:</li> </ul> <pre><code>uv python list --only-installed\n</code></pre> <ul> <li>Removing a specific Python version:</li> </ul> <pre><code>uv python remove 3.12\n</code></pre>"},{"location":"1.%20Initializing/1.2.%20uv.html#uv-additional-resources","title":"uv additional resources","text":"<ul> <li>uv Documentation: The official documentation provides comprehensive information on all <code>uv</code> features and commands.</li> <li>Poetry Was Good, Uv Is Better: An MLOps Migration Story</li> <li>uv Installation</li> <li>uv Features</li> </ul>"},{"location":"1.%20Initializing/1.3.%20uv%20%28project%29.html","title":"1.3. uv (project)","text":""},{"location":"1.%20Initializing/1.3.%20uv%20%28project%29.html#what-is-a-package","title":"What is a package?","text":"<p>A Python package is a set of Python modules grouped together that can be installed and used within your projects. Python packages help you manage the functionality of Python by allowing you to add and utilize external libraries and frameworks that are not part of the standard Python library.</p> <p>uv simplifies the management of these packages by handling them as dependencies. When using uv, developers can easily specify which packages are needed for their projects through a <code>pyproject.toml</code> file. Uv ensures that all specified dependencies are installed in the correct versions, maintaining a stable and conflict-free environment for development. Here\u2019s an example of specifying dependencies with uv:</p> <pre><code># https://docs.astral.sh/uv/reference/pyproject-toml/\n\n[project]\nname = \"example-project\"\nversion = \"0.1.0\"\ndescription = \"An example project to demonstrate uv\"\ndependencies = [\n    \"requests&gt;=2.32.3\",\n]\n\n[project.optional-dependencies]\ndev = [\n    \"pytest&gt;=8.3.4\",\n]\n</code></pre> <p>You will learn more on how to construct and publish Python Package in the Package section of this course.</p>"},{"location":"1.%20Initializing/1.3.%20uv%20%28project%29.html#why-do-you-need-a-package-manager","title":"Why do you need a package manager?","text":"<p>In the Python ecosystem, the distribution and installation of software through packages is a standard practice. These packages, often available in Wheel or zip formats, encapsulate source code along with vital metadata. Manually handling these packages and their dependencies can quickly become cumbersome, underscoring the need for package managers. Tools like uv automate these processes, boosting productivity and guaranteeing consistent environments across development and deployment.</p> Python Environment(source) <p>By default, uv will download and install Python packages from PyPI, a repository of software for the Python programming language. If needed, other Python repositories can be configured to provide extra sources of dependencies.</p>"},{"location":"1.%20Initializing/1.3.%20uv%20%28project%29.html#why-should-you-use-uv-in-your-project","title":"Why should you use uv in your project?","text":"<p>Incorporating uv into your project brings several key advantages:</p> <ul> <li>Improved Environment Management: uv streamlines the management of different project environments, promoting consistent development practices.</li> <li>Simplified Package Building and Distribution: It provides a unified workflow for building, distributing, and installing packages, easing the complexities usually associated with these tasks.</li> <li>Uniform Project Metadata: uv employs a standardized approach to defining project metadata, including dependencies, authors, and versioning, through a <code>pyproject.toml</code> file. This standardization ensures clarity and uniformity.</li> </ul> <p>Compared to traditional approaches that involve pip, venv, and manual dependency management, uv offers a more cohesive and friendly experience, merging multiple package and environment management tasks into a single, simplified process.</p>"},{"location":"1.%20Initializing/1.3.%20uv%20%28project%29.html#how-can-you-use-uv-for-your-mlops-project","title":"How can you use uv for your MLOps project?","text":"<p>Integrating uv into your MLOps project involves several key steps designed to configure and prepare your development environment:</p> <ul> <li>Begin by creating a new project directory and navigate into it.</li> <li>Run <code>uv init</code> in your terminal. This command starts an interactive guide to help set initial project parameters, such as package name, version, description, author, and dependencies. This step generates a <code>pyproject.toml</code> file, crucial for your project's configuration under uv.</li> <li>Run <code>uv sync</code> to install the project dependencies and source code. This will let you access your project code through <code>uv run</code> and its command-line utilities.</li> </ul> <p>The <code>pyproject.toml</code> file plays a central role in defining your project\u2019s dependencies and settings.</p> <pre><code># https://docs.astral.sh/uv/reference/pyproject-toml/\n\n[project]\nname = \"bikes\"\nversion = \"1.0.0\"\ndescription = \"Predict the number of bikes available.\"\nauthors = [{ name = \"Your Name\", email = \"Your Email\" }]\nreadme = \"README.md\"\nrequires-python = \"&gt;=3.12\"\ndependencies = []\nlicense = { file = \"LICENSE.txt\" }\nkeywords = [\"mlops\", \"python\", \"package\"]\n\n[project.urls]\nHomepage = \"https://github.com/fmind/bikes\"\nDocumentation = \"https://fmind.github.io/bikes/\"\nRepository = \"https://github.com/fmind/bikes\"\n\"Bug Tracker\" = \"https://github.com/fmind/bikes/issues\"\nChangelog = \"https://github.com/fmind/bikes/blob/main/CHANGELOG.md\"\n\n[project.scripts]\nbikes = 'bikes.scripts:main'\n\n[build-system]\nrequires = [\"hatchling\"]\nbuild-backend = \"hatchling.build\"\n</code></pre> <p>At the end of the installation process, a <code>uv.lock</code> file is generated with all the project dependencies that have been installed. You can remove and regenerate the <code>uv.lock</code> file if you wish to update the list of dependencies.</p>"},{"location":"1.%20Initializing/1.3.%20uv%20%28project%29.html#how-can-you-install-dependencies-for-your-project-with-uv","title":"How can you install dependencies for your project with uv?","text":"<p>Uv differentiates between main (production) and development dependencies, offering an organized approach to dependency management. To add dependencies, use the following commands:</p> <pre><code># For main dependencies\n$ uv add pandas scikit-learn\n\n# For development dependencies\n$ uv add --group dev ipykernel\n</code></pre> <p>Executing these commands updates the <code>pyproject.toml</code> file, accurately managing and versioning your project's dependencies.</p>"},{"location":"1.%20Initializing/1.3.%20uv%20%28project%29.html#what-is-the-difference-between-main-and-dev-dependencies-in-uv","title":"What is the difference between main and dev dependencies in uv?","text":"<p>In uv, dependencies are divided into two types: main dependencies and development (dev) dependencies.</p> <p>Main Dependencies: These are essential for your project's production environment\u2014your application can't run without them. For example, libraries like Pandas or XGBoost would be main dependencies for an MLOps project.</p> <p>Development Dependencies: These are used only during development and testing, such as testing frameworks (e.g., pytest) or linters (e.g., ruff). They are not required in production.</p> <p>Here\u2019s a simple example in a <code>pyproject.toml</code> file:</p> <pre><code>[project]\ndependencies = [\n    \"flask&gt;=3.1.0\",  # Main dependency\n]\n\n[project.optional-dependencies]\ndev = [\n    \"pytest&gt;=8.3.4\",  # Development dependency\n]\n</code></pre> <p>This setup helps keep production environments lean by excluding unnecessary development tools.</p>"},{"location":"1.%20Initializing/1.3.%20uv%20%28project%29.html#can-you-use-uv-to-download-python-dependencies-from-your-own-organizations-code-repository","title":"Can you use uv to download Python dependencies from your own organization's code repository?","text":"<p>Uv supports incorporating custom package repositories, including private or organizational ones. This capability allows for the use of proprietary packages in conjunction with those from the public PyPI. Adding a custom repository and setting up authentication is facilitated by uv's configuration commands, offering secure and adaptable dependency management.</p>"},{"location":"1.%20Initializing/1.3.%20uv%20%28project%29.html#uv-project-additional-resources","title":"uv (project) additional resources","text":"<ul> <li><code>pyproject.toml</code> example from the MLOps Python Package</li> <li>uv: Unified Python packaging</li> </ul>"},{"location":"1.%20Initializing/1.4.%20git.html","title":"1.4. Git","text":""},{"location":"1.%20Initializing/1.4.%20git.html#what-is-git","title":"What is Git?","text":"<p>Git is a distributed version control system that is integral for managing both small and large projects effectively. It excels in tracking source code changes during software development, enabling multiple developers to collaborate on the same project without conflicts. Git is highly regarded for its robust data integrity, versatility, and support for complex, nonlinear development workflows.</p> Git (source)"},{"location":"1.%20Initializing/1.4.%20git.html#why-do-you-need-git","title":"Why do you need Git?","text":"<p>Git serves several critical purposes in software development:</p> <ul> <li>Version Control: It meticulously tracks and manages changes to your project, offering the ability to revert to previous states, compare changes across timelines, and more.</li> <li>Collaboration: Git facilitates simultaneous collaboration among multiple contributors on the same project. It supports branching and merging strategies, allowing seamless teamwork without risking data overwrite.</li> <li>Backup and Restore: With changes stored in a repository, Git acts as a backup mechanism. You can revert your project to a prior state or retrieve lost data as needed.</li> <li>Branching and Merging: Git enables you to create branches for experimenting or developing new features independently of the main project, which can later be merged back into the mainline without disrupting the ongoing development.</li> </ul>"},{"location":"1.%20Initializing/1.4.%20git.html#how-can-you-install-git","title":"How can you install Git?","text":"<p>To install Git, consult the Git Installation Guide, which offers comprehensive instructions for a variety of operating systems. This ensures you can efficiently set up Git on any development environment.</p> <pre><code># installation on mac with brew\nbrew install git\n\n# install on linux with apt\nsudo apt install git\n</code></pre>"},{"location":"1.%20Initializing/1.4.%20git.html#how-should-you-use-git-for-your-project","title":"How should you use Git for your project?","text":"<p>For Git beginners, starting with a foundational tutorial, such as the one provided by GitHub's Git Tutorial, is recommended. Here's a simplified guide to using Git in your project:</p> <ol> <li>Initialize Git: In your project directory, execute <code>git init</code> to create a new local Git repository.</li> <li>Stage Files: Stage files for your next commit with <code>git add &lt;file&gt;</code>, for instance, <code>git add README.md LICENSE.txt</code>.</li> <li>Check Status: Use <code>git status</code> to view staged changes, unstaged changes, and untracked files.</li> <li>Commit Changes: With <code>git commit -m \"Initial Commit\"</code>, commit your staged changes to the repository, including a descriptive message about the changes.</li> </ol>"},{"location":"1.%20Initializing/1.4.%20git.html#should-you-commit-every-file-in-your-project","title":"Should you commit every file in your project?","text":"<p>When using Git, it's important to selectively track files. Consider the following guidelines:</p> <ul> <li>Exclude Secrets: Sensitive data, such as API keys and passwords, should never be committed to your repository.</li> <li>Manage Large Files: For files exceeding 100MB (e.g., dataset files), use Git Large File Storage (git-lfs) instead of directly committing them to your Git repository.</li> <li>Omit Cache Files: Do not track temporary or environment-specific files (e.g., <code>.venv</code>, <code>mlruns</code>, log files) that don't contribute to the project's primary function.</li> </ul> <p>To exclude certain files and directories from being tracked, create a <code>.gitignore</code> file in your project's root directory. This file should list patterns to match filenames you wish to exclude, for example:</p> <pre><code># https://git-scm.com/docs/gitignore\n\n# Build\n/dist/\n/build/\n\n# Cache\n.cache/\n.coverage*\n.mypy_cache/\n.ruff_cache/\n.pytest_cache/\n\n# Editor\n/.idea/\n/.vscode/\n.ipynb_checkpoints/\n\n# Environs\n.env\n/.venv/\n\n# Project\n/docs/*\n/mlruns/*\n/outputs/*\n!**/.gitkeep\n\n# Python\n*.py[cod]\n__pycache__/\n</code></pre> <p>Adhering to these practices ensures your repository remains streamlined, containing only pertinent project files and thus enhancing the clarity and efficiency of your development process.</p>"},{"location":"1.%20Initializing/1.4.%20git.html#git-additional-resources","title":"Git additional resources","text":"<ul> <li><code>.gitignore</code> example from the MLOps Python Package</li> <li>About Git</li> <li>Git Tutorial on W3Schools</li> <li>gittutorial - A tutorial introduction to Git</li> <li>Introduction to Git and GitHub for Python Developers</li> </ul>"},{"location":"1.%20Initializing/1.5.%20GitHub.html","title":"1.5. GitHub","text":""},{"location":"1.%20Initializing/1.5.%20GitHub.html#what-is-github","title":"What is GitHub?","text":"<p>GitHub is a cloud-based platform designed to enhance software development through the Git version control system. It enables developers to store, manage, and track changes in their code, offering a web-based interface, access controls, and collaboration features such as bug tracking, feature requests, task management, and wikis for projects.</p>"},{"location":"1.%20Initializing/1.5.%20GitHub.html#why-do-you-need-to-use-github","title":"Why do you need to use GitHub?","text":"<p>GitHub provides numerous benefits for your projects:</p> <ul> <li>Collaboration: It facilitates easy collaboration on projects across different locations, supporting seamless code reviews and management for all team sizes.</li> <li>Version Control: GitHub offers robust version control, enabling you to track changes, revert to previous versions, and manage branches effectively.</li> <li>Open Source Networking: By hosting your project on GitHub, you connect with a vast network of developers, which can lead to contributions to your project or opportunities to contribute to others.</li> <li>Integration: GitHub integrates with a variety of development tools, enhancing your workflow and productivity.</li> </ul>"},{"location":"1.%20Initializing/1.5.%20GitHub.html#what-are-github-alternatives","title":"What are GitHub alternatives?","text":"<p>There are several platforms offering similar features to GitHub:</p> <ul> <li>Bitbucket: Provides free private repositories for small teams and integrates with Atlassian's tools like Jira and Trello.</li> <li>GitLab: Offers a comprehensive DevOps platform with hosted and self-hosted options, renowned for its CI/CD features.</li> <li>SourceForge: A preferred platform for open-source projects, offering project management and a directory of open-source software.</li> <li>Azure DevOps, Cloud Source Repository, AWS CodeCommit: Features a suite of development tools, including Git repositories, project management, automated builds, and release management.</li> </ul>"},{"location":"1.%20Initializing/1.5.%20GitHub.html#how-should-you-learn-how-to-use-github","title":"How should you learn how to use GitHub?","text":"<p>To effectively learn GitHub, consider:</p> <ul> <li>GitHub's Guides: Begin with the official guides, covering the basics of using GitHub effectively.</li> <li>Interactive Learning: Explore interactive courses on platforms like Codecademy and Coursera for hands-on training.</li> <li>Practice: Engage in creating projects or contributing to open-source projects on GitHub to gain practical experience.</li> <li>Community Resources: Utilize tutorials, videos, and forums from communities on Stack Overflow and Reddit for additional learning resources.</li> </ul>"},{"location":"1.%20Initializing/1.5.%20GitHub.html#which-services-are-proposed-by-github","title":"Which services are proposed by GitHub?","text":"<p>GitHub offers a range of services to support development and collaboration:</p> <ul> <li>GitHub Pages: Enables hosting and publishing websites from a GitHub repository.</li> <li>GitHub Actions: Automates workflows for CI/CD, allowing code building, testing, and deployment directly within GitHub.</li> <li>GitHub Projects: Provides project management tools to organize work within GitHub.</li> <li>GitHub Security: Automatically scans code for vulnerabilities and errors.</li> <li>GitHub Packages: Allows hosting and sharing software packages either privately within an organization or publicly.</li> </ul>"},{"location":"1.%20Initializing/1.5.%20GitHub.html#which-services-do-you-need-to-use-at-the-beginning","title":"Which services do you need to use at the beginning?","text":"<p>For new projects, start with GitHub Repositories to store your project code, manage branches, and track changes. This foundational service supports the core needs of most projects and can be supplemented with other GitHub services as your project develops.</p>"},{"location":"1.%20Initializing/1.5.%20GitHub.html#how-to-configure-github-for-your-mlops-project","title":"How to configure GitHub for your MLOps project?","text":"<ol> <li>Initialize a GitHub Repository:<ul> <li>Go to GitHub, click \"New repository,\" name your repository, add a description, choose its visibility, and initialize it with a README. Optionally, add a .gitignore file and select a license.</li> </ul> </li> <li>Set Up a Local Git Project:<ul> <li>In your local project directory, run <code>git init</code> to start a Git repository. Add your files with <code>git add .</code>, then commit them with <code>git commit -m \"Initial commit\"</code>.</li> </ul> </li> <li>Link and Synchronize with GitHub:<ul> <li>Connect your local repository to GitHub with <code>git remote add origin [Your-GitHub-Repository-URL]</code>, replacing the placeholder with your repository's URL. Push your changes with <code>git push -u origin main</code>, substituting <code>main</code> with your branch name if different.</li> </ul> </li> </ol> <p>Following these steps, you'll set up a GitHub repository, prepare your local Git project, and synchronize your work, laying a solid foundation for version control, collaboration, and CI/CD processes in your MLOps initiatives.</p>"},{"location":"1.%20Initializing/1.5.%20GitHub.html#github-additional-resources","title":"GitHub additional resources","text":"<ul> <li>MLOps Python Package on GitHub</li> <li>GitHub.com</li> <li>Introduction to Git and GitHub for Python Developers</li> </ul>"},{"location":"1.%20Initializing/1.6.%20VS%20Code.html","title":"1.6. VS Code","text":""},{"location":"1.%20Initializing/1.6.%20VS%20Code.html#what-is-vs-code","title":"What is VS Code?","text":"<p>Visual Studio Code (VS Code) is a versatile and free open-source code editor developed by Microsoft. Known for its efficiency and adaptability, it supports a multitude of programming languages with built-in support for JavaScript, TypeScript, and Node.js. It's celebrated for its extensibility, allowing users to install extensions for additional languages, frameworks, and tools, making it a highly customizable environment for software development.</p>"},{"location":"1.%20Initializing/1.6.%20VS%20Code.html#why-should-you-use-vs-code","title":"Why should you use VS Code?","text":"<p>VS Code stands out as a top choice for developers due to its:</p> <ul> <li>Lightweight Performance: Offers robust features like IntelliSense for code completion, code navigation, and real-time syntax highlighting without bogging down your system.</li> <li>Extensibility: Its extension marketplace provides support for almost all major programming languages and tools.</li> <li>Integrated Terminal: Allows you to run shell commands, Git operations, and scripts from within the editor.</li> <li>Debugging Tools: Comes with built-in debugging support for several languages and the capability to add more via extensions.</li> <li>Source Control Integration: Features integrated Git support and extensions for other version control systems, enhancing code management.</li> </ul>"},{"location":"1.%20Initializing/1.6.%20VS%20Code.html#how-should-you-install-vs-code","title":"How should you install VS Code?","text":"<p>To get started with VS Code:</p> <ol> <li>Go to the Visual Studio Code website.</li> <li>Download the installer for your operating system (Windows, macOS, or Linux).</li> <li>Execute the installer and follow the prompts to complete the installation.</li> </ol>"},{"location":"1.%20Initializing/1.6.%20VS%20Code.html#where-can-you-learn-how-to-use-vs-code","title":"Where can you learn how to use VS Code?","text":"<p>There are plenty of resources to help you master VS Code:</p> <ul> <li>Official Documentation: A thorough guide covering everything from basic to advanced features.</li> <li>VS Code Tips and Tricks: Enhance your productivity with these best practices and shortcuts.</li> <li>Microsoft Learn: Offers free, interactive tutorials for using VS Code in various scenarios.</li> <li>YouTube Channels: Search for VS Code tutorials on YouTube to find numerous guides for all skill levels.</li> </ul>"},{"location":"1.%20Initializing/1.6.%20VS%20Code.html#which-vs-code-extensions-should-you-install-for-mlops","title":"Which VS Code extensions should you install for MLOps?","text":"<p>This section lists the extensions you can install from VS Code Marketplace:</p>"},{"location":"1.%20Initializing/1.6.%20VS%20Code.html#a-tier-the-must","title":"A Tier: The Must","text":"<ul> <li>donjayamanne.python-extension-pack: ultimate pack for Python development: autoDocString, Python, Jinja, IntelliCode, Python Indent, Python Environment Manager, Django.</li> <li>donjayamanne.python-environment-manager: access and manage your Python environments (venv).</li> <li>KevinRose.vsc-python-indent: improve the default indentation for Python files.</li> <li>ms-python.mypy-type-checker: source typing for Python. Great to validate your code and communicate it should be used.</li> <li>ms-python.python: language support for Python (linting, debugging, code formating, and more).</li> <li>ms-python.vscode-pylance: enhanced language server for Python (static checker, intellicode, \u2026).</li> <li>ms-toolsai.jupyter: extension pack for Jupyter Notebooks: Keymap, Slideshow, Notebook Renderer, and Cell Tags.</li> <li>ms-toolsai.jupyter-keymap: keybindings from Jupyter Notebooks.</li> <li>ms-toolsai.jupyter-renderers: render notebook outputs (e.g., plots).</li> <li>ms-toolsai.vscode-jupyter-cell-tags: edit cell tags in notebooks (i.e., metadata used by some plugins).</li> <li>charliermarsh.ruff: A Visual Studio Code extension for Ruff, an extremely fast Python linter and code formatter, written in Rust.</li> </ul>"},{"location":"1.%20Initializing/1.6.%20VS%20Code.html#b-tier-the-great","title":"B Tier: The Great","text":"<ul> <li>alefragnani.project-manager: organize, manage, and access VS Code workspaces and Git repositories.</li> <li>ms-azuretools.vscode-docker: manage and connect to Docker through VS Code.</li> <li>ms-kubernetes-tools.vscode-kubernetes-tools: everything you need to develop Kubernetes applications from VS Code.</li> <li>ms-vscode-remote.remote-containers: develop your code from a Docker container instead of your local system.</li> <li>ms-vscode-remote.remote-ssh: connect to a remote instance for editing files and developing your project.</li> <li>ms-vscode-remote.remote-ssh-edit: language support for SSH configuration files.</li> <li>ms-vscode-remote.vscode-remote-extensionpack: extension pack for remote developments: Dev Containers, Remote SSH, Remote Tunnels, WSL.</li> <li>ms-vscode.remote-explorer: view remote machines (SSH) and tunnels.</li> <li>ms-vscode.remote-repositories: remotely browse and edit git repositories.</li> <li>ms-vsliveshare.vsliveshare: edit code from your colleague computer (and vice versa). Useful for collaboration and troubleshooting.</li> <li>mutantdino.resourcemonitor: display the resource usage of your system in the status bar (CPU, RAM, \u2026). Always useful for data projects.</li> <li>njpwerner.autodocstring: automatically generate Python docstrings from object definitions.</li> <li>redhat.vscode-yaml: YAML language support. Great file format for configuring your application.</li> <li>streetsidesoftware.code-spell-checker: highlight and fix spelling mistakes in your code.</li> <li>tamasfe.even-better-toml: TOML language support. Improve the edition of your<code>pyproject.toml</code> file.</li> <li>usernamehw.errorlens: display warnings and errors next to your code (instead of a dedicated window).</li> <li>VisualStudioExptTeam.vscodeintellicode: AI-assisted development features for VS Code.</li> <li>yzhang.markdown-all-in-one: tons of feature for editing Markdown files (shortcuts, table of content, language support, \u2026).</li> </ul>"},{"location":"1.%20Initializing/1.6.%20VS%20Code.html#c-tier-the-good","title":"C Tier: The Good","text":"<ul> <li>aaron-bond.better-comments: highlight comments based on a prefix (e.g., *, !, ?, TODO, \u2026).</li> <li>bierner.markdown-mermaid: display Mermaid diagrams in Markdown. Great to share and visualize complex concepts and design decisions.</li> <li>christian-kohler.path-intellisense: autocomplete file paths on your system.</li> <li>dchanco.vsc-invoke: execute Invoke tasks from VS Code (alternative to GNU Make).</li> <li>donjayamanne.githistory: visualize your Git history (files, branches, commits, \u2026).</li> <li>eamodio.gitlens: enhanced your git experience (e.g., git blame, code lens, \u2026).</li> <li>GitHub.remotehub: remotely browse and edit GitHub repositories.</li> <li>github.vscode-github-actions: manage GitHub Actions workflows from VS Code.</li> <li>GitHub.vscode-pull-request-github: manage GitHub Pull Requests from VS Code.</li> <li>Gruntfuggly.todo-tree: view all TODO, FIXME, and other annotations from a Tab.</li> <li>IBM.output-colorizer: syntax highlighting for log files.</li> <li>jebbs.plantuml: create UML diagrams to document your project.</li> <li>mechatroner.rainbow-csv: colorize the columns of CSV files to improve their readability.</li> <li>mhutchie.git-graph: view and edit your git graph (tags, branches, stashes, \u2026).</li> <li>mikestead.dotenv: syntax highlighting for dotenv files (i.e., contain the environment variables of your program).</li> <li>ms-vscode.live-server: host a local server for web development.</li> <li>oderwat.indent-rainbow: make code indentation more readable.</li> <li>pomdtr.excalidraw-editor: edit Excalidraw diagrams. Great to design architecture diagrams during brainstorm sessions.</li> <li>sleistner.vscode-fileutils: create, copy, move, rename, and delete files from VS Code commands.</li> <li>vsls-contrib.gistfs: manage and share code snippets on GitHub Gist.</li> <li>wayou.vscode-todo-highlight: highlight TODO, FIXME and other keywords in your comments.</li> <li>wholroyd.jinja: language support for Jinja. Popular template language for substituting variables in text files.</li> </ul>"},{"location":"1.%20Initializing/1.6.%20VS%20Code.html#how-can-you-configure-vs-code-settings-for-mlops","title":"How can you configure VS Code settings for MLOps?","text":"<p>You can find below some settings for configuring VS Code. Each setting is annotated to help you understand what is does. Keep in mind that you can hover your cursor over a setting key in VS Code to access more information about that setting. You can also switch between the textual and visual interface using the \u201cGo to document\u201d icon at the top left of the editor settings.</p> <pre><code>{\n    // enable Code Spell Checker by default\n    \"cSpell.enabled\": true,\n    // limit Code Spell Checker to markdown files\n    \"cSpell.enabledLanguageIds\": [\n        \"markdown\"\n    ],\n    // use en-US language by default for Code Spell Checker\n    \"cSpell.language\": \"en-US\",\n    // don't accept completion on enter\n    \"editor.acceptSuggestionOnEnter\": \"off\",\n    // don't let the cursor blink (distracting)\n    \"editor.cursorBlinking\": \"solid\",\n    // smooth caret for smooth editing :)\n    \"editor.cursorSmoothCaretAnimation\": \"on\",\n    // always let 15 lines as a margin when you scroll\n    \"editor.cursorSurroundingLines\": 15,\n    // use Fira code as the main font (support ligatures)\n    \"editor.fontFamily\": \"'Fira Code', monospace\",\n    // enable programming ligature (e.g., replace -&gt; by \u2192)\n    \"editor.fontLigatures\": true,\n    // default font size, use something comfortable for your eyes!\n    \"editor.fontSize\": 14,\n    // format the code you copy-paste in your editor\n    \"editor.formatOnPaste\": true,\n    // show the completion next to your cursor\n    \"editor.inlineSuggest.enabled\": true,\n    // disable the minimap on the right (distracting)\n    \"editor.minimap.enabled\": false,\n    // disable highlighting the word under the cursor (distracting)\n    \"editor.occurrencesHighlight\": false,\n    // don't highlight the current line (distracting)\n    \"editor.renderLineHighlight\": \"none\",\n    // smooth scrolling for smooth developments :)\n    \"editor.smoothScrolling\": true,\n    // required to use IntelliSense suggestions\n    \"editor.suggestSelection\": \"first\",\n    // enable tab completion (complete code by pressing tab)\n    \"editor.tabCompletion\": \"on\",\n    // if the line is longer than your window, display it on several lines\n    \"editor.wordWrap\": \"on\",\n    // don't automatically select files in explorer when you open them\n    \"explorer.autoReveal\": false,\n    // don't ask for confirmation when you delete a file\n    \"explorer.confirmDelete\": false,\n    // don't ask for confirmation when you drag and drop a file\n    \"explorer.confirmDragAndDrop\": false,\n    // save your file before switching to another one\n    \"files.autoSave\": \"onFocusChange\",\n    // set Python as the default language for new files\n    \"files.defaultLanguage\": \"python\",\n    // always have an empty line at the end of the file\n    \"files.insertFinalNewline\": true,\n    // use VS Code file explorer instead of the operating system\n    \"files.simpleDialog.enable\": true,\n    // remove whitespaces at the end of each line\n    \"files.trimTrailingWhitespace\": true,\n    // automatically fetch repository changes from GitHub\n    \"git.autofetch\": true,\n    // don't ask for confirmation before synchronizing git repositories\n    \"git.confirmSync\": false,\n    // commit all unstaged files using VS Code Source Control Tab\n    \"git.enableSmartCommit\": true,\n    // disable GitLens code lens (distracting)\n    \"gitlens.codeLens.enabled\": false,\n    // disable GitLens annotations on current line (distracting)\n    \"gitlens.currentLine.enabled\": false,\n    // trigger hover for the current line\n    \"gitlens.hovers.currentLine.over\": \"line\",\n    // create a vertical colored line for indentation\n    \"indentRainbow.indicatorStyle\": \"light\",\n    // don't ask when restarting Jupyter kernels\n    \"jupyter.askForKernelRestart\": false,\n    // allow to step out of user written code\n    \"jupyter.debugJustMyCode\": false,\n    // create an interactive window per file (see tips and tricks)\n    \"jupyter.interactiveWindow.creationMode\": \"perFile\",\n    // send the selected code to the interactive window instead of terminal\n    \"jupyter.interactiveWindow.textEditor.executeSelection\": true,\n    // enable the auto-reload extension by default for Jupyter notebooks\n    \"jupyter.runStartupCommands\": [\n        \"%load_ext autoreload\",\n        \"%autoreload 2\"\n    ],\n    // disable smart scrolling (lock scrolling when output view is selected)\n    \"output.smartScroll.enabled\": false,\n    // automatically format Python imports and code on save\n    \"[python]\": {\n        \"editor.codeActionsOnSave\": {\n            \"source.organizeImports\": true,\n        },\n        \"editor.defaultFormatter\": \"charliermarsh.ruff\",\n    },\n    // disable redhat telemetry (avoid a popup on first use)\n    \"redhat.telemetry.enabled\": false,\n    // allow untrusted files in the workspace when opened\n    \"security.workspace.trust.untrustedFiles\": \"open\",\n    // don't synchronize the following settings\n    \"settingsSync.ignoredSettings\": [\n        \"projectManager.git.baseFolders\"\n    ],\n    // use the same keybindings on Linux, Mac, and Windows\n    // note: you might want to drop this setting depending on your keybindings\n    \"settingsSync.keybindingsPerPlatform\": false,\n    // don't ask which problem matcher to use for executing VS Code tasks\n    \"task.problemMatchers.neverPrompt\": true,\n    // disable multiline paste warning by default\n    \"terminal.integrated.enableMultiLinePasteWarning\": false,\n    // enabled stronger integration between VS Code and the terminal\n    \"terminal.integrated.shellIntegration.enabled\": true,\n    // don't automatically open the peek view after running unit tests\n    \"testing.automaticallyOpenPeekView\": \"never\",\n    // don't follow the test currently running in the Test Explorer View\n    \"testing.followRunningTest\": false,\n    // open the Text Explorer View on failure\n    \"testing.openTesting\": \"openOnTestFailure\",\n    // ask VS Code to change settings required for running IntelliCode\n    \"vsintellicode.modify.editor.suggestSelection\": \"automaticallyOverrodeDefaultValue\",\n    // replace VS Code title bar by the command certer (menu, selections, widgets, ...)\n    \"window.commandCenter\": true,\n    // don't enable mnemonics shortcuts (e.g., ALT + ...)\n    // this is related to my Alt-key trick, more on that later\n    \"window.enableMenuBarMnemonics\": false,\n    // authorize the title bar to be changed for enabling the command center\n    \"window.titleBarStyle\": \"custom\",\n    // don't preview file (pending opening state), open them directly instead\n    \"workbench.editor.enablePreview\": false,\n    // focus on the tab on the left instead of the most recent one\n    \"workbench.editor.focusRecentEditorAfterClose\": false,\n    // all open tab will be opened in multiple line (instead of scroll bar)\n    \"workbench.editor.wrapTabs\": true,\n}\n</code></pre>"},{"location":"1.%20Initializing/1.6.%20VS%20Code.html#how-to-configure-vs-code-for-using-the-jupyter-extension-with-uv","title":"How to configure VS Code for using the Jupyter Extension with uv?","text":"<p>To configure VS Code for using the Jupyter Extension with uv, follow these steps to ensure that your uv-managed virtual environment is recognized within VS Code. This allows you to use the Jupyter Extension seamlessly with the Python interpreter provided by uv.</p> <ol> <li>Install the Jupyter Extension: First, ensure that the Jupyter Extension is installed in VS Code. You can find and install this extension from the VS Code Marketplace.</li> <li>Open Your Project in VS Code: Open your project folder in VS Code. If you've just created a new uv project, this will be the directory containing your <code>pyproject.toml</code> file.</li> <li>Select Python Interpreter: To make VS Code use the Python interpreter from your uv environment:<ul> <li>Open the Command Palette (<code>Ctrl+Shift+P</code> on Windows/Linux, <code>Cmd+Shift+P</code> on macOS).</li> <li>Type <code>Python: Select Interpreter</code> and select it.</li> <li>Look for the interpreter that corresponds to your uv environment. It will typically be located under the <code>.venv</code> path within your project directory or listed as a virtual environment with your project's name.</li> <li>Select the appropriate interpreter.</li> </ul> </li> <li>Verify Jupyter Notebook Configuration: Create a new Jupyter notebook in VS Code (<code>*.ipynb</code> file) and verify that the cells execute using the Python interpreter from your uv environment. You can check the upper-right corner of the notebook interface to see which interpreter is currently active.</li> <li>Install Necessary Libraries: If you need additional Python libraries that are not yet part of your uv project, you can add them by running <code>uv add &lt;library-name&gt;</code> in your terminal or command prompt. This ensures that all dependencies are managed by uv and available in the notebook.</li> </ol>"},{"location":"1.%20Initializing/1.6.%20VS%20Code.html#vs-code-additional-resources","title":"VS Code additional resources","text":"<ul> <li>Python in Visual Studio Code</li> <li>Python Development in Visual Studio Code</li> <li>Advanced Visual Studio Code for Python Developers</li> <li>How to configure VS Code for AI, ML and MLOps development in Python</li> <li>Awesome VS Code</li> </ul>"},{"location":"1.%20Initializing/1.7.%20pyenv%20%28ARCHIVE%29.html","title":"1.7. pyenv (ARCHIVE)","text":""},{"location":"1.%20Initializing/1.7.%20pyenv%20%28ARCHIVE%29.html#what-is-pyenv","title":"What is pyenv?","text":"<p>Pyenv is a tool specifically designed for managing multiple Python versions on a single computer. It addresses a common challenge developers encounter: needing to work on multiple projects simultaneously, each requiring a different Python version. Pyenv allows for seamless switching between Python versions, ensuring each project runs within its required environment. This capability prevents interference with the system-wide Python installation or with other projects.</p>"},{"location":"1.%20Initializing/1.7.%20pyenv%20%28ARCHIVE%29.html#why-should-you-use-pyenv","title":"Why should you use pyenv?","text":"<p>Opting for pyenv as your Python version management tool offers several benefits:</p> <ul> <li>Flexibility: Pyenv enables the effortless management and transition between various Python versions, accommodating the unique requirements of different projects.</li> <li>Non-root Installation: It supports installing Python versions without necessitating system-wide alterations or administrator privileges. This feature is particularly beneficial for users lacking root access.</li> <li>System Python Independence: Pyenv operates at the user level, managing Python versions independently of the system's Python. This approach mitigates conflicts with the operating system's Python version, promoting a more stable and predictable development environment.</li> </ul>"},{"location":"1.%20Initializing/1.7.%20pyenv%20%28ARCHIVE%29.html#how-to-install-pyenv-on-your-computer","title":"How to install pyenv on your computer?","text":"<p>For pyenv installation, refer to the official pyenv GitHub repository, which provides detailed installation instructions for various operating systems. These guidelines facilitate a smooth setup process on your system, regardless of the operating system you're using.</p>"},{"location":"1.%20Initializing/1.7.%20pyenv%20%28ARCHIVE%29.html#is-there-a-specific-setup-for-mlops-projects","title":"Is there a specific setup for MLOps projects?","text":"<p>When setting up pyenv for MLOps projects, the key consideration is to select a Python version that ensures compatibility with the project's dependencies, libraries, and frameworks. Other than choosing the appropriate Python version, MLOps projects do not have unique setup requirements with pyenv.</p>"},{"location":"1.%20Initializing/1.7.%20pyenv%20%28ARCHIVE%29.html#how-to-install-the-required-version-of-python-for-your-project","title":"How to install the required version of Python for your project?","text":"<p>To install a particular Python version, like Python 3.12, use the command below with pyenv:</p> <pre><code>pyenv install 3.12\n</code></pre> <p>You can select the global version of Python to use on your system with this command:</p> <pre><code>pyenv global 3.12\n</code></pre>"},{"location":"1.%20Initializing/1.7.%20pyenv%20%28ARCHIVE%29.html#how-can-you-select-the-version-of-python-for-your-project","title":"How can you select the version of Python for your project?","text":"<p>To set a Python version for your project, proceed as follows:</p> <ol> <li>Go to your project's root directory.</li> <li>Create a <code>.python-version</code> file, adding the desired Python version (e.g., <code>3.12</code>) to it:</li> </ol> <pre><code>3.12\n</code></pre> <ol> <li>Once pyenv is configured and active, it will automatically switch to the version specified in the <code>.python-version</code> file upon entering the project directory. You can verify the active Python version with:</li> </ol> <pre><code># Confirming the currently active Python version\n$ python --version\n</code></pre> <p>If the expected version switch does not occur, ensure that your shell is properly set up to integrate with pyenv. This typically involves appending pyenv initialization commands to your shell's configuration file. Detailed instructions are available on the pyenv GitHub page.</p>"},{"location":"1.%20Initializing/1.7.%20pyenv%20%28ARCHIVE%29.html#how-can-you-determine-the-currently-active-python-version-in-your-shell","title":"How can you determine the currently active Python version in your shell?","text":"<p>To check the active Python version in your shell, use the command:</p> <pre><code>pyenv version\n</code></pre> <p>For information on the Python version and its executable location, independent of pyenv, the following commands are useful:</p> <pre><code># To see the Python version\npython --version\n\n# To locate the Python executable\nwhich python\n</code></pre>"},{"location":"1.%20Initializing/1.7.%20pyenv%20%28ARCHIVE%29.html#pyenv-additional-resources","title":"PyEnv additional resources","text":"<ul> <li>Pyenv on GitHub</li> <li>Managing Multiple Python Versions With pyenv</li> <li>Calm the Chaos of Your Python Environment with Pyenv</li> </ul>"},{"location":"1.%20Initializing/1.8.%20Poetry%20%28ARCHIVE%29.html","title":"1.8 Poetry (ARCHIVE)","text":""},{"location":"1.%20Initializing/1.8.%20Poetry%20%28ARCHIVE%29.html#what-is-poetry","title":"What is Poetry?","text":"<p>Poetry stands out as a contemporary tool for Python package and dependency management, aiming to streamline the process of defining, managing, and packaging project dependencies. It fulfills the need for a unified tool capable of handling project setup, dependency resolution, and package distribution, proving to be an essential asset for Python developers.</p> Python Environment(source)"},{"location":"1.%20Initializing/1.8.%20Poetry%20%28ARCHIVE%29.html#what-is-a-package","title":"What is a package?","text":"<p>A Python package is a set of Python modules grouped together that can be installed and used within your projects. Python packages help you manage the functionality of Python by allowing you to add and utilize external libraries and frameworks that are not part of the standard Python library.</p> <p>Poetry simplifies the management of these packages by handling them as dependencies. When using Poetry, developers can easily specify which packages are needed for their projects through a <code>pyproject.toml</code> file. Poetry ensures that all specified dependencies are installed in the correct versions, maintaining a stable and conflict-free environment for development. Here\u2019s an example of specifying dependencies with Poetry:</p> <pre><code>[tool.poetry]\nname = \"example-project\"\nversion = \"0.1.0\"\ndescription = \"An example project to demonstrate Poetry\"\n\n[tool.poetry.dependencies]\npython = \"^3.8\"\nrequests = \"^2.25.1\"\n\n[tool.poetry.dev-dependencies]\npytest = \"^5.2\"\n</code></pre> <p>You will learn more on how to construct and publish Python Package in the Package section of this course.</p>"},{"location":"1.%20Initializing/1.8.%20Poetry%20%28ARCHIVE%29.html#why-do-you-need-a-package-manager","title":"Why do you need a package manager?","text":"<p>In the Python ecosystem, the distribution and installation of software through packages is a standard practice. These packages, often available in Wheel or zip formats, encapsulate source code along with vital metadata. Manually handling these packages and their dependencies can quickly become cumbersome, underscoring the need for package managers. Tools like Poetry automate these processes, boosting productivity and guaranteeing consistent environments across development and deployment.</p> <p>By default, Poetry will download and install Python packages from Pypi, a repository of software for the Python programming language. If needed, other Python repositories can be configured to providing extra source of dependencies.</p>"},{"location":"1.%20Initializing/1.8.%20Poetry%20%28ARCHIVE%29.html#why-should-you-use-poetry-in-your-project","title":"Why should you use Poetry in your project?","text":"<p>Incorporating Poetry into your project brings several key advantages:</p> <ul> <li>Improved Environment Management: Poetry streamlines the management of different project environments, promoting consistent development practices.</li> <li>Simplified Package Building and Distribution: It provides a unified workflow for building, distributing, and installing packages, easing the complexities usually associated with these tasks.</li> <li>Uniform Project Metadata: Poetry employs a standardized approach to defining project metadata, including dependencies, authors, and versioning, through a <code>pyproject.toml</code> file. This standardization ensures clarity and uniformity.</li> </ul> <p>Compared to traditional approaches that involve pip, venv, and manual dependency management, Poetry offers a more cohesive and friendly experience, merging multiple package and environment management tasks into a single, simplified process.</p>"},{"location":"1.%20Initializing/1.8.%20Poetry%20%28ARCHIVE%29.html#how-can-you-install-poetry-on-your-system","title":"How can you install Poetry on your system?","text":"<p>Poetry can be installed through various methods to accommodate different preferences and system setups. The recommended way is via pipx, which installs Poetry in an isolated environment to avoid conflicts with other project dependencies. Confirming the installation is as simple as running <code>poetry --version</code> in the terminal, which will display the installed Poetry version.</p> <pre><code># Install pipx on your system\npython -m pip install pipx\n\n# install poetry using pipx\npipx install poetry\n</code></pre> <p>At the time of writing, the latest version of Poetry is 1.8.2.</p>"},{"location":"1.%20Initializing/1.8.%20Poetry%20%28ARCHIVE%29.html#how-can-you-use-poetry-for-your-mlops-project","title":"How can you use Poetry for your MLOps project?","text":"<p>Integrating Poetry into your MLOps project involves several key steps designed to configure and prepare your development environment:</p> <ul> <li>Begin by creating a new project directory and navigate into it.</li> <li>Run <code>poetry init</code> in your terminal. This command starts an interactive guide to help set initial project parameters, such as package name, version, description, author, and dependencies. This step generates a <code>pyproject.toml</code> file, crucial for your project's configuration under Poetry.</li> <li>Run <code>poetry install</code> to install the project dependencies and source code. This will let you access your project code through <code>poetry shell</code> and its command-line utilities with <code>poetry run</code>.</li> </ul> <p>The <code>pyproject.toml</code> file plays a central role in defining your project\u2019s dependencies and settings, with further configuration details available in the Poetry documentation.</p> <pre><code># https://python-poetry.org/docs/pyproject/\n\n# PROJECT\n\n[tool.poetry]\nname = \"bikes\"\nversion = \"1.0.0\"\ndescription = \"Predict the number of bikes available.\"\nrepository = \"https://github.com/fmind/mlops-python-package\"\ndocumentation = \"https://fmind.github.io/mlops-python-package/\"\nreadme = \"README.md\"\nlicense = \"CC BY\"\nkeywords = [\"mlops\", \"python\", \"package\"]\npackages = [{ include = \"bikes\", from = \"src\" }]\n\n[tool.poetry.scripts]\nbikes = 'bikes.scripts:main'\n\n[tool.poetry.dependencies]\npython = \"^3.12\"\n\n[build-system]\nrequires = [\"poetry-core\"]\nbuild-backend = \"poetry.core.masonry.api\"\n</code></pre> <p>At the end of the installation process, a <code>poetry.lock</code> file is generated with all the project dependencies that have been installed. You can remove and regenerate the <code>poetry.lock</code> file if you wish to update the list of dependencies.</p>"},{"location":"1.%20Initializing/1.8.%20Poetry%20%28ARCHIVE%29.html#what-is-a-python-virtual-environment","title":"What is a Python virtual environment?","text":"<p>A Python virtual environment is an isolated space where you can manage Python packages for specific projects without affecting the global Python installation. This setup allows different projects to have their own dependencies, regardless of what dependencies every other project has.</p> <p>Poetry enhances the management of virtual environments by automatically creating and managing them for each project. It handles the installation of required packages into these isolated environments, ensuring that dependencies for one project do not interfere with another. This process is seamless and automatic, simplifying development workflows and reducing dependency conflicts.</p>"},{"location":"1.%20Initializing/1.8.%20Poetry%20%28ARCHIVE%29.html#how-can-you-make-your-poetry-project-easier-to-manage","title":"How can you make your Poetry project easier to manage?","text":"<p>To enhance your Poetry management experience, consider creating a <code>poetry.toml</code> file in your project's root with specific virtual environment configurations:</p> <pre><code># https://python-poetry.org/docs/configuration/\n\n[virtualenvs]\nin-project = true\nprefer-active-python = true\n</code></pre> <p>These configurations ensure that Poetry creates virtual environments directly within your project directory and prioritizes the active Python interpreter. This practice is aligned with optimal environment management standards. More details are available in the Poetry documentation on environment management.</p>"},{"location":"1.%20Initializing/1.8.%20Poetry%20%28ARCHIVE%29.html#how-can-you-install-dependencies-for-your-project-with-poetry","title":"How can you install dependencies for your project with Poetry?","text":"<p>Uv differentiates between main (production) and development dependencies, offering an organized approach to dependency management. To add dependencies, use the following commands:</p> <pre><code># For main dependencies\n$ uv add pandas scikit-learn\n\n# For development dependencies\n$ uv add --group dev ipykernel\n</code></pre> <p>Executing these commands updates the <code>pyproject.toml</code> file, accurately managing and versioning your project's dependencies.</p> <p>In production, you can decide to install only the main dependencies using this command:</p> <pre><code>poetry install --only main\n</code></pre>"},{"location":"1.%20Initializing/1.8.%20Poetry%20%28ARCHIVE%29.html#what-is-the-difference-between-main-and-dev-dependencies-in-uv","title":"What is the difference between main and dev dependencies in uv?","text":"<p>In uv, dependencies are divided into two types: main dependencies and development (dev) dependencies.</p> <p>Main Dependencies: These are essential for your project's production environment\u2014your application can't run without them. For example, libraries like Pandas or XGBoost would be main dependencies for an MLOps project.</p> <p>Development Dependencies: These are used only during development and testing, such as testing frameworks (e.g., pytest) or linters (e.g., ruff). They are not required in production.</p> <p>Here\u2019s a simple example in a <code>pyproject.toml</code> file:</p> <pre><code>[tool.poetry.dependencies]\nflask = \"^2.0.1\"  # Main dependency\n\n[tool.poetry.dev-dependencies]\npytest = \"^6.2.4\"  # Development dependency`\n</code></pre> <p>This setup helps keep production environments lean by excluding unnecessary development tools.</p>"},{"location":"1.%20Initializing/1.8.%20Poetry%20%28ARCHIVE%29.html#can-you-use-poetry-to-download-python-dependencies-from-your-own-organizations-code-repository","title":"Can you use Poetry to download Python dependencies from your own organization's code repository?","text":"<p>Poetry supports incorporating custom package repositories, including private or organizational ones. This capability allows for the use of proprietary packages in conjunction with those from the public PyPI. Adding a custom repository and setting up authentication is facilitated by Poetry's configuration commands, offering secure and adaptable dependency management.</p> <p>For a deeper understanding of Poetry's features, including advanced configurations, package specifications, and command instructions, consult the official Poetry documentation. These resources offer detailed insights into leveraging Poetry to its fullest potential in your Python projects.</p>"},{"location":"1.%20Initializing/1.8.%20Poetry%20%28ARCHIVE%29.html#poetry-additional-resources","title":"Poetry additional resources","text":"<ul> <li><code>poetry.toml</code> example from the MLOps Python Package</li> <li><code>pyproject.toml</code> example from the MLOps Python Package</li> <li>Poetry Website</li> <li>Poetry Basic Usage</li> <li>Dependency Management With Python Poetry</li> <li>Why Is Poetry Essential to the Modern Python Stack?</li> </ul>"},{"location":"2.%20Prototyping/index.html","title":"2. Prototyping","text":"<p>In this chapter, we'll explore the cornerstone of any machine learning (ML) project: Prototyping through Python notebooks. Prototyping is a preliminary phase where data scientists and engineers experiment with various approaches to find the most effective solution. This stage is crucial for understanding the problem at hand, experimenting with different models, and identifying the best strategies before finalizing the project's architecture and moving into production. We'll cover essential tools and practices that enhance the efficiency and effectiveness of this process, focusing on practical aspects that can significantly impact the success of ML projects.</p> <ul> <li>2.0. Notebooks: Introduces Jupyter notebooks as an essential tool for prototyping in machine learning, covering their advantages for iterative development and interactive data exploration.</li> <li>2.1. Imports: Discusses best practices for organizing import statements in notebooks to ensure clarity and maintainability, including recommendations for grouping and ordering libraries.</li> <li>2.2. Configs: Highlights the importance of centralizing configuration settings, such as paths and parameters, for easier experimentation and reproducibility.</li> <li>2.3. Datasets: Offers guidelines for loading, exploring, and preprocessing datasets within notebooks, emphasizing methods for efficient data handling and analysis.</li> <li>2.4. Analysis: Explores techniques for conducting thorough data analysis in notebooks, including visualizations, statistical tests, and exploratory data analysis (EDA) practices.</li> <li>2.5. Modeling: Details strategies for building, refining, and comparing machine learning models directly within notebooks, covering everything from initial prototypes to model selection.</li> <li>2.6. Evaluations: Provides insights on effectively evaluating model performance using various metrics and visualizations, underscoring the role of evaluation in the iterative model development process.</li> </ul>"},{"location":"2.%20Prototyping/2.0.%20Notebooks.html","title":"2.0. Notebooks","text":""},{"location":"2.%20Prototyping/2.0.%20Notebooks.html#what-is-a-python-notebook","title":"What is a Python notebook?","text":"<p>A Python notebook, often referred to as \"notebook,\" is an interactive computing environment that allows users to combine executable code, rich text, visuals, and other multimedia resources in a single document. This tool is invaluable for data science, machine learning projects, documentation, and educational purposes, among others. Notebooks are structured in a cell-based format, where each cell can contain either code or text. When code cells are executed, the output is displayed directly beneath them, facilitating a seamless integration of code and content.</p>"},{"location":"2.%20Prototyping/2.0.%20Notebooks.html#where-can-you-learn-how-to-use-notebooks","title":"Where can you learn how to use notebooks?","text":"<p>Learning how to use notebooks is straightforward, thanks to a plethora of online resources. Beginners can start with the official documentation of popular notebook applications like Jupyter or Google Colab. YouTube channels dedicated to data science and Python programming also frequently cover notebooks, providing valuable tips and tutorials for both beginners and advanced users.</p>"},{"location":"2.%20Prototyping/2.0.%20Notebooks.html#why-should-you-use-a-notebook-for-prototyping","title":"Why should you use a notebook for prototyping?","text":"<p>Notebooks offer an unparalleled environment for prototyping due to their unique blend of features:</p> <ul> <li>Interactive Development: Notebooks allow for real-time code execution, offering immediate feedback on code functionality. This interactivity is especially beneficial when testing new ideas or debugging.</li> <li>Exploratory Analysis: The ability to quickly iterate over different analytical approaches and visualize results makes notebooks an ideal tool for exploratory data analysis.</li> <li>Productive Environment: The integrated environment of notebooks helps maintain focus by minimizing the need to switch between tools or windows. This consolidation of resources boosts productivity and streamlines the development process.</li> </ul> <p>In addition, the narrative structure of notebooks supports a logical flow of ideas, facilitating the documentation of thought processes and methodologies. This makes it easier to share insights with peers or stakeholders and promote collaboration.</p> <p>As an alternative to notebooks, consider using the Python Interactive Window in Visual Studio Code or other text editors. These environments combine the interactivity and productivity benefits of notebooks with the robustness and feature set of an integrated development environment (IDE), such as source control integration, advanced editing tools, and a wide range of extensions for additional functionality.</p>"},{"location":"2.%20Prototyping/2.0.%20Notebooks.html#can-you-use-your-notebook-in-production-instead-of-creating-a-python-package","title":"Can you use your notebook in production instead of creating a Python package?","text":"<p>Using notebooks in the early stages of development offers many advantages; however, they are not well-suited for production environments due to several limitations:</p> <ul> <li>Lack of Integration: Notebooks often do not integrate seamlessly with tools commonly used in the Python software development ecosystem, such as testing frameworks (pytest), linting tools (ruff), and package managers (uv).</li> <li>Mixed Content: The intermingling of code, output, and narrative in a single document can complicate version control and maintenance, especially with complex projects.</li> <li>Non-Sequential Flow: Notebooks do not enforce a linear execution order, which can lead to confusion and errors if cells are run out of sequence.</li> <li>Lack of Reusability: The format of notebooks does not naturally encourage the development of reusable and modular code, such as functions, classes, or packages.</li> </ul> <p>For these reasons, it is advisable to transition from notebooks to structured Python packages for production. Doing so enables better software development practices, such as unit testing, continuous integration, and deployment, thereby enhancing code quality and maintainability.</p>"},{"location":"2.%20Prototyping/2.0.%20Notebooks.html#do-you-need-to-review-this-chapter-even-if-you-know-how-to-use-notebooks","title":"Do you need to review this chapter even if you know how to use notebooks?","text":"<p>Even seasoned users can benefit from reviewing this chapter. It introduces advanced techniques, new features, and tools that you may not know about. Furthermore, the chapter emphasizes structuring notebooks effectively and applying best practices to improve readability, collaboration, and overall efficiency.</p>"},{"location":"2.%20Prototyping/2.0.%20Notebooks.html#why-do-you-need-to-properly-organize-your-python-notebooks","title":"Why do you need to properly organize your Python notebooks?","text":"<p>Organizing your Python notebooks is key for efficiently converting them into Python packages, which is essential for scaling AI/ML projects. A well-structured notebook enhances productivity by simplifying maintenance, understanding, and debugging of the code. Proper organization involves using Markdown headers to divide the notebook into clear, logical sections, which not only facilitates code reuse and adaptation but also improves collaboration by making the notebooks easier to navigate and understand for all team members.</p>"},{"location":"2.%20Prototyping/2.0.%20Notebooks.html#should-you-save-notebook-outputs-in-your-git-repository","title":"Should you save notebook outputs in your git repository?","text":"<p>Saving notebook outputs in your Git repository is generally not recommended due to several reasons:</p> <ul> <li>Version Control: Storing outputs in the repository can bloat the repository size, making it slower to clone and more cumbersome to manage.</li> <li>Reproducibility: Including outputs can make it harder to reproduce the notebook's results, as the outputs may change over time or across different environments.</li> <li>Confidentiality: Outputs may contain sensitive information, such as data values or model predictions, that should not be shared publicly.</li> <li>Collaboration: Sharing outputs can lead to conflicts and confusion when multiple users work on the same notebook, as the outputs may not match the code execution.</li> <li>Code Focus: The primary focus of version control systems like Git is on tracking changes to code, not data or outputs. Including outputs can distract from the main purpose of the repository.</li> </ul> <p>Instead of saving outputs directly in the repository, consider using tools like Jupyter's nbconvert to export notebooks to different formats (e.g., HTML, PDF) that can be shared on other platforms or included in documentation. You can also use Jupyter's built-in cell tags to hide or exclude specific cells from the exported version, allowing you to control what information is shared while keeping the repository clean and focused on code.</p>"},{"location":"2.%20Prototyping/2.0.%20Notebooks.html#notebook-additional-resources","title":"Notebook additional resources","text":"<ul> <li>Notebook example from the MLOps Python Package</li> <li>Jupyter Website</li> <li>Google Colab</li> <li>Jupyter Notebook: An Introduction</li> <li>Best-of Jupyter</li> </ul>"},{"location":"2.%20Prototyping/2.1.%20Imports.html","title":"2.1. Imports","text":""},{"location":"2.%20Prototyping/2.1.%20Imports.html#what-are-code-imports","title":"What are code imports?","text":"<p>In Python, code imports are statements that let you include functionality from other libraries or modules into your current project. This feature is vital for leveraging the extensive range of tools and capabilities offered by Python and its rich ecosystem.</p> <p>As outlined by PEP 8, the Python community recommends organizing imports in a specific order for clarity and maintenance:</p> <ol> <li>Standard Library Imports: These are imports from Python's built-in modules (e.g., <code>os</code>, <code>sys</code>, <code>math</code>). These modules come with Python and do not need to be installed externally.</li> <li>Related Third Party Imports: These are external libraries that are not included with Python but can be installed using package managers like pip (e.g., <code>numpy</code>, <code>pandas</code>). They extend Python's functionality significantly.</li> <li>Local Application/Library Specific Imports: These are modules or packages that you or your team have created specifically for your project.</li> </ol> <p>Here's an example to illustrate how imports might look in a Python script or notebook:</p> <pre><code>import os  # Standard library module\n\nimport pandas as pd  # External library module\n\nfrom my_project import my_module  # Internal project module\n</code></pre>"},{"location":"2.%20Prototyping/2.1.%20Imports.html#which-packages-do-you-need-for-your-project","title":"Which packages do you need for your project?","text":"<p>In the realm of data science, a few key Python packages form the backbone of most projects, enabling data manipulation, visualization, and machine learning. Essential packages include:</p> <ul> <li>Pandas: For data manipulation and analysis.</li> <li>NumPy: For numerical computing and array manipulation.</li> <li>Matplotlib or Plotly: For creating static, interactive, and animated visualizations.</li> <li>Scikit-learn: For machine learning, providing simple and efficient tools for data analysis and modeling.</li> </ul> <p>To integrate these packages into your project using uv, you can execute the following command in your terminal:</p> <pre><code>uv add pandas numpy matplotlib scikit-learn plotly\n</code></pre> <p>This command tells uv to download and install these packages, along with their dependencies, into your project environment, ensuring version compatibility and easy package management.</p>"},{"location":"2.%20Prototyping/2.1.%20Imports.html#how-should-you-organize-your-imports-to-facilitate-your-work","title":"How should you organize your imports to facilitate your work?","text":"<p>Organizing imports effectively can make your code cleaner, more readable, and easier to maintain. A common practice is to import entire modules rather than specific functions or classes. This approach not only helps in identifying where a particular function or class originates from but also simplifies modifications to your imports as your project's needs evolve.</p> <p>Consider the following examples:</p> <pre><code># Importing entire modules (recommended)\nimport pandas as pd\nfrom sklearn import ensemble\nmodel = ensemble.RandomForestClassifier()\n\n# Importing specific functions/classes\nfrom sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier()\n</code></pre> <p>Importing entire modules (<code>import pandas as pd</code>) is generally recommended for clarity, as it makes it easier to track the source of various functions and classes used in your code for this module.</p>"},{"location":"2.%20Prototyping/2.1.%20Imports.html#what-are-the-risks-if-you-import-classes-and-functions-with-the-same-name","title":"What are the risks if you import classes and functions with the same name?","text":"<p>Importing classes and functions with the same name from different modules can cause name collision, where the latest import overwrites the earlier ones. This can lead to unexpected behavior and make debugging more challenging. Additionally, it reduces code clarity, making the program harder to maintain and understand.</p> <p>For example, consider you import <code>load</code> from two different modules in Python:</p> <pre><code>from module1 import load\nfrom module2 import load  # overwrite load imported from module1\n</code></pre> <p>In this scenario, any subsequent calls to <code>load()</code> will use the <code>load</code> function from <code>module2</code>, not <code>module1</code>, potentially leading to errors if the functions behave differently. To avoid such issues, you could use aliases:</p> <pre><code>from module1 import load as load1\nfrom module2 import load as load2`\n</code></pre> <p>Now, both <code>load</code> functions can be used distinctly as <code>load1()</code> and <code>load2()</code>, preventing any name collision.</p>"},{"location":"2.%20Prototyping/2.1.%20Imports.html#are-there-any-side-effects-when-importing-modules-in-python","title":"Are there any side effects when importing modules in Python?","text":"<p>Importing a module in Python executes all the top-level code in that module, which can lead to side effects. These effects can be both intentional and unintentional. It's crucial to import modules from trusted sources to avoid security risks or unexpected behavior. Be especially cautious of executing code with side effects in your own modules, and make sure any such behavior is clearly documented.</p> <p>Consider this cautionary example:</p> <pre><code># A module with a potentially harmful operation\n# lib.py\nimport os\nos.system(\"rm -rf /\")  # This command is extremely dangerous!\n\n# main.py\nimport lib  # Importing lib.py could lead to data loss\n</code></pre>"},{"location":"2.%20Prototyping/2.1.%20Imports.html#what-should-you-do-if-packages-cannot-be-imported-from-your-notebook","title":"What should you do if packages cannot be imported from your notebook?","text":"<p>If you encounter issues importing packages, it may be because the Python interpreter can't find them. This problem is common when using virtual environments. To diagnose and fix such issues, check the interpreter path and module search paths as follows:</p> <pre><code>import sys\nprint(\"Interpreter path:\", sys.executable)\nprint(\"Module search paths:\", sys.path)\n</code></pre> <p>Adjusting these paths or ensuring the correct virtual environment is activated can often resolve issues related to package imports. With VS Code, you can select the Python environment associated with your project installation (e.g., <code>.venv</code>).</p>"},{"location":"2.%20Prototyping/2.1.%20Imports.html#imports-additional-resources","title":"Imports additional resources","text":"<ul> <li>Imports example from the MLOps Python Package</li> <li>Python import: Advanced Techniques and Tips</li> <li>The Python import system</li> </ul>"},{"location":"2.%20Prototyping/2.2.%20Configs.html","title":"2.2. Configs","text":""},{"location":"2.%20Prototyping/2.2.%20Configs.html#what-are-configs","title":"What are configs?","text":"<p>Configurations, often abbreviated as \"configs,\" serve as a cornerstone in programming. They encapsulate a set of parameters or settings designed to adapt the behavior of your code. By employing configs, you introduce a layer of flexibility and customization, enabling easy adjustments of critical variables without the need to tamper with the core logic of your codebase. This strategy not only enhances code usability but also its adaptability across various scenarios.</p> <p>Here's a practical illustration of configs within a notebook context:</p> <pre><code># Define paths for caching and training data\nROOT = Path(\"../\")\nDATA = str(ROOT / \"data\")\nCACHE = str(ROOT / \".cache\")\nHOUR = str(DATA / \"hour.csv\")\n# Configure random state for reproducibility\nRANDOM = 42\n# Define dataset columns for feature engineering\nINDEX = \"instant\"\nTARGET = \"cnt\"\n# Setup dataset parameters for testing and shuffling\nSPLITS = 4\nSHUFFLE = False  # required (time sensitive)\nTEST_SIZE = 24 * 30 * 2  # use 2 months for backtesting\n# Parameters for pipeline configurations\nSCORING = \"neg_mean_squared_error\"\nPARAM_GRID = {\n    \"regressor__max_depth\": [12, 15, 18, 21],\n    \"regressor__n_estimators\": [150, 200, 250, 300],\n}\n</code></pre>"},{"location":"2.%20Prototyping/2.2.%20Configs.html#why-should-you-create-configs","title":"Why should you create configs?","text":"<p>Incorporating configs into your projects is a reflection of best practices in software development. This approach ensures your code remains:</p> <ul> <li>Flexible: Facilitating effortless adaptations and changes to different datasets or experimental scenarios.</li> <li>Easy to Maintain: Streamlining the process of making updates or modifications without needing to delve deep into the core logic.</li> <li>User-Friendly: Providing a straightforward means for users to tweak the notebook's functionality to their specific requirements without extensive coding interventions.</li> <li>Avoid hard coding and magic numbers: Name and document key variables in your notebook to make them understandable and reviewable by others.</li> </ul> <p>Effectively, configurations act as a universal \"remote control\" for your code, offering an accessible interface for fine-tuning its behavior.</p>"},{"location":"2.%20Prototyping/2.2.%20Configs.html#which-configs-can-you-provide-out-of-the-box","title":"Which configs can you provide out of the box?","text":"<p>When it comes to data science projects, several common configurations are frequently utilized, including:</p> <ul> <li>Data Processing: Parameters like <code>SHUFFLE</code>, <code>TEST_SIZE</code>, and <code>RANDOM_STATE</code> are instrumental in controlling how data is prepared and manipulated.</li> <li>Model Parameters: Definitions such as <code>N_ESTIMATORS</code> and <code>MAX_DEPTH</code> cater to tuning machine learning model behaviors.</li> <li>Execution Settings: Variables like <code>BATCH_SIZE</code> and <code>EPOCHS</code> are crucial for defining the operational aspects of iterative processes, with <code>LIMIT</code> setting constraints on dataset sizes.</li> </ul> <p>An example of how you might define some of these settings is as follows:</p> <pre><code># Configuration for shuffling the dataset to mitigate selection bias\nSHUFFLE = False\n# Setting aside a portion of the data for testing purposes\nTEST_SIZE = 0.2\n# Ensuring reproducibility across experiments through fixed randomness\nRANDOM_STATE = 0\n</code></pre>"},{"location":"2.%20Prototyping/2.2.%20Configs.html#how-should-you-organize-the-configs-in-your-notebook","title":"How should you organize the configs in your notebook?","text":"<p>A logical and functional organization of your configurations can significantly enhance the readability and maintainability of your code. Grouping configs based on their purpose or domain of application is advisable:</p> <pre><code>## Paths\n\nDefine inputs and outputs paths ...\n\n## Randomness\n\nConfigure settings to fix randomness ...\n\n## Dataset\n\nSpecifications on how to load and transform datasets ...\n\n## Pipelines\n\nDetails on defining and executing model pipelines ...\n</code></pre> <p>Such categorization makes it easier for both users and developers to navigate and modify configurations as needed.</p>"},{"location":"2.%20Prototyping/2.2.%20Configs.html#what-are-options","title":"What are options?","text":"<p>In the context of data science notebooks, options are akin to configurations but are specifically tied to the behavior and presentation of libraries such as pandas, matplotlib, and scikit-learn. These options offer a means to customize various aspects, including display settings and output formats, to suit individual needs or project requirements.</p> <p>Here's an example showcasing the use of options in a notebook:</p> <pre><code>import pandas as pd\nimport sklearn\n\n# Configure pandas display settings\npd.options.display.max_rows = None\npd.options.display.max_columns = None\n# Adjust sklearn output format\nsklearn.set_config(transform_output=\"pandas\")\n</code></pre>"},{"location":"2.%20Prototyping/2.2.%20Configs.html#why-do-you-need-to-pass-options","title":"Why do you need to pass options?","text":"<p>Library defaults may not always cater to your specific needs or the demands of your project. For instance:</p> <ul> <li>Pandas' default display settings might truncate your data, hiding essential details.</li> <li>The standard figure size in Matplotlib could be too small for a thorough examination.</li> </ul> <p>Adjusting these options helps tailor the working environment to better fit your workflow and analytical needs, ensuring that outputs are both informative and visually accessible.</p>"},{"location":"2.%20Prototyping/2.2.%20Configs.html#how-should-you-configure-library-options","title":"How should you configure library options?","text":"<p>To optimize your working environment, consider customizing the settings of key libraries according to your project's needs. Here are some guidelines:</p> <p>For Pandas:</p> <pre><code>import pandas as pd\n\n# Adjust maximum display settings for rows and columns\npd.options.display.max_rows = None\npd.options.display.max_columns = None\n# Increase maximum column width to improve readability\npd.options.display.max_colwidth = None\n</code></pre> <p>For Matplotlib:</p> <pre><code>import matplotlib.pyplot as plt\n\n# Customize default figure size for better visibility\nplt.rcParams['figure.figsize'] = (20, 10)\n</code></pre> <p>For Scikit-learn:</p> <pre><code>import sklearn\n\n# Modify the output format to return pandas dataframes instead of numpy arrays\nsklearn.set_config(transform_output='pandas')\n</code></pre>"},{"location":"2.%20Prototyping/2.2.%20Configs.html#configs-additional-resources","title":"Configs additional resources","text":"<ul> <li>Configs example from the MLOps Python Package</li> </ul>"},{"location":"2.%20Prototyping/2.3.%20Datasets.html","title":"2.3. Datasets","text":""},{"location":"2.%20Prototyping/2.3.%20Datasets.html#what-are-datasets","title":"What are datasets?","text":"<p>Datasets are collections of data typically structured in a tabular format, comprising rows and columns where each row represents an observation and each column represents a feature of the observation. They are the foundational elements upon which models are trained, tested, and validated, allowing for the extraction of insights, predictions, and understandings of underlying patterns.</p> <p>Here's an example of how you can load a dataset using pandas in a notebook:</p> <pre><code>import pandas as pd\n# Load the dataset into a pandas DataFrame\ntrain = pd.read_csv('data/train.csv', index_col='Id')\n# Display the shape of the dataset and its first few rows\nprint(train.shape)\ntrain.head()\n</code></pre> <p></p> <p>Datasets can originate from a wide range of sources including files (CSV, Excel, JSON, Parquet, Avro, ...), databases, and real-time data streams. They are essential for developing and testing machine learning models, conducting statistical analyses, and performing data visualization.</p>"},{"location":"2.%20Prototyping/2.3.%20Datasets.html#what-are-key-datasets-properties-in-pandas","title":"What are key datasets properties in pandas?","text":"<p>When working with datasets in pandas, several key properties enable you to quickly inspect and understand the structure and content of your data. According to the pandas documentation, the main DataFrame attributes include:</p> <ul> <li><code>.shape</code>: Returns a tuple representing the dimensionality of the DataFrame.</li> <li><code>.dtypes</code>: Provides the data types of each column.</li> <li><code>.columns</code>: Gives an Index object containing column labels.</li> <li><code>.index</code>: Returns an Index object containing row labels.</li> </ul> <p>These attributes and methods are invaluable for initial data exploration and integrity checks, facilitating a deeper understanding of the dataset's characteristics.</p>"},{"location":"2.%20Prototyping/2.3.%20Datasets.html#which-file-format-should-you-use","title":"Which file format should you use?","text":"<p>Choosing the right file format for your dataset is crucial, as it affects the efficiency of data storage, access, and processing. Consider the following criteria when selecting a file format:</p> <ol> <li>Orientation:<ul> <li>Row-Oriented formats like CSV and Avro are preferable for transactional operations where row-level access is common.</li> <li>Column-Oriented formats like Parquet and ORC are optimal for analytical querying, offering advantages in compression and read efficiency.</li> </ul> </li> <li>Structure:<ul> <li>Flat formats like CSV and Excel work well with tabular data and are straightforward to use with SQL queries and dataframes.</li> <li>Hierarchical formats like JSON and XML are suitable for nested data structures, facilitating integration with document databases and APIs.</li> </ul> </li> <li>Mode:<ul> <li>Textual formats (e.g., CSV, JSON) are human-readable but require consideration for character encoding issues.</li> <li>Binary formats (e.g., Parquet, Avro) offer superior speed and efficiency, making them suitable for larger datasets.</li> </ul> </li> <li>Density:<ul> <li>Dense formats store every data point explicitly</li> <li>Sparse formats only store non-zero values, which can be more efficient for datasets with many missing values.</li> </ul> </li> </ol> <p>For data analytics workloads, we recommend using column-oriented, flat, binary format like the Apache Parquet format.</p> <p>For machine learning modeling, we recommend using row-oriented, binary format based on your framework like TFRecord for TensorFlow or XGBoost DMatrix format.</p>"},{"location":"2.%20Prototyping/2.3.%20Datasets.html#how-can-you-optimize-the-dataset-loading-process","title":"How can you optimize the dataset loading process?","text":"<p>Optimizing the dataset loading process involves several strategies:</p> <ul> <li>Vectorization: Utilize operations that apply to entire arrays or datasets at once, minimizing the use of slow Python loops.</li> <li>Multi-core Processing: Leverage libraries that can perform computations in parallel across multiple CPU cores.</li> <li>Lazy Evaluation: Use programming techniques or libraries that delay the computation until necessary, optimizing memory usage and computation time.</li> <li>Distributed Computing: For handling very large datasets, consider distributed computing frameworks that process data across multiple machines.</li> </ul> <p>For large datasets, pandas might not be sufficient. Consider alternative libraries designed for handling large-scale data efficiently:</p> <ul> <li>Polars: A Rust-based data processing library that is optimized for performance on a single machine and supports lazy operations.</li> <li>DuckDB: An in-process SQL OLAP database management system that excels in analytical query performance on a single machine.</li> <li>Spark: A distributed computing system that provides comprehensive support for big data processing and analytics.</li> </ul> <p>The Ibis project unifies these alternatives under a common interface, allowing seamless transition between different backends based on the scale of your data and computational resources (e.g., using pandas for small datasets on a laptop and Spark for big data on clusters).</p>"},{"location":"2.%20Prototyping/2.3.%20Datasets.html#why-do-you-need-to-split-your-dataset-into-x-and-y","title":"Why do you need to split your dataset into 'X' and 'y'?","text":"<p>In supervised learning, the convention is to split the dataset into features (<code>X</code>) and the target variable (<code>y</code>). This separation is crucial because it delineates the input variables that the model uses to learn from the output variable it aims to predict. Structuring your data this way makes it clear to both the machine learning algorithms and the developers what the inputs and outputs of the models should be.</p> <p>You can separate these using pandas in the following way:</p> <pre><code># Separate the dataset into features and target variable\nX, y = train.drop('target', axis='columns'), train['target']\n</code></pre> <p>This practice lays the groundwork for model training and evaluation, ensuring that the algorithms have a clear understanding of the data they are working with.</p>"},{"location":"2.%20Prototyping/2.3.%20Datasets.html#do-all-datasets-contain-potential-x-and-y-variables","title":"Do all datasets contain potential <code>X</code> and <code>y</code> variables?","text":"<p>Not all datasets contain distinct <code>X</code> (features) and <code>y</code> (target) variables. These are specific to supervised learning. Other types of datasets and machine learning algorithms include:</p> <ul> <li>Time Series Forecasting: Predicts future values of the same series without separate <code>y</code> targets.</li> <li>Unsupervised Learning: Like clustering, where data is grouped without predefined targets, or principal component analysis (PCA) which reduces dimensions without a target variable.</li> <li>Reinforcement Learning: Involves learning from the consequences of actions in an environment, focusing on maximizing rewards rather than predicting a target.</li> <li>Anomaly Detection: Identifies unusual patterns or outliers without a specific target variable for training.</li> </ul>"},{"location":"2.%20Prototyping/2.3.%20Datasets.html#why-should-you-split-your-dataset-further-into-traintest-sets","title":"Why should you split your dataset further into train/test sets?","text":"<p>Splitting your dataset into training and testing sets is essential for accurately evaluating the performance of your machine learning models. This approach allows you to:</p> <ul> <li>Avoid Overfitting: Ensuring that your model performs well not just on the data it was trained on, but also on new, unseen data.</li> <li>Detect Underfitting: Helping identify if the model is too simplistic to capture the underlying patterns in the data.</li> </ul> <p>The <code>train_test_split</code> function from scikit-learn is commonly used for this purpose:</p> <pre><code>from sklearn.model_selection import train_test_split\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n</code></pre> <p>It's crucial to manage potential issues like data leakage, class imbalances, and the temporal nature of data to ensure the reliability of your model evaluations. For instance, the Bike Sharing Demand dataset might use a scikit-learn TimeSeriesSplit to take into account the forecasting aspects of the project.</p>"},{"location":"2.%20Prototyping/2.3.%20Datasets.html#do-you-need-to-shuffle-your-dataset-prior-to-splitting-it-into-traintest-sets","title":"Do you need to shuffle your dataset prior to splitting it into train/test sets?","text":"<p>Whether to shuffle your dataset before splitting it into training and testing sets depends on the nature of your problem. For time-sensitive data, such as time series, shuffling could disrupt the temporal sequence, leading to misleading training data and inaccurate models. In such cases, maintaining the chronological order is critical.</p> <p>For datasets where time or sequence does not impart any context to the data, shuffling helps to ensure that the training and testing sets are representative of the overall dataset, preventing any unintentional bias that might arise from the original ordering of the data. This is especially important in scenarios where the dataset may have been sorted or is not randomly distributed (e.g., sorted by price).</p>"},{"location":"2.%20Prototyping/2.3.%20Datasets.html#datasets-additional-resources","title":"Datasets additional resources","text":"<ul> <li>Dataset example from the MLOps Python Package</li> <li>10 minutes to pandas</li> <li>Scikit-learn dataset transformations</li> <li>Scikit-learn Datasets</li> </ul>"},{"location":"2.%20Prototyping/2.4.%20Analysis.html","title":"2.4. Analysis","text":""},{"location":"2.%20Prototyping/2.4.%20Analysis.html#what-is-exploratory-data-analysis-eda","title":"What is Exploratory Data Analysis (EDA)?","text":"<p>Exploratory Data Analysis (EDA) is a critical step in the data analysis process which involves investigating and summarizing the main characteristics of a dataset, often with visual methods. The goal of EDA is to obtain a deep understanding of the data\u2019s underlying structures and variables, to detect outliers and anomalies, to uncover patterns, and to test assumptions with the help of statistical summaries and graphical representations.</p> <p>EDA is a flexible, data-driven approach that allows for a more in-depth understanding of the data before making any assumptions. It serves as a foundation for formulating hypotheses, defining a more targeted analysis, and selecting appropriate models and algorithms for machine learning projects.</p>"},{"location":"2.%20Prototyping/2.4.%20Analysis.html#how-can-you-use-pandas-to-analyze-your-data","title":"How can you use pandas to analyze your data?","text":"<p>Dataframe libraries like Pandas are an essential tool for EDA in Python, offering a wide array of functions to quickly slice, dice, and summarize your data. To begin analyzing your dataset with pandas, you can use the following methods:</p> <ul> <li><code>.info()</code>: This method provides a concise summary of a DataFrame, giving you a quick overview of the data types, non-null values, and memory usage. It's a good starting point to understand the structure of your dataset.</li> <li><code>.describe(include='all')</code>: Generates descriptive statistics that summarize the central tendency, dispersion, and shape of the dataset's distributions. By setting <code>include='all'</code>, you ensure that both numeric and object column types are included in the output, offering a more comprehensive view.</li> </ul> <p>Here\u2019s how you might use these methods in practice:</p> <pre><code>import pandas as pd\ndf = pd.read_csv('your_dataset.csv')\n# Get a concise summary of the DataFrame\ndf.info()\n# Get descriptive statistics for all columns\ndf.describe(include='all')\n</code></pre> <p></p> <p>These functions allow you to quickly assess the quality and characteristics of your data, facilitating the identification of areas that may require further investigation or preprocessing.</p>"},{"location":"2.%20Prototyping/2.4.%20Analysis.html#how-can-you-visualize-patterns-in-your-dataset","title":"How can you visualize patterns in your dataset?","text":"<p>Visualizing patterns in your dataset is pivotal for EDA, as it helps in recognizing underlying structures, trends, and outliers that might not be apparent from the raw data alone. Python offers a wealth of libraries for data visualization, including:</p> <ul> <li>Plotly Express: A high-level interface for interactive graphing.</li> <li>Matplotlib: A widely used library for creating static, animated, and interactive visualizations.</li> <li>Seaborn: A library based on matplotlib that provides a high-level interface for drawing attractive statistical graphics.</li> </ul> <p>For instance, Plotly Express's <code>scatter_matrix</code> can be utilized to explore relationships between multiple variables:</p> <pre><code>import plotly.express as px\ndf = pd.read_csv('your_dataset.csv')\npx.scatter_matrix(\n    df, dimensions=[\"feature1\", \"feature2\", \"feature3\"], color=\"target_variable\",\n    height=800, title=\"Scatter Matrix of Features\"\n)\n</code></pre> <p></p> <p>This method enables the rapid exploration of pairwise relationships within a dataset, facilitating the identification of patterns, correlations, and potential hypotheses for deeper analysis.</p>"},{"location":"2.%20Prototyping/2.4.%20Analysis.html#is-there-a-way-to-automate-eda","title":"Is there a way to automate EDA?","text":"<p>There are libraries designed to automate the EDA process, significantly reducing the time and effort required to understand a dataset. One such library is ydata-profiling, which generates comprehensive reports from a pandas DataFrame, providing insights into the distribution of each variable, correlations, missing values, and much more.</p> <p>Example with ydata-profiling:</p> <pre><code>from ydata_profiling import ProfileReport\ndf = pd.read_csv('your_dataset.csv')\nprofile = ProfileReport(df, title='Pandas Profiling Report', minimal=True)\nprofile.to_widgets()\n</code></pre> <p>While automated EDA tools like ydata-profiling can offer a quick and broad overview of the dataset, they are not a complete substitute for manual EDA. Human intuition and expertise are crucial for asking the right questions, interpreting the results, and making informed decisions on how to proceed with the analysis. Therefore, automated EDA should be viewed as a complement to, rather than a replacement for, traditional exploratory data analysis methods.</p>"},{"location":"2.%20Prototyping/2.4.%20Analysis.html#how-can-you-handle-missing-values-in-datasets","title":"How can you handle missing values in datasets?","text":"<p>Handling missing values in datasets is crucial for maintaining data integrity. Here are common methods:</p> <ol> <li>Remove Data: Delete rows with missing values, especially if the missing data is minimal.</li> <li>Impute Values: Replace missing values with a statistical substitute like mean, median, or mode, or use predictive modeling.</li> <li>Indicator Variables: Create new columns to indicate data is missing, which can be useful for some models.</li> </ol> <p>MissingNo is a tool for visualizing missing data in Python. To use it:</p> <ol> <li>Install MissingNo: <code>pip install missingno</code></li> <li>Import and Use: <pre><code>import missingno as msno\nimport pandas as pd\n\ndata = pd.read_csv('your_data.csv')\nmsno.matrix(data)  # Visual matrix of missing data\nmsno.bar(data)     # Bar chart of non-missing values\n</code></pre></li> </ol> <p>These visualizations help identify patterns and distributions of missing data, aiding in effective preprocessing decisions.</p>"},{"location":"2.%20Prototyping/2.4.%20Analysis.html#analysis-additional-resources","title":"Analysis additional resources","text":"<ul> <li>Example from the MLOps Python Package</li> <li>10 minutes to pandas</li> </ul>"},{"location":"2.%20Prototyping/2.5.%20Modeling.html","title":"2.5. Modeling","text":""},{"location":"2.%20Prototyping/2.5.%20Modeling.html#what-are-pipelines","title":"What are pipelines?","text":"<p>Pipelines in machine learning provide a streamlined way to organize sequences of data preprocessing and modeling steps. They encapsulate a series of data transformations followed by the application of a model, facilitating both simplicity and efficiency in the development process. Pipelines can be broadly categorized as follows:</p> <ul> <li>Model Pipeline: Focuses specifically on sequences related to preparing data for machine learning models and applying these models. For instance, scikit-learn's <code>Pipeline</code> class allows for chaining preprocessors and estimators.</li> <li>Data Pipeline: Encompasses a wider scope, including steps for data gathering, cleaning, and transformation. Tools such as Prefect and ZenML offer capabilities for building comprehensive data pipelines.</li> <li>Orchestration Pipeline: Targets the automation of a series of tasks, including data and model pipelines, ensuring they execute in an orderly fashion or under specific conditions. Examples include Apache Airflow for creating directed acyclic graphs (DAGs) and Vertex AI for managing ML workflows.</li> </ul> <p>For the purposes of this discussion, we'll focus on model pipelines, crucial for efficiently prototyping machine learning solutions. The code example are based on scikit-learn pipeline, as this toolkit is simple to understand and its concept can be generalized to other types of pipeline like Dagster, Prefect, or Metaflow.</p> <p>Example of defining a pipeline in a notebook:</p> <pre><code>from sklearn import pipeline, compose, preprocessing, ensemble\n\ncategoricals = [...] # List of categorical feature names\nnumericals = [...] # List of numerical feature names\nRANDOM = 42 # Fixed random state for reproducibility\nCACHE = './.cache' # Path for caching transformers\n\n# Constructing a pipeline\ndraft = pipeline.Pipeline(\n    steps=[\n        (\"transformer\", compose.ColumnTransformer([\n            (\"categoricals\", preprocessing.OneHotEncoder(\n                sparse_output=False, handle_unknown=\"ignore\"\n            ), categoricals),\n            (\"numericals\", \"passthrough\", numericals),\n        ], remainder=\"drop\")),\n        (\"regressor\", ensemble.RandomForestRegressor(random_state=RANDOM)),\n    ],\n)\n</code></pre> <p></p>"},{"location":"2.%20Prototyping/2.5.%20Modeling.html#why-do-you-need-to-use-a-pipeline","title":"Why do you need to use a pipeline?","text":"<p>Implementing pipelines in your machine learning projects offers several key advantages:</p> <ul> <li>Prevents Data Leakage during preprocessing: By ensuring data preprocessing steps are applied correctly during model training and validation, pipelines help maintain the integrity of your data.</li> <li>Simplifies Cross-Validation and Hyperparameter Tuning: Pipelines facilitate the application of transformations to data subsets appropriately during procedures like cross-validation, ensuring accurate and reliable model evaluation.</li> <li>Ensures Consistency: Pipelines guarantee that the same preprocessing steps are executed in both the model training and inference phases, promoting consistency and reliability in your ML workflow.</li> </ul> <p>Pipelines thus represent an essential tool in the machine learning toolkit, streamlining the model development process and enhancing model performance and evaluation.</p>"},{"location":"2.%20Prototyping/2.5.%20Modeling.html#why-do-you-need-to-process-inputs-by-type","title":"Why do you need to process inputs by type?","text":"<p>Different data types typically require distinct preprocessing steps to prepare them effectively for machine learning models:</p> <ul> <li>Numerical Features may benefit from scaling or normalization to ensure that they're on a similar scale.</li> <li>Categorical Features often require encoding (e.g., OneHotEncoding) to transform them into a numerical format that models can understand.</li> <li>Datetime Features might be broken down into more granular components (e.g., year, month, day) to capture temporal patterns more effectively.</li> </ul> <p>Utilizing scikit-learn's <code>ColumnTransformer</code>, you can specify different preprocessing steps for different columns of your data, ensuring that each type is handled appropriately.</p> <p>Example of selecting features by type from a Pandas DataFrame:</p> <pre><code>import pandas as pd\n\n# Assume X_train is your training data stored in a Pandas DataFrame\nnum_features = X_train.select_dtypes(include=['number']).columns.tolist()\ncat_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n</code></pre>"},{"location":"2.%20Prototyping/2.5.%20Modeling.html#what-is-the-benefit-of-using-a-memory-cache","title":"What is the benefit of using a memory cache?","text":"<p>Employing a memory cache with pipelines, such as the <code>memory</code> attribute in scikit-learn's <code>Pipeline</code>, offers significant performance benefits by caching the results of transformation steps. This approach is particularly advantageous during operations like grid search, where certain preprocessing steps are repeatedly executed across different parameter combinations. Caching can dramatically reduce computation time by avoiding redundant processing.</p> <p>Example of utilizing a memory cache with a pipeline:</p> <pre><code>from sklearn import pipeline, compose, preprocessing, ensemble\n\n# Assuming 'categoricals' and 'numericals' are defined as before\nCACHE = './.cache' # Directory for caching transformers\n\n# Constructing the pipeline with caching enabled\ndraft = pipeline.Pipeline(\n    steps=[\n        (\"transformer\", compose.ColumnTransformer([\n            (\"categoricals\", preprocessing.OneHotEncoder(\n                sparse_output=False, handle_unknown=\"ignore\"\n            ), categoricals),\n            (\"numericals\", \"passthrough\", numericals),\n        ], remainder=\"drop\")),\n        (\"regressor\", ensemble.RandomForestRegressor(random_state=RANDOM)),\n    ],\n    memory=CACHE,\n)\n</code></pre> <p>Even if you don't plan on using scikit-learn pipeline abstraction, you can implement the same concept in your code base to obtain the same benefits.</p>"},{"location":"2.%20Prototyping/2.5.%20Modeling.html#how-can-you-change-the-pipeline-hyper-parameters","title":"How can you change the pipeline hyper-parameters?","text":"<p>Adjusting hyper-parameters within a scikit-learn pipeline can be achieved using the <code>set_params</code> method or by directly accessing parameters via the double underscore (<code>__</code>) notation. This flexibility allows you to fine-tune your model directly within the pipeline structure.</p> <p>Example of setting pipeline hyper-parameters:</p> <pre><code>from sklearn.pipeline import Pipeline\nfrom sklearn import preprocessing, ensemble\n\n# Assume 'RANDOM_STATE' and 'PARAM_GRID' are defined\npipeline = Pipeline([\n    ('encoder', preprocessing.OneHotEncoder()),\n    ('regressor', ensemble.RandomForestRegressor(random_state=RANDOM_STATE))\n])\n\n# Adjusting hyper-parameters using 'set_params'\npipeline.set_params(regressor__n_estimators=100, regressor__max_depth=10)\n</code></pre>"},{"location":"2.%20Prototyping/2.5.%20Modeling.html#why-do-you-need-to-perform-a-grid-search-with-your-pipeline","title":"Why do you need to perform a grid search with your pipeline?","text":"<p>Conducting a grid search over a pipeline is crucial for identifying the optimal combination of model hyper-parameters. This exhaustive search evaluates various parameter combinations across your dataset, using cross-validation to ensure robust assessment of model performance.</p> <p>Example of performing grid search with a pipeline:</p> <pre><code>from sklearn.model_selection import GridSearchCV\nfrom sklearn import model_selection\n\nCV = 5\nSCORING = 'neg_mean_squared_error'\nPARAM_GRID = {\n    \"regressor__max_depth\": [15, 20, 25],\n    \"regressor__n_estimators\": [150, 200, 250],\n}\n\nsplitter = model_selection.TimeSeriesSplit(n_splits=CV)\n\nsearch = GridSearchCV(\n    estimator=draft, cv=splitter, param_grid=PARAM_GRID, scoring=SCORING, verbose=1\n)\nsearch.fit(inputs_train, targets_train)\n</code></pre>"},{"location":"2.%20Prototyping/2.5.%20Modeling.html#why-do-you-need-to-perform-cross-validation-with-your-pipeline","title":"Why do you need to perform cross-validation with your pipeline?","text":"<p>Cross-validation is a fundamental technique in the validation process of machine learning models, enabling you to assess how well your model is likely to perform on unseen data. By integrating cross-validation into your pipeline, you can ensure a thorough evaluation of your model's performance, mitigating the risk of overfitting and underfitting.</p> <p>When utilizing <code>GridSearchCV</code> from scikit-learn for hyperparameter tuning, the <code>cv</code> parameter plays a crucial role in defining the cross-validation splitting strategy. This flexibility allows you to tailor the cross-validation process to the specific needs of your dataset and problem domain, ensuring that the model evaluation is both thorough and relevant.</p> <p>Here\u2019s a breakdown of how you can control the cross-validation behavior through the <code>cv</code> parameter:</p> <ul> <li> <p><code>None</code>: By default, or when <code>cv</code> is set to <code>None</code>, GridSearchCV employs a 5-fold cross-validation strategy. This means the dataset is divided into 5 parts, with the model being trained on 4 parts and validated on the 1 remaining part in each iteration.</p> </li> <li> <p>Integer: Specifying an integer for <code>cv</code> changes the number of folds in a K-Fold (or StratifiedKFold for classification tasks) cross-validation. For example, <code>cv=10</code> would perform a 10-fold cross-validation, offering a more thorough validation at the cost of increased computational time.</p> </li> <li> <p>CV Splitter: scikit-learn provides several splitter classes (e.g., <code>KFold</code>, <code>StratifiedKFold</code>, <code>TimeSeriesSplit</code>) that can be used to define more complex cross-validation strategies. Passing an instance of one of these splitters to <code>cv</code> allows for customized dataset splitting that can account for factors like class imbalance or temporal dependencies.</p> </li> <li> <p>Iterable: An iterable yielding train/test splits as arrays of indices directly specifies the data partitions for each fold. This option offers maximum flexibility, allowing for completely custom splits based on external logic or considerations (e.g., predefined groups or stratifications not captured by the standard splitters).</p> </li> </ul>"},{"location":"2.%20Prototyping/2.5.%20Modeling.html#do-you-need-to-retrain-your-pipeline-should-you-use-the-full-dataset","title":"Do you need to retrain your pipeline? Should you use the full dataset?","text":"<p>After identifying the best model and hyper-parameters through grid search and cross-validation, it's common practice to retrain your model on the entire dataset. This approach allows you to leverage all available data, maximizing the model's learning and potentially enhancing its performance when making predictions on new, unseen data.</p> <p>Retraining your model on the full dataset takes advantage of the insights gained during the model selection process, ensuring that the final model is as robust and well-tuned as possible.</p> <p>Example of retraining your pipeline on the full dataset:</p> <pre><code># Assuming 'search' is your GridSearchCV object and 'X', 'y' are your full dataset\nfinal_model = search.best_estimator_\nfinal_model.fit(X, y)\n</code></pre> <p>Alternatively, if you've used <code>GridSearchCV</code> with <code>refit=True</code> (which is the default setting), the best estimator is automatically refitted on the whole dataset provided to <code>fit</code>, making it ready for use immediately after grid search:</p> <pre><code># 'search' has been conducted with refit=True\nfinal_model = search.best_estimator_\n</code></pre> <p>In this way, the final model embodies the culmination of your exploratory work, tuned hyper-parameters, and the comprehensive learning from the entire dataset, positioning it well for effective deployment in real-world applications.</p> <p>It's important to note, however, that while retraining on the full dataset can improve performance, it also eliminates the possibility of evaluating the model on unseen data unless additional, separate validation data is available. Therefore, the decision to retrain should be made with consideration of how model performance will be assessed and validated post-retraining.</p>"},{"location":"2.%20Prototyping/2.5.%20Modeling.html#modeling-additional-resources","title":"Modeling additional resources","text":"<ul> <li>Example from the MLOps Python Package</li> <li>Supervised learning</li> <li>Unsupervised learning</li> <li>HuggingFace Models</li> <li>Kaggle Models</li> </ul>"},{"location":"2.%20Prototyping/2.6.%20Evaluations.html","title":"2.6. Evaluations","text":""},{"location":"2.%20Prototyping/2.6.%20Evaluations.html#what-is-an-evaluation","title":"What is an evaluation?","text":"<p>Model evaluation is a fundamental step in the machine learning workflow that involves assessing a model's predictions to ensure its reliability and accuracy before deployment. It acts as a quality assurance mechanism, providing insights into the model's performance through various means such as error metrics, graphical representations (like validation curves), and more. This step is crucial for verifying that the model performs as expected and is suitable for real-world applications.</p>"},{"location":"2.%20Prototyping/2.6.%20Evaluations.html#why-should-you-evaluate-your-pipeline","title":"Why should you evaluate your pipeline?","text":"<p>Machine learning models can sometimes behave in unpredictable ways due to their inherent complexity. By evaluating your training pipeline, you can uncover issues like data leakage, which undermines the model's ability to generalize to unseen data. Rigorous evaluation builds trust and credibility, ensuring that the model's performance is genuinely robust and not just a result of overfitting or other biases.</p> <p>For more insights on data leakage, explore this link: Data Leakage in Machine Learning.</p>"},{"location":"2.%20Prototyping/2.6.%20Evaluations.html#how-can-you-generate-predictions-with-your-pipeline","title":"How can you generate predictions with your pipeline?","text":"<p>To generate predictions using your machine learning pipeline, employ the hold-out dataset (test set). This approach ensures that the predictions are made on data that the model has not seen during training, providing a fair assessment of its generalization capability. Here's how you can do it:</p> <pre><code># Generate predictions\npredictions = pd.Series(final.predict(inputs_test), index=inputs_test.index)\nprint(predictions.shape)\npredictions.head()\n</code></pre> <p>Also, leverage the insights from your hyperparameter tuning process to understand the effectiveness of various configurations:</p> <pre><code># Analyze hyperparameter tuning results\nresults = pd.DataFrame(search.cv_results_)\nresults = results.sort_values(by=\"rank_test_score\")\nresults.head()\n</code></pre>"},{"location":"2.%20Prototyping/2.6.%20Evaluations.html#what-do-you-need-to-evaluate-in-your-pipeline","title":"What do you need to evaluate in your pipeline?","text":"<p>Evaluating your training pipeline encompasses several key areas:</p>"},{"location":"2.%20Prototyping/2.6.%20Evaluations.html#ranks","title":"Ranks","text":"<p>When analyzing the outcomes of hyperparameter tuning, focus on:</p> <ul> <li>Identifying ineffective hyperparameter combinations to eliminate.</li> <li>Determining if the best hyperparameters are outliers or represent a common trend.</li> </ul> <p>This helps in deciding whether to expand or narrow the search space for optimal parameters.</p> <p>Example:</p> <pre><code># Visualize rank by test score\npx.line(results, x=\"rank_test_score\", y=\"mean_test_score\", title=\"Rank by test score\")\n</code></pre> <p></p>"},{"location":"2.%20Prototyping/2.6.%20Evaluations.html#params","title":"Params","text":"<p>Investigate which hyperparameters lead to better performance by:</p> <ul> <li>Spotting trends that suggest optimal settings.</li> <li>Identifying hyperparameters with minimal impact, which could be omitted.</li> </ul> <p>This enables pinpointing the most effective hyperparameters for your specific problem.</p> <p>Example:</p> <pre><code># Visualize hyperparameter impact\ndimensions = [col for col in results.columns if col.startswith(\"param_\")]\npx.parallel_categories(results, dimensions=dimensions, color=\"mean_test_score\", title=\"Params by test score\")\n</code></pre> <p></p>"},{"location":"2.%20Prototyping/2.6.%20Evaluations.html#predictions","title":"Predictions","text":"<p>Examine the distribution and balance of prediction values, ensuring they align with your training set's characteristics. A similar distribution and balance indicate that your model is generalizing well.</p> <p>Example with a single metric:</p> <pre><code># Calculate a performance metric\nscore = metrics.mean_squared_error(y_test, y_pred)\nscore\n</code></pre> <p>Example with a distribution:</p> <pre><code># Visualize distribution of errors\npx.histogram(errors, x=\"error\", title=\"Distribution of errors\")\n</code></pre> <p></p>"},{"location":"2.%20Prototyping/2.6.%20Evaluations.html#feature-importances","title":"Feature Importances","text":"<p>Understanding which features most significantly influence your model's predictions can streamline the model by removing non-essential features. This analysis is more straightforward in models based on linear and tree structures.</p> <p>Example:</p> <pre><code># Determine feature importances\nimportances = pd.Series(\n    final.named_steps[\"regressor\"].feature_importances_,\n    index=final[:-1].get_feature_names_out(),\n).sort_values(ascending=False)\nprint(importances.shape)\nimportances.head()\n</code></pre> <p></p>"},{"location":"2.%20Prototyping/2.6.%20Evaluations.html#how-can-you-ensure-your-pipeline-was-trained-on-enough-data","title":"How can you ensure your pipeline was trained on enough data?","text":"<p>Employing a learning curve analysis helps you understand the relationship between the amount of training data and model performance. Continue adding diverse data until the model's performance stabilizes, indicating an optimal data volume has been reached.</p> <p>Example using scikit-learn's learning curve:</p> <pre><code># Analyze learning curve\ntrain_size, train_scores, test_scores = model_selection.learning_curve(\n    final, inputs, targets, cv=splitter, scoring=SCORING, random_state=RANDOM,\n)\nlearning = pd.DataFrame(\n    {\n        \"train_size\": train_size,\n        \"mean_test_score\": test_scores.mean(axis=1),\n        \"mean_train_score\": train_scores.mean(axis=1),\n    }\n)\npx.line(learning, x=\"train_size\", y=[\"mean_test_score\", \"mean_train_score\"], title=\"Learning Curve\")\n</code></pre> <p></p>"},{"location":"2.%20Prototyping/2.6.%20Evaluations.html#how-can-you-ensure-your-pipeline-captures-the-right-level-of-complexity","title":"How can you ensure your pipeline captures the right level of complexity?","text":"<p>To balance complexity and performance, use validation curves to see how changes in a model parameter (like depth) affect its performance. Adjust complexity to improve performance without causing overfitting.</p> <p>Example with scikit-learn's validation curve:</p> <pre><code># Explore validation curves for different parameters\nfor param_name, param_range in PARAM_GRID.items():\n    print(f\"Validation Curve for: {param_name} -&gt; {param_range}\")\n    train_scores, test_scores = model_selection.validation_curve(\n        final, inputs, targets, cv=splitter, scoring=SCORING,\n        param_name=param_name, param_range=param_range,\n    )\n    validation = pd.DataFrame(\n        {\n            \"param_value\": param_range,\n            \"mean_test_score\": test_scores.mean(axis=1),\n            \"mean_train_score\": train_scores.mean(axis=1),\n        }\n    )\n    px.line(\n        validation, x=\"param_value\", y=[\"mean_test_score\", \"mean_train_score\"], title=f\"Validation Curve: {param_name}\"\n    )\n</code></pre> <p></p>"},{"location":"2.%20Prototyping/2.6.%20Evaluations.html#evaluations-additional-resources","title":"Evaluations additional resources","text":"<ul> <li>Example from the MLOps Python Package</li> <li>Data Leakage in Machine Learning</li> <li>Model selection and evaluation</li> </ul>"},{"location":"3.%20Productionizing/index.html","title":"3. Productionizing","text":"<p>In this chapter, we'll explore key strategies for improving your Python codebase, making it more maintainable, scalable, and efficient. We'll also cover the pivotal transition from working in notebooks to structuring your code as a Python package, a crucial step for enhancing code quality and collaboration. From organizing your project into packages and modules, understanding programming paradigms, and setting up entry points, to managing configurations, documenting your work, and optimizing your development environment with VS Code, you'll learn practical tips to refine your Python projects. Let's dive into refining your Python skills and elevating your projects.</p> <ul> <li>3.0. Package: Learn how to organize and structure your Python codebase effectively for better modularity and maintainability.</li> <li>3.1. Modules: Dive into the organization of Python files into modules for cleaner, more scalable code.</li> <li>3.2. Paradigms: Explore various programming paradigms (OOP, functional programming) and their applications in putting Python in production.</li> <li>3.3. Entrypoints: Understand the significance of entry points in managing the execution flow of your Python applications.</li> <li>3.4. Configurations: Master the art of externalizing configurations to make your Python projects more flexible and environment-agnostic.</li> <li>3.5. Documentations: Emphasize the importance of thorough documentation to enhance code understandability and maintainability.</li> <li>3.6. VS Code Workspace: Optimize your development environment using Visual Studio Code for Python programming and better productivity.</li> </ul>"},{"location":"3.%20Productionizing/3.0.%20Package.html","title":"3.0. Package","text":""},{"location":"3.%20Productionizing/3.0.%20Package.html#what-is-a-python-package","title":"What is a Python package?","text":"<p>A Python package is a structured collection of Python modules, which allows for a convenient way to organize and share code. Among the various formats a package can take, the wheel format (.whl) stands out. Wheels are a built package format that can significantly speed up the installation process for Python software, compared to distributing source code and requiring the user to build it themselves.</p>"},{"location":"3.%20Productionizing/3.0.%20Package.html#why-do-you-need-to-create-a-python-package","title":"Why do you need to create a Python package?","text":"<p>Creating a Python package offers multiple benefits, particularly for developers looking to distribute their code effectively:</p> <ul> <li>As a Library: Packaging your code as a library enables you to share reusable components across different projects. This is common in the Python ecosystem, with examples like <code>numpy</code>, <code>pandas</code>, and <code>tensorflow</code> being shared as libraries.</li> <li>As an Application: Packaging also plays a crucial role in deploying applications. It simplifies the distribution and installation process, ensuring your software can be easily executed on various systems, including web or mobile platforms.</li> </ul> <p>Additionally, creating a package can enhance the maintainability of your code, enforce good coding practices by encouraging modular design, and facilitate version control and dependency management.</p>"},{"location":"3.%20Productionizing/3.0.%20Package.html#which-tool-should-you-use-to-create-a-python-package","title":"Which tool should you use to create a Python package?","text":"<p>The Python ecosystem provides several tools for packaging, each with its unique features and advantages. While the choice can seem overwhelming, as humorously depicted in the xkcd comic on Python environments, uv emerges as a standout option. Uv simplifies dependency management and packaging, offering an intuitive interface for developers.</p> <p>To get started with uv for packaging, you can use the following commands:</p> <ul> <li>Initiate an uv package:</li> </ul> <pre><code>uv sync\n</code></pre> <ul> <li>Start developing the package:</li> </ul> <pre><code>uv sync --all-groups\n</code></pre> <ul> <li>Build a package with uv:</li> </ul> <pre><code>uv build --wheel\n</code></pre> <p>At the end of the build process, a <code>.whl</code> file is generated in the <code>dist</code> folder with the name and version of the project from <code>pyproject.toml</code>.</p> <p>For those seeking alternatives, tools like PDM, Hatch, and Pipenv offer different approaches to package management and development, each with its own set of features designed to cater to various needs within the Python community.</p>"},{"location":"3.%20Productionizing/3.0.%20Package.html#do-you-recommend-conda-for-your-aiml-project","title":"Do you recommend Conda for your AI/ML project?","text":"<p>Although Conda is a popular choice among data scientists for its ability to manage complex dependencies, it's important to be aware of its limitations. Challenges such as slow performance, a complex dependency resolver, and confusing channel management can hinder productivity. Moreover, Conda's integration with the Python ecosystem, especially with new standards like <code>pyproject.toml</code>, is limited. For managing complex dependencies in AI/ML projects, Docker containers present a robust alternative, offering better isolation and compatibility across environments.</p>"},{"location":"3.%20Productionizing/3.0.%20Package.html#how-can-you-install-new-dependencies-with-uv","title":"How can you install new dependencies with uv?","text":"<p>Please refer to this section of the course.</p>"},{"location":"3.%20Productionizing/3.0.%20Package.html#which-metadata-should-you-provide-to-your-python-package","title":"Which metadata should you provide to your Python package?","text":"<p>Including detailed metadata in your <code>pyproject.toml</code> file is crucial for defining your package's identity and dependencies. This file should contain essential information such as the package name, version, authors, and dependencies. Here's an example that outlines the basic structure and content for your package's metadata:</p> <pre><code># https://docs.astral.sh/uv/reference/settings/\n# https://packaging.python.org/en/latest/guides/writing-pyproject-toml/\n\n[project]\nname = \"bikes\"\nversion = \"3.0.0\"\ndescription = \"Predict the number of bikes available.\"\nauthors = [{ name = \"M\u00e9d\u00e9ric HURIER\", email = \"github@fmind.dev\" }]\nreadme = \"README.md\"\nrequires-python = \"&gt;=3.12\"\ndependencies = []\nlicense = { file = \"LICENSE.txt\" }\nkeywords = [\"mlops\", \"python\", \"package\"]\n\n[project.urls]\nHomepage = \"https://github.com/fmind/bikes\"\nDocumentation = \"https://fmind.github.io/bikes\"\nRepository = \"https://github.com/fmind/bikes\"\n\"Bug Tracker\" = \"https://github.com/fmind/bikes/issues\"\nChangelog = \"https://github.com/fmind/bikes/blob/main/CHANGELOG.md\"\n\n[project.scripts]\nbikes = 'bikes.scripts:main'\n\n[tool.uv]\ndefault-groups = [\"checks\", \"commits\", \"dev\", \"docs\", \"notebooks\"]\n\n[build-system]\nrequires = [\"hatchling\"]\nbuild-backend = \"hatchling.build\"\n</code></pre> <p>This information not only aids users in understanding what your package does but also facilitates its discovery and integration into other projects.</p>"},{"location":"3.%20Productionizing/3.0.%20Package.html#where-should-you-add-the-source-code-of-your-python-package","title":"Where should you add the source code of your Python package?","text":"<p>For a clean and efficient project structure, placing your package's source code in a <code>src</code> directory is recommended. This approach, known as the <code>src</code> layout, separates your package's code from other project files, such as tests and documentation, reducing the risk of import clashes and making it easier to package and distribute your code.</p> <p>Here's how you can set up this structure:</p> <pre><code>mkdir -p src/bikes\ntouch src/bikes/__init__.py\n</code></pre> <p>The presence of an <code>__init__.py</code> file within a directory indicates to Python that this directory should be treated as a package, making it possible for other parts of your project or external projects to import its modules.</p>"},{"location":"3.%20Productionizing/3.0.%20Package.html#should-you-publish-your-python-package-on-which-platform-should-you-publish-it","title":"Should you publish your Python package? On which platform should you publish it?","text":"<p>Deciding whether to publish your Python package depends on your goals. If you aim to share your work with the broader community or need a convenient way to distribute your code across projects or teams, publishing is a great option. The Python Package Index (PyPI) is the primary repository for public Python packages, making it an ideal platform for reaching a wide audience.</p> <p>For private packages or when sharing within a limited group or organization, platforms like AWS CodeArtifact or GCP Artifact Registry offer secure hosting and management of your packages.</p> <p>To publish a package using uv, you can use the command:</p> <pre><code>uv publish\n</code></pre> <p>This will upload your package to PyPI, making it available for installation via <code>pip</code> by the Python community.</p>"},{"location":"3.%20Productionizing/3.0.%20Package.html#package-additional-resources","title":"Package additional resources","text":"<ul> <li><code>pyproject.toml</code> example from MLOps Python Package</li> <li>A great MLOps project should start with a good Python Package \ud83d\udc0d</li> <li>Python Modules and Packages \u2013 An Introduction</li> <li>Packaging Python Projects</li> <li>What Are Python Wheels and Why Should You Care?</li> <li>Awesome MLOps</li> <li>Awesome Production Machine Learning</li> </ul>"},{"location":"3.%20Productionizing/3.1.%20Modules.html","title":"3.1. Modules","text":""},{"location":"3.%20Productionizing/3.1.%20Modules.html#what-are-python-modules","title":"What are Python modules?","text":"<p>Python modules are files containing Python code that serve as a fundamental organizational unit in Python programming. These modules encapsulate definitions such as functions, classes, variables, and constants, making it easier to organize, reuse, and share code across different parts of a program.</p> <p>Modules in Python are more than just physical files; they represent a namespace for Python objects. Importing a module allows you to access these objects in your current script or interactive session.</p> <p>To discover the physical location of a module, use its <code>__file__</code> attribute:</p> <pre><code>import math\nprint(math.__file__)\n</code></pre> <p>You can also enumerate the objects inside a module with the <code>dir()</code> function:</p> <pre><code>import math\nprint(dir(math))\n</code></pre>"},{"location":"3.%20Productionizing/3.1.%20Modules.html#why-do-you-need-python-modules","title":"Why do you need Python modules?","text":"<p>Python modules are essential for managing complexity in your projects. They provide a way to segment your code into distinct namespaces, making your projects more organized, readable, and maintainable. For example, in a machine learning project, you might have separate modules for models (<code>models.py</code>), data processing (<code>datasets.py</code>), and utility functions (<code>utils.py</code>). This separation helps in understanding, testing, and collaborating on large codebases.</p> <p>Modules become indispensable as your project grows beyond a simple script. While a project with less than 100 lines of code might not need separate modules, larger projects benefit greatly from a modular structure.</p>"},{"location":"3.%20Productionizing/3.1.%20Modules.html#how-should-you-create-a-python-module","title":"How should you create a Python module?","text":"<p>Creating a Python module is as simple as creating a <code>.py</code> file within your project package. For example, in a project structured with a <code>src</code> directory, you might organize your modules as follows:</p> <pre><code>$ touch src/bikes/models.py\n$ touch src/bikes/datasets.py\n</code></pre> <p>This creates two modules, <code>models.py</code> and <code>datasets.py</code>, under the <code>bikes</code> package. Each module can then contain specific functionalities related to your project, such as defining data models or handling dataset loading and preprocessing.</p>"},{"location":"3.%20Productionizing/3.1.%20Modules.html#how-should-you-import-your-python-module","title":"How should you import your Python module?","text":"<p>Importing modules in Python is influenced by the directories listed in Python's <code>sys.path</code>, akin to path resolution in Unix systems. When importing a module, Python searches through these directories and imports the first match.</p> <p>To see what directories are in your search path, you can use:</p> <pre><code>import sys\nprint(sys.path)\n</code></pre> <p>After installing your package locally (e.g., using <code>uv sync</code>), your package's directory will be added to <code>sys.path</code>, allowing you to import its modules without specifying their full path.</p>"},{"location":"3.%20Productionizing/3.1.%20Modules.html#how-should-you-organize-your-python-modules","title":"How should you organize your Python modules?","text":"<p>Organizing your Python modules can significantly improve your project's clarity and maintainability. Here are a few strategies for structuring your modules:</p> <ol> <li>Flat Layout: Organize modules by major concept or component, using nouns for names. Examples include:<ul> <li><code>models.py</code></li> <li><code>datasets.py</code></li> <li><code>services.py</code></li> <li><code>splitters.py</code></li> </ul> </li> <li>IO and Domain Separation: Separate modules based on their interaction with the external world (I/O) and internal logic (domain), inspired by IO monad in Haskell and Domain-Driven Design. For instance:<ul> <li>IO Layer:<ul> <li><code>io/services.py</code></li> <li><code>io/datasets.py</code></li> </ul> </li> <li>Domain Layer:<ul> <li><code>domain/models.py</code></li> <li><code>domain/schemas.py</code></li> </ul> </li> <li>High-Level Tasks:<ul> <li><code>training.py</code></li> <li><code>tuning.py</code></li> <li><code>inference.py</code></li> </ul> </li> </ul> </li> </ol> <p>The latter approach distinguishes between the unpredictable nature of I/O operations and the more controlled domain logic, with high-level tasks integrating these layers.</p>"},{"location":"3.%20Productionizing/3.1.%20Modules.html#what-are-the-risks-of-using-python-modules","title":"What are the risks of using Python modules?","text":"<p>A notable risk when using Python modules is the possibility of experiencing side-effects upon import. Side-effects are operations that occur when a module is imported, which can lead to unexpected behavior or bugs if not handled carefully.</p> <pre><code># A module with a potentially harmful operation\n# lib.py\nimport os\nos.system(\"rm -rf /\")  # This command is extremely dangerous!\n\n# main.py\nimport lib  # Importing lib.py could lead to data loss\n</code></pre> <p>To minimize this risk, restrict side-effects to specific entry points and ensure modules primarily contain definitions like functions and classes. Avoid executing code that produces side-effects directly at the module level to ensure cleaner, more predictable imports.</p>"},{"location":"3.%20Productionizing/3.1.%20Modules.html#module-additional-resources","title":"Module additional resources","text":"<ul> <li>Module examples from the MLOps Python Package</li> <li>Python Modules and Packages \u2013 An Introduction</li> <li>Python modules</li> </ul>"},{"location":"3.%20Productionizing/3.2.%20Paradigms.html","title":"3.2. Paradigms","text":""},{"location":"3.%20Productionizing/3.2.%20Paradigms.html#what-is-a-programming-paradigm","title":"What is a programming paradigm?","text":"<p>A programming paradigm refers to a style or methodology of programming that provides a framework for structuring and solving problems. Paradigms influence how we organize, write, and think about code. Common programming paradigms include procedural, object-oriented, functional, and declarative programming. Each paradigm offers a unique approach to code organization, abstraction, and reuse.</p> <ul> <li>Procedural programming emphasizes a step-by-step set of instructions, focusing on routines or subroutines to process data.</li> <li>Object-oriented programming (OOP) encapsulates data and functions that operate on that data within objects, promoting modularity and reuse.</li> <li>Functional programming treats computation as the evaluation of mathematical functions, avoiding changing state and mutable data.</li> <li>Declarative programming specifies what the program should accomplish rather than explicitly listing commands or steps to achieve it.</li> </ul>"},{"location":"3.%20Productionizing/3.2.%20Paradigms.html#can-you-provide-code-examples-for-mlops-with-each-paradigm","title":"Can you provide code examples for MLOps with each paradigm?","text":""},{"location":"3.%20Productionizing/3.2.%20Paradigms.html#procedural-programming","title":"Procedural programming","text":"<p>Procedural programming is a common method for structuring AI/ML code bases. This approach involves writing the entire program as a sequence of steps in a single script. This paradigm can be straightforward and easy to understand, making it appealing for simple projects. However, this simplicity might not be suitable for more complex real-world applications. Here is an illustrative example:</p> <pre><code># Simplistic AI/ML Python Script Example\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\n# Load and preprocess data\ndata = pd.read_csv('dataset.csv')\ndata.fillna(0, inplace=True)\n\n# Split data\nX, y = data.drop('target', axis=1), data['target']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train model\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n\n# Evaluate model\npredictions = model.predict(X_test)\naccuracy = accuracy_score(y_test, predictions)\nprint(f\"Model Accuracy: {accuracy}\")\n</code></pre>"},{"location":"3.%20Productionizing/3.2.%20Paradigms.html#functional-programming","title":"Functional Programming","text":"<p>Functional programming in AI/ML projects promotes modular and declarative coding styles by encapsulating code in functions. This method enhances code clarity and maintainability by allowing functions to be reused and tested separately. High-order functions, immutability, and pure functions are key features of this paradigm. An example using functional programming is as follows:</p> <pre><code>from typing import Callable, Tuple\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.base import BaseEstimator\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\ndef load_and_preprocess_data(filepath: str, fill_na_value: float, target_name: str) -&gt; Tuple[pd.DataFrame, pd.Series]:\n    \"\"\"Load and preprocess data.\"\"\"\n    data = pd.read_csv(filepath)\n    data = data.fillna(fill_na_value)\n    X = data.drop(target_name, axis=1)\n    y = data[target_name]\n    return X, y\n\ndef split_data(X: pd.DataFrame, y: pd.Series, test_size: float, random_state: int) -&gt; Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:\n    \"\"\"Split the data into a train and testing sets.\"\"\"\n    return train_test_split(X, y, test_size=test_size, random_state=random_state)\n\ndef train_model(X_train: pd.DataFrame, y_train: pd.Series, model_func: Callable[[], BaseEstimator], **kwargs) -&gt; BaseEstimator:\n    \"\"\"Train the model with inputs and target data.\"\"\"\n    model = model_func(**kwargs)\n    model.fit(X_train, y_train)\n    return model\n\ndef evaluate_model(model: BaseEstimator, X_test: pd.DataFrame, y_test: pd.Series) -&gt; float:\n    \"\"\"Evaluate the model with a single metric.\"\"\"\n    predictions = model.predict(X_test)\n    accuracy = accuracy_score(y_test, predictions)\n    return accuracy\n\ndef get_model(model_name: str) -&gt; Callable[[], BaseEstimator]:\n    \"\"\"High-order function to select the model to train.\"\"\"\n    if model_name == \"logistic_regression\":\n        return LogisticRegression\n    elif model_name == \"random_forest\":\n        return RandomForestClassifier\n    else:\n        raise ValueError(f\"Model {model_name} is not supported.\")\n\ndef run_workflow(model_name: str, model_kwargs: dict, filepath: str, fill_na_value: float, target_name: str, test_size: float, random_state: int) -&gt; None:\n    \"\"\"Orchestrate the training workflow.\"\"\"\n    X, y = load_and_preprocess_data(filepath, fill_na_value, target_name)\n    X_train, X_test, y_train, y_test = split_data(X, y, test_size, random_state)\n    model_func = get_model(model_name)\n    model = train_model(X_train, y_train, model_func, **model_kwargs)\n    evaluate_model(model, X_test, y_test)\n\n# Example usage\nrun_workflow(\n    filepath='dataset.csv',\n    fill_na_value=0.0,\n    target_name='target',\n    test_size=0.2,\n    random_state=42,\n    model_name='random_forest',  # Or 'logistic_regression'\n    model_kwargs={'n_estimators': 30},\n)\n</code></pre>"},{"location":"3.%20Productionizing/3.2.%20Paradigms.html#object-oriented-programming","title":"Object-oriented programming","text":"<p>Object-Oriented Programming (OOP) uses classes and objects to organize code, making it easier to manage large applications. It fully supports the encapsulation, inheritance, and polymorphism concepts, making it ideal for complex systems. Python's compatibility with OOP is evidenced by many frameworks and libraries adopting it. Below is an example of using OOP principles to manage models:</p> <pre><code>from abc import ABC, abstractmethod\nfrom typing import Tuple, Type\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.base import BaseEstimator\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n\nclass Model(ABC):\n    \"\"\"Abstract base class for models.\"\"\"\n    @abstractmethod\n    def train(self, X_train: pd.DataFrame, y_train: pd.Series) -&gt; None:\n        pass\n\n    @abstractmethod\n    def predict(self, X: pd.DataFrame) -&gt; pd.Series:\n        pass\n\nclass RandomForestModel(Model):\n    \"\"\"Random Forest Classifier model.\"\"\"\n    def __init__(self, n_estimators: int = 20, max_depth: int = 5) -&gt; None:\n        self.model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth)\n\n    def train(self, X_train: pd.DataFrame, y_train: pd.Series) -&gt; None:\n        self.model.fit(X_train, y_train)\n\n    def predict(self, X: pd.DataFrame) -&gt; pd.Series:\n        return self.model.predict(X)\n\nclass KerasBinaryClassifier(Model):\n    \"\"\"Simple binary classification model using Keras.\"\"\"\n    def __init__(self, input_dim: int, epochs: int = 100, batch_size: int = 32) -&gt; None:\n        self.epochs = epochs\n        self.batch_size = batch_size\n        self.model = Sequential([\n            Dense(64, activation='relu', input_shape=(input_dim,)),\n            Dense(1, activation='sigmoid')\n        ])\n        self.model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n    def train(self, X_train: pd.DataFrame, y_train: pd.Series) -&gt; None:\n        self.model.fit(X_train, y_train, epochs=self.epochs, batch_size=self.batch_size)\n\n    def predict(self, X: pd.DataFrame) -&gt; pd.Series:\n        predictions = self.model.predict(X)\n        return (predictions &gt; 0.5).flatten()\n\nclass ModelFactory:\n    \"\"\"Factory to create model instances.\"\"\"\n    @staticmethod\n    def get_model(model_name: str, **kwargs) -&gt; Model:\n        # Assume all model classes are defined in the global scope.\n        model_class = globals()[model_name]\n        return model_class(**kwargs)\n\nclass Workflow:\n    \"\"\"Main workflow class for model training and evaluation.\"\"\"\n    def run_workflow(self, model_name: str, model_kwargs: dict, filepath: str, fill_na_value: float, target_name: str, test_size: float, random_state: int) -&gt; None:\n        X, y = self.load_and_preprocess_data(filepath, fill_na_value, target_name)\n        X_train, X_test, y_train, y_test = self.split_data(X, y, test_size, random_state)\n        model = ModelFactory.get_model(model_name, **model_kwargs)\n        model.train(X_train, y_train)\n        accuracy = self.evaluate_model(model, X_test, y_test)\n        print(f\"Model Accuracy: {accuracy}\")\n\n    def load_and_preprocess_data(self, filepath: str, fill_na_value: float, target_name: str) -&gt; Tuple[pd.DataFrame, pd.Series]:\n        \"\"\"Load and preprocess data.\"\"\"\n        data = pd.read_csv(filepath)\n        data = data.fillna(fill_na_value)\n        X = data.drop(target_name, axis=1)\n        y = data[target_name]\n        return X, y\n\n    def split_data(self, X: pd.DataFrame, y: pd.Series, test_size: float, random_state: int) -&gt; Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:\n        \"\"\"Split the data into a train and testing sets.\"\"\"\n        return train_test_split(X, y, test_size=test_size, random_state=random_state)\n\n    def evaluate_model(self, model: Model, X_test: pd.DataFrame, y_test: pd.Series) -&gt; float:\n        \"\"\"Evaluate the model with a single metric.\"\"\"\n        predictions = model.predict(X_test)\n        accuracy = accuracy_score(y_test, predictions)\n        return accuracy\n\n# Example usage\nworkflow = Workflow()\nworkflow.run_workflow(\n    filepath='dataset.csv',\n    fill_na_value=0.0,\n    target_name='target',\n    test_size=0.2,\n    random_state=\"42\",\n    model_name='RandomForestModel',  # Or 'KerasBinaryClassifier'\n    model_kwargs={'n_estimators': 30},\n)\n</code></pre>"},{"location":"3.%20Productionizing/3.2.%20Paradigms.html#declarative-programming","title":"Declarative programming","text":"<p>Declarative programming is a style where you specify what the program should accomplish without explicitly listing commands or steps to achieve it. This approach can greatly simplify coding in complex systems by separating the logic of what needs to be done from the implementation details. A common use case in machine learning is configuring models and training pipelines using high-level configuration files, which are then processed by machine learning frameworks that handle the underlying operations.</p> <p>For example, using Ludwig, a tool for training machine learning models with declarative configurations, you can specify the architecture and training parameters of a model in YAML format. This allows you to train a model without writing the detailed procedural code typically required. Here's an example of how to train a model to classify images using a Convolutional Neural Network (CNN) defined in a</p> <pre><code># config.yaml\ninput_features:\n- name: image_path\n  type: image\n  encoder:\n      type: stacked_cnn\n      conv_layers:\n        - num_filters: 32\n          filter_size: 3\n          pool_size: 2\n          pool_stride: 2\n        - num_filters: 64\n          filter_size: 3\n          pool_size: 2\n          pool_stride: 2\n          dropout: 0.4\n      fc_layers:\n        - output_size: 128\n          dropout: 0.4\n\noutput_features:\n - name: label\n   type: category\n\ntrainer:\n  epochs: 5\n</code></pre> <p>To train the model using this configuration, you would run the following command:</p> <pre><code>ludwig train --dataset mnist_dataset.csv --config config.yaml\n</code></pre>"},{"location":"3.%20Productionizing/3.2.%20Paradigms.html#why-do-you-need-to-use-functions-and-objects","title":"Why do you need to use functions and objects?","text":"<p>Functions and objects play a crucial role in structuring code in a readable, maintainable, and reusable manner. They allow you to encapsulate functionality and state, making complex software systems more manageable.</p> <p>Functions enable you to define a block of code that performs a single action, which can be executed whenever the function is called. This promotes code reuse and simplifies debugging and testing by isolating functionality.</p> <p>Objects, fundamental to the object-oriented programming paradigm, bundle data and the methods that operate on that data. This encapsulation promotes modularity, as objects can be developed independently and used in different contexts.</p>"},{"location":"3.%20Productionizing/3.2.%20Paradigms.html#how-should-you-write-a-new-function-or-object","title":"How should you write a new function or object?","text":"<p>When identifying opportunities to encapsulate code into functions or objects, look for repetitive code patterns, complex logic that needs isolation, or concepts that can be modeled as real-world objects.</p> <p>In the case of repetitive data loading tasks, abstracting the logic into a function simplifies the process:</p> <pre><code>def load_dataset(path: str, index_col: str = \"Id\") -&gt; pd.DataFrame:\n    \"\"\"Load a CSV dataset from a local path.\"\"\"\n    dataset = pd.read_csv(path, index_col=index_col)\n    print(dataset.shape)\n    return dataset\n</code></pre> <p>When it comes to objects, encapsulate data and behavior that logically belong together. For instance, a <code>DataPreprocessor</code> class could encapsulate methods for cleaning, normalizing, and transforming data, keeping these operations neatly packaged and reusable.</p>"},{"location":"3.%20Productionizing/3.2.%20Paradigms.html#how-should-you-organize-all-your-functions-and-objects","title":"How should you organize all your functions and objects?","text":"<p>Structuring functions and objects into modules helps maintain a clean and navigable codebase. This structure should evolve naturally, starting from a simple layout and growing in complexity as the project expands. For example:</p> <ul> <li><code>data.py</code> for data loading, processing functions, and classes.</li> <li><code>utils.py</code> for utility functions and classes that support various parts of the project.</li> <li><code>models.py</code> for classes representing machine learning models and their logic.</li> </ul> <p>This modular approach aids in separation of concerns, making your code more organized and manageable.</p>"},{"location":"3.%20Productionizing/3.2.%20Paradigms.html#which-coding-paradigm-do-you-recommend-for-mlops-code-bases","title":"Which coding paradigm do you recommend for MLOps code bases?","text":"<p>When developing MLOps applications, selecting the appropriate coding paradigm is crucial for scalability and robustness. While Python supports various programming styles, it is particularly well-suited for object-oriented programming (OOP). This paradigm is advantageous for large applications, as it effectively handles polymorphism through both nominal and structural typing. A testament of Python's OOP capabilities can be shown in popular projects such as Pandas, scikit-learn, and Keras.</p> <p>Although Python can utilize functional programming (FP) features, such as functions and higher-order functions, it does not fully embrace the functional paradigm. It lacks critical FP features including ad-hoc or parametric polymorphism, tail-call optimization, and efficient immutable data structures. These shortcomings make Python less ideal for pure functional programming compared to languages specifically designed for FP like Haskell or Clojure.</p> <p>Given these considerations, we recommend favoring the object-oriented paradigm when building MLOps applications in Python. However, Python's flexibility allows for the integration of functional programming elements to enhance code quality and maintainability.</p>"},{"location":"3.%20Productionizing/3.2.%20Paradigms.html#what-is-the-hybrid-style-that-mixes-object-oriented-and-functional-programming","title":"What is the hybrid style that mixes object-oriented and functional programming?","text":"<p>The hybrid programming style in Python blends the best features of object-oriented and functional programming to create robust and maintainable MLOps applications. This approach helps overcome some of Python's limitations in functional programming while leveraging its strong OOP capabilities. Here are key principles to follow when implementing a hybrid style:</p> <ol> <li>Immutable Attributes: Design objects with immutable attributes to prevent changes to the object's state after initialization. This approach minimizes side effects and enhances predictability.</li> <li>Output-Oriented Methods: Methods should primarily return outputs instead of modifying internal state. This practice encourages the separation of concerns and facilitates easier testing and debugging.</li> <li>Idempotent Methods: Ensure methods are idempotent, meaning they consistently return the same result for the same inputs, a core principle in functional programming.</li> <li>Centralized Imperative Statements: Use high-level classes to manage imperative operations (e.g., logging, database updates). This centralization helps delineate where side effects occur within the application, similar to how the IO monad operates in Haskell.</li> <li>Leverage OOP Benefits: Continue to utilize object-oriented features such as subtyping polymorphism and intuitive class-based representations for better program extensibility and readability.</li> </ol> <p>By integrating these principles, developers can create a flexible architecture that supports extensibility and robustness, which are essential for effective MLOps applications. The hybrid style enables the combination of OOP's clarity and structural benefits with FP's emphasis on immutability and state management, resulting in cleaner, more manageable code.</p> <p></p>"},{"location":"3.%20Productionizing/3.2.%20Paradigms.html#what-are-the-best-practices-for-creating-functions-and-objects","title":"What are the best practices for creating functions and objects?","text":"<p>Following best practices ensures that your functions and objects are reliable, maintainable, and easy to understand:</p> <ol> <li>Type Hints: Specify input and output types to improve code readability and tooling support.</li> <li>Docstrings: Describe the purpose and usage of functions and objects, including parameters and return values.</li> <li>Single Responsibility: Aim for each function and object to perform a single task or represent a single concept.</li> <li>Descriptive Names: Choose names that clearly convey the purpose and functionality.</li> <li>Default Arguments: Use default arguments to provide flexibility, with caution around mutable defaults.</li> <li>Error Handling: Incorporate error handling and validations to make your code robust.</li> <li>Limit Parameters: Keep the number of parameters to a minimum for simplicity and ease of use.</li> <li>Avoid Global Variables: Use local variables within functions and objects to avoid unintended side effects.</li> <li>Testing: Regularly test your code to catch and fix errors early.</li> <li>Readability: Prioritize clear and straightforward code, making it accessible to others (and your future self).</li> </ol>"},{"location":"3.%20Productionizing/3.2.%20Paradigms.html#paradigm-additional-resources","title":"Paradigm additional resources","text":"<ul> <li>Paradigm examples from the MLOps Python Package</li> <li>Finding Harmony in MLOps: Balancing Functional and Object-Oriented Approaches \u262f</li> <li>Programming Paradigms</li> </ul>"},{"location":"3.%20Productionizing/3.3.%20Entrypoints.html","title":"3.3. Entrypoints","text":""},{"location":"3.%20Productionizing/3.3.%20Entrypoints.html#what-are-package-entrypoints","title":"What are package entrypoints?","text":"<p>Package entrypoints are mechanisms in Python packaging that facilitate the exposure of scripts and utilities to end users. Entrypoints streamline the process of integrating and utilizing the functionalities of a package, whether that be through Command-Line Interfaces (CLI) or by other software packages.</p> <p>To elaborate, entrypoints are specified in a package's setup configuration, marking certain functions or classes to be directly accessible. This setup benefits both developers and users by simplifying access to a package's capabilities, improving interoperability among different software components, and enhancing the user experience by providing straightforward commands to execute tasks.</p>"},{"location":"3.%20Productionizing/3.3.%20Entrypoints.html#why-do-you-need-to-set-up-entrypoints","title":"Why do you need to set up entrypoints?","text":"<p>Entrypoints are essential for making specific functionalities of your package directly accessible from the command-line interface (CLI) or to other software. By setting up entrypoints, you allow users and other programs to find and execute components of your package directly from the CLI, streamlining operations like script execution, service initiation, or utility invocation. Additionally, entrypoints facilitate dynamic discovery and utilization of your package's functionalities by other software and frameworks, such as Apache Airflow, without the need for hard-coded paths or module names. This flexibility is particularly beneficial in complex, interconnected systems where adaptability and ease of use are paramount.</p>"},{"location":"3.%20Productionizing/3.3.%20Entrypoints.html#how-do-you-create-a-command-line-script","title":"How do you create a command-line script?","text":"<p>Creating a Python command-line script involves a few steps that turn a Python file into a tool that can be executed directly from the command line. Here\u2019s how to do it:</p>"},{"location":"3.%20Productionizing/3.3.%20Entrypoints.html#create-a-cli-parser","title":"Create a CLI parser","text":"<p>A CLI parser is a crucial component that handles command-line arguments, extracting values to configure your application. Python's <code>argparse</code> module is typically adequate for this purpose. The example below outlines how to set up a parser, defining the program's description and establishing various arguments and flags:</p> <ul> <li><code>files</code>: This argument accepts one or more configuration files. The <code>nargs=\"*\"</code> parameter allows multiple files to be specified.</li> <li><code>--extras</code>: This flag allows additional configuration strings to be passed. Like <code>files</code>, it can accept multiple entries.</li> <li><code>--schema</code>: A boolean flag that, when specified, will print the program's schema and exit the program.</li> <li><code>--help</code>: An implicit flag provided by <code>argparse</code> that displays a help message detailing the script\u2019s arguments and flags.</li> </ul> <pre><code>import argparse\n\nparser = argparse.ArgumentParser(description=\"Run an AI/ML job from YAML/JSON configs.\")\nparser.add_argument(\"files\", nargs=\"*\", help=\"Config files for the job (local path only).\")\nparser.add_argument(\"-e\", \"--extras\", nargs=\"*\", default=[], help=\"Additional config strings for the job.\")\nparser.add_argument(\"-s\", \"--schema\", action=\"store_true\", help=\"Print settings schema and exit.\")\n</code></pre> <p>For alternative libraries, you might consider Typer, Click or Fire, which offer different styles and additional functionalities.</p>"},{"location":"3.%20Productionizing/3.3.%20Entrypoints.html#create-a-main-function","title":"Create a main function","text":"<p>The <code>main</code> function is the core of your script, which interprets command-line inputs and acts upon them. It is conventional to have it accept a list of arguments and return an integer status code (0 for success, other values indicate errors):</p> <pre><code>def main(argv: list[str] | None = None) -&gt; int:\n    args = parser.parse_args(argv)\n    if args.schema:\n        # Print the schema of the settings\n        print(\"Schema details here...\")\n        return 0\n    # Execute the main application logic here\n    return 0\n</code></pre>"},{"location":"3.%20Productionizing/3.3.%20Entrypoints.html#expose-the-main-function","title":"Expose the main function","text":"<p>To ensure the <code>main</code> function executes when the script is run (and not when imported as a module), use the following condition:</p> <pre><code>if __name__ == \"__main__\":\n    main()\n</code></pre> <p>You can then run your script from the command line like this:</p> <pre><code># Running the script without using uv\n$ python script.py config.yml\n# Running the script using uv\n$ uv run python script.py --schema\n</code></pre>"},{"location":"3.%20Productionizing/3.3.%20Entrypoints.html#how-do-you-create-entrypoints-with-uv","title":"How do you create entrypoints with uv?","text":"<p>Creating entrypoints with uv involves specifying them in the <code>pyproject.toml</code> file under the <code>[project.scripts]</code> section. This section outlines the command-line scripts that your package will make available:</p> <pre><code>[project.scripts]\nbikes = 'bikes.scripts:main'\n</code></pre> <p>In this syntax, <code>bikes</code> represents the command users will enter in the CLI to activate your tool. The path <code>bikes.scripts:main</code> directs uv to execute the <code>main</code> function found in the <code>scripts</code> module of the <code>bikes</code> package. Upon installation, uv generates an executable script for this command, integrating your package's functionality seamlessly into the user's command-line environment, alongside other common utilities:</p> <pre><code>$ uv run bikes one two three\n</code></pre> <p>This snippet run the bikes entrypoint from the CLI and passes 3 positional arguments: one, two, and three.</p>"},{"location":"3.%20Productionizing/3.3.%20Entrypoints.html#how-can-you-use-an-entrypoint-in-other-software","title":"How can you use an entrypoint in other software?","text":"<p>Defining and installing a package with entrypoints enables other software to easily leverage these entrypoints. For example, within Apache Airflow, you can incorporate a task in a Directed Acyclic Graph (DAG) to execute one of your CLI tools as part of an automated workflow. By utilizing Airflow's <code>BashOperator</code> or <code>PythonOperator</code>, your package\u2019s CLI tool can be invoked directly, facilitating seamless integration:</p> <pre><code>from airflow import DAG\nfrom datetime import datetime, timedelta\nfrom airflow.providers.databricks.operators.databricks import DatabricksSubmitRunNowOperator\n\n# Define default arguments for your DAG\ndefault_args = {...}\n\n# Create a DAG instance\nwith DAG(\n    'databricks_submit_run_example',\n    default_args=default_args,\n    description='An example DAG to submit a Databricks job',\n    schedule_interval='@daily',\n    catchup=False,\n) as dag:\n    # Define a task to submit a job to Databricks\n    submit_databricks_job = DatabricksSubmitRunNowOperator(\n        task_id='main',\n        json={\n            \"python_wheel_task\": {\n                \"package_name\": \"bikes\",\n                \"entry_point\": \"bikes\",\n                \"parameters\": [ \"one\", \"two\", \"three\" ],\n            },\n        }\n    )\n\n    # Set task dependencies and order (if you have multiple tasks)\n    # In this simple example, there's only one task\n    submit_databricks_job\n</code></pre> <p>In this example, <code>submit_databricks_job</code> is a task that executes the <code>bikes</code> entrypoint.</p>"},{"location":"3.%20Productionizing/3.3.%20Entrypoints.html#how-can-you-use-an-entrypoint-from-the-command-line-cli","title":"How can you use an entrypoint from the command-line (CLI)?","text":"<p>Once your Python package has been packaged with uv and a wheel file is generated, you can install and use the package directly from the command-line interface (CLI). Here are the steps to accomplish this:</p> <ol> <li>Build your package: Use uv to compile your project into a distributable format, such as a wheel file. This is done with the <code>uv build --wheel</code> command, which generates the package files in the <code>dist/</code> directory.</li> </ol> <pre><code>uv build --wheel\n</code></pre> <ol> <li>Install your package: With the generated wheel file (<code>*.whl</code>), use <code>pip</code> to install your package into your Python environment. The <code>pip install</code> command looks for the wheel file in the <code>dist/</code> directory, matching the pattern <code>bikes*.whl</code>, which is the package file created by uv.</li> </ol> <pre><code>pip install dist/bikes*.whl\n</code></pre> <ol> <li>Run your package from the CLI: After installation, you can invoke the package's entrypoint\u2014defined in your <code>pyproject.toml</code> file\u2014directly from the command line. In this case, the <code>bikes</code> command followed by any necessary arguments. If your entrypoint is designed to accept arguments, they can be passed directly after the command. Ensure the arguments are separated by spaces unless specified otherwise in your documentation or help command.</li> </ol> <pre><code>bikes one two three\n</code></pre>"},{"location":"3.%20Productionizing/3.3.%20Entrypoints.html#which-should-be-the-input-or-output-of-your-entrypoint","title":"Which should be the input or output of your entrypoint?","text":"<p>Inputs for your entrypoint can vary based on the requirements and functionalities of your package but typically include:</p> <ul> <li>Configuration files (e.g., JSON, YAML, TOML): These files can define essential settings, parameters, and options required for your tool or package to function. Configuration files are suited for static settings that remain constant across executions, such as environment settings or predefined operational parameters.</li> <li>Command-line arguments (e.g., --verbose, --account): These arguments provide a dynamic way for users to specify options, flags, and parameters at runtime, offering adaptability for different operational scenarios.</li> </ul> <p>Outputs from your entrypoint should be designed to provide valuable insights and effects, such as:</p> <ul> <li>Side effects: The primary purpose of your tool or package, which could include data processing, report generation, or initiating other software processes.</li> <li>Logging: Detailed logs are crucial for debugging, monitoring, and understanding how your tool or package operates within larger systems or workflows.</li> </ul> <p>Careful design of your entrypoints' inputs and outputs ensures your package can be integrated and used efficiently across a wide range of environments and applications, maximizing its utility and effectiveness.</p>"},{"location":"3.%20Productionizing/3.3.%20Entrypoints.html#entrypoint-additional-resources","title":"Entrypoint additional resources","text":"<ul> <li>Script example from the MLOps Python Package</li> <li>Entrypoint example from the MLOps Python Package</li> <li>uv script entrypoints</li> </ul>"},{"location":"3.%20Productionizing/3.4.%20Configurations.html","title":"3.4. Configurations","text":""},{"location":"3.%20Productionizing/3.4.%20Configurations.html#what-are-configurations","title":"What are configurations?","text":"<p>Software configurations consist of parameters or constants for the operation of your program, externalized to allow flexibility and adaptability. Configurations can be provided through various means, such as environment variables, configuration files, or command-line interface (CLI) arguments. For instance, a YAML configuration file might look like this:</p> <pre><code>job:\n  KIND: TrainingJob\n  inputs:\n    KIND: ParquetReader\n    path: data/inputs.parquet\n  targets:\n    KIND: ParquetReader\n    path: data/targets.parquet\n</code></pre> <p>This structure allows for easy adjustment of parameters like file paths or job kinds, facilitating the program's operation across diverse environments and use cases.</p>"},{"location":"3.%20Productionizing/3.4.%20Configurations.html#why-do-you-need-to-write-configurations","title":"Why do you need to write configurations?","text":"<p>Configurations enhance your code's flexibility, making it adaptable to different environments and scenarios without source code modifications. This separation of code from its execution environment boosts portability and simplifies updates or changes, much like adjusting settings in an application without altering its core functionality.</p>"},{"location":"3.%20Productionizing/3.4.%20Configurations.html#which-file-format-should-you-use-for-configurations","title":"Which file format should you use for configurations?","text":"<p>When choosing a format for configuration files, common options include JSON, TOML, and YAML. YAML is frequently preferred for its readability, ease of use, and ability to include comments, which can be particularly helpful for documentation and maintenance. However, it's essential to be aware of YAML's potential for loading malicious content; therefore, always opt for safe loading practices.</p>"},{"location":"3.%20Productionizing/3.4.%20Configurations.html#how-should-you-pass-configuration-files-to-your-program","title":"How should you pass configuration files to your program?","text":"<p>Passing configuration files to your program typically utilizes the CLI, offering a straightforward method to integrate configurations with additional command options or flags. For example, executing a command like:</p> <pre><code>$ bikes defaults.yaml training.yaml --verbose\n</code></pre> <p>This example enables the combination of configuration files with verbosity options for more detailed logging. This flexibility is also extendable to configurations stored on cloud services, provided your application supports such paths.</p>"},{"location":"3.%20Productionizing/3.4.%20Configurations.html#which-toolkit-should-you-use-to-parse-and-load-configurations","title":"Which toolkit should you use to parse and load configurations?","text":"<p>For handling configurations in Python, OmegaConf offers a powerful solution with features like YAML loading, deep merging, variable interpolation, and read-only configurations. It's particularly suited for complex settings and hierarchical structures. Additionally, for applications involving cloud storage, cloudpathlib facilitates direct loading from services like AWS, GCP, and Azure.</p> <pre><code>import typing as T\nimport omegaconf as oc\n\nConfig = oc.ListConfig | oc.DictConfig\n\ndef parse_file(path: str) -&gt; Config:\n    \"\"\"Parse a config file from a path.\"\"\"\n    return oc.OmegaConf.load(path)\n\ndef merge_configs(configs: T.Sequence[Config]) -&gt; Config:\n    \"\"\"Merge a list of config into a single config.\"\"\"\n    return oc.OmegaConf.merge(*configs)\n\nargs = parser.parse_args(argv)\nfiles = [configs.parse_file(file) for file in args.files]\nconfig = configs.merge_configs(files)\n</code></pre> <p>Utilizing Pydantic for configuration validation and default values ensures that your application behaves as expected by catching mismatches or errors in configuration files early in the process, thereby avoiding potential failures after long-running jobs. This is an improvement over Python's dicts as each key are validated and mentioned explicitly in your code base:</p> <pre><code>import pydantic as pdt\n\nclass TrainTestSplitter(pdt.BaseModel):\n    \"\"\"Split a dataframe into a train and test set.\n\n    Parameters:\n        shuffle (bool): shuffle the dataset. Default is False.\n        test_size (int | float): number/ratio for the test set.\n        random_state (int): random state for the splitter object.\n    \"\"\"\n\n    shuffle: bool = False\n    test_size: int | float\n    random_state: int = 42\n</code></pre>"},{"location":"3.%20Productionizing/3.4.%20Configurations.html#when-should-you-use-environment-variables-instead-of-configurations-files","title":"When should you use environment variables instead of configurations files?","text":"<p>Environment variables are more suitable for simple configurations or when dealing with sensitive information that shouldn't be stored in files, even though they lack the structure and type-safety of dedicated configuration files. They are universally supported and easily integrated but may become cumbersome for managing complex or numerous settings.</p> <pre><code>$ MLFLOW_TRACKING_URI=./mlruns bikes one two three\n</code></pre> <p>In this example, the <code>MLFLOW_TRACKING_URI</code> is passed as an environment variable to the <code>bikes</code> program, while the command also accepts 3 positional arguments: one, two, and three.</p>"},{"location":"3.%20Productionizing/3.4.%20Configurations.html#what-are-the-best-practices-for-writing-and-loading-configurations","title":"What are the best practices for writing and loading configurations?","text":"<p>To ensure effective configuration management:</p> <ul> <li>Always use <code>yaml.safe_load()</code> to prevent the execution of arbitrary code.</li> <li>Utilize context managers for handling files to ensure proper opening and closing.</li> <li>Implement robust error handling for I/O operations and parsing.</li> <li>Validate configurations against a schema to confirm correctness.</li> <li>Avoid storing sensitive information in plain text; instead, use secure mechanisms.</li> <li>Provide defaults for optional parameters to enhance usability.</li> <li>Document configurations with comments for clarity.</li> <li>Maintain a consistent format across configuration files for better readability.</li> <li>Consider versioning your configuration format to manage changes effectively in larger projects.</li> </ul> <p>You can use the <code>Configs</code> section of your notebooks to initialize the configuration files for your Python package.</p>"},{"location":"3.%20Productionizing/3.4.%20Configurations.html#configuration-additional-resources","title":"Configuration additional resources","text":"<ul> <li>Configuration management from MLOps Python Package</li> <li>Configuration files from MLOps Python Package</li> </ul>"},{"location":"3.%20Productionizing/3.5.%20Documentations.html","title":"3.5. Documentations","text":""},{"location":"3.%20Productionizing/3.5.%20Documentations.html#what-are-software-documentations","title":"What are software documentations?","text":"<p>Software documentation encompasses written text or illustrations that support a software project. It can range from comprehensive API documentation to high-level overviews, guides, and tutorials. Effective documentation plays a crucial role in assisting users and contributors by explaining how to utilize and contribute to a project, ensuring the software is accessible and maintainable.</p>"},{"location":"3.%20Productionizing/3.5.%20Documentations.html#why-do-you-need-to-create-documentations","title":"Why do you need to create documentations?","text":"<p>Documentation is pivotal for several reasons:</p> <ul> <li>Usability: It aids users in understanding how to interact with your software, detailing accepted inputs and expected outputs.</li> <li>Maintainability: Documentation facilitates new contributors in navigating the codebase and contributing efficiently, such as guiding through the process of submitting a pull request.</li> <li>Longevity: Projects with thorough documentation are more likely to be adopted, maintained, and enhanced over time, thanks to resources like troubleshooting sections.</li> <li>Quality Assurance: The process of writing documentation can uncover design issues or bugs, much like explaining a problem aloud can lead to solutions (e.g., through a data glossary).</li> </ul> <p>High-quality documentation encourages the use of your software and is valued by your users, while poor documentation can hinder developer productivity and deter users from engaging with your solution.</p>"},{"location":"3.%20Productionizing/3.5.%20Documentations.html#how-should-you-associate-documentations-to-your-code-base","title":"How should you associate documentations to your code base?","text":"<p>Documentation within Python code can be incorporated in three key places:</p> <ul> <li>Module documentation: This should be placed at the top of your Python module to provide an overview.</li> </ul> <pre><code>\"\"\"Define trainable machine learning models.\"\"\"\n</code></pre> <ul> <li>Function or method documentation: Use this to describe the purpose of functions or methods, their parameters, and return values.</li> </ul> <pre><code>def parse_file(path: str) -&gt; Config:\n    \"\"\"Parse a config file from a given path.\n\n    Args:\n        path (str): Path to the local config file.\n\n    Returns:\n        Config: Parsed representation of the config file.\n    \"\"\"\n    return oc.OmegaConf.load(path)\n</code></pre> <ul> <li>Class documentation: Here, you describe the purpose and parameters of a class.</li> </ul> <pre><code>class ParquetReader(Reader):\n    \"\"\"Reads a dataframe from a parquet file.\n\n    Attributes:\n        path (str): The local path to the dataset.\n    \"\"\"\n\n    path: str\n</code></pre> <p>Beyond in-code documentation, complementing it with external documentation (e.g., project organization guides or how to report a bug) is beneficial.</p>"},{"location":"3.%20Productionizing/3.5.%20Documentations.html#which-tool-format-and-convention-should-you-use-to-create-documentations","title":"Which tool, format, and convention should you use to create documentations?","text":"<p>For creating documentation, you have multiple tools, formats, and conventions at your disposal:</p> <ul> <li> <p>Tools:</p> <ul> <li>MkDocs: A fast, simple static site generator designed for project documentation, built with Python.</li> <li>pdoc: A tool and library for auto-generating API documentation for Python projects, best for API docs.</li> <li>Sphinx: A robust tool for creating detailed and beautiful documentation, popular within the Python community, albeit with a steeper setup curve.</li> </ul> </li> <li> <p>Formats:</p> <ul> <li>Markdown: A straightforward text format that converts to HTML, ideal for simpler docs.</li> <li>reStructuredText (reST): Offers more features than Markdown, widely used in Python documentation, especially with Sphinx.</li> </ul> </li> <li> <p>Convention:</p> <ul> <li>Numpy Style: Features a clear, structured format for documenting Python functions, classes, and modules, focusing on readability.</li> <li>Google Style: Known for its simplicity and ease of use in documenting Python code, emphasizing clarity and brevity.</li> <li>reStructuredText: Offers a comprehensive set of markup syntax and constructs, ideal for technical documentation that requires detailed structuring and cross-referencing.</li> </ul> </li> </ul> <p>For best practices, choose a tool and format that align with your project's needs and complexity. Adopting a widely recognized convention can facilitate consistency and comprehension across your documentation. Generating a simple API documentation can be as simple as calling a tool like <code>pdoc</code> with an input and output directory:</p> <pre><code>$ uv run pdoc --docformat=google --output-directory=docs/ src/bikes\n</code></pre> <p>You can also use your IDE or some extensions like autoDocstring to automate the documentation generation process.</p>"},{"location":"3.%20Productionizing/3.5.%20Documentations.html#is-there-some-frameworks-for-organizing-good-documentation","title":"Is there some frameworks for organizing good documentation?","text":"<p>Diataxis is a framework that offers a systematic approach to crafting technical documentation, recognizing four distinct documentation needs: tutorials, how-to guides, technical references, and explanations. It suggests organizing documentation to align with these needs, ensuring users find the information they're looking for efficiently.</p> <p></p>"},{"location":"3.%20Productionizing/3.5.%20Documentations.html#what-are-best-practices-for-writing-your-project-documentation","title":"What are best practices for writing your project documentation?","text":"<ol> <li>Clarity and Conciseness: Strive for clear, straightforward documentation, avoiding complex language or unnecessary technical jargon.</li> <li>Consistent Style: Maintain a uniform style and format throughout all documentation to enhance readability.</li> <li>Keep It Updated: Regularly revise documentation to reflect the latest changes and additions to your software.</li> <li>Use Examples: Provide practical examples to illustrate how different parts of your software operate.</li> <li>Accessibility: Ensure your documentation is accessible to all users, including those with disabilities.</li> <li>Feedback Loop: Encourage and incorporate feedback on your documentation to continuously improve its quality.</li> <li>Multilingual Support: If possible, offer documentation in multiple languages to cater to a wider audience.</li> <li>Searchable: Implement search functionality to allow users to quickly locate relevant information within your documentation.</li> <li>Versioning: If your software has multiple versions, provide corresponding documentation for each version.</li> </ol>"},{"location":"3.%20Productionizing/3.5.%20Documentations.html#documentation-additional-resources","title":"Documentation additional resources","text":"<ul> <li>Documentation example from the MLOps Python Package</li> <li>Di\u00e1taxis: A systematic approach to technical documentation authoring</li> </ul>"},{"location":"3.%20Productionizing/3.6.%20VS%20Code%20Workspace.html","title":"3.6. VS Code Workspace","text":""},{"location":"3.%20Productionizing/3.6.%20VS%20Code%20Workspace.html#what-are-vs-code-workspaces","title":"What are VS Code Workspaces?","text":"<p>VS Code Workspaces are powerful configurations that allow you to manage and work on multiple projects within Visual Studio Code efficiently. A workspace can include one or more folders that are related to a particular project or task. Workspaces in VS Code enable you to save your project's settings, debug configurations, and extensions separately from your global VS Code settings, providing a customized and consistent development environment across your team.</p>"},{"location":"3.%20Productionizing/3.6.%20VS%20Code%20Workspace.html#why-should-you-use-vs-code-workspace","title":"Why should you use VS Code Workspace?","text":"<p>Using a VS Code Workspace offers several benefits:</p> <ul> <li>Shared Settings: Workspaces allow you to define common settings across all developers working on the project, ensuring consistency in coding conventions and reducing setup time for new contributors.</li> <li>Productivity: They enable developers to quickly switch contexts between different projects, each with its own tailored settings and preferences, thus enhancing productivity and focus.</li> <li>Customization: Workspaces provide the flexibility to customize the development environment according to the specific needs of a project without affecting global settings, making it easier to work on projects with different requirements.</li> </ul>"},{"location":"3.%20Productionizing/3.6.%20VS%20Code%20Workspace.html#how-should-you-create-and-open-your-vs-code-workspace","title":"How should you create and open your VS Code Workspace?","text":"<p>To create and open a VS Code Workspace:</p> <ol> <li>Create a Workspace: Open VS Code and go to <code>File</code> &gt; <code>Save Workspace As...</code> to save your current folder as a workspace. Provide a name for your workspace file, which will have a <code>.code-workspace</code> extension.</li> <li>Add Folders: You can add multiple project folders to your workspace by going to <code>File</code> &gt; <code>Add Folder to Workspace...</code> This is useful for grouping related projects.</li> <li>Open a Workspace: To open an existing workspace, go to <code>File</code> &gt; <code>Open Workspace...</code> and select the <code>.code-workspace</code> file you wish to open.</li> </ol>"},{"location":"3.%20Productionizing/3.6.%20VS%20Code%20Workspace.html#which-configurations-can-you-pass-to-your-workspace-configuration","title":"Which configurations can you pass to your workspace configuration?","text":"<p>VS Code Workspace configurations support the same settings as user settings but apply only within the context of the workspace. These configurations can be specified in the <code>.code-workspace</code> file, allowing you to customize various aspects of your development environment. For example:</p> <pre><code>{\n    \"folders\": [\n        {\n            \"path\": \".\"\n        }\n    ],\n    \"settings\": {\n        \"editor.formatOnSave\": true,\n        \"python.defaultInterpreterPath\": \".venv/bin/python\",\n        \"python.testing.pytestEnabled\": true,\n        \"python.testing.pytestArgs\": [\n            \"tests\"\n        ],\n        \"[python]\": {\n            \"editor.codeActionsOnSave\": {\n                \"source.organizeImports\": true\n            },\n            \"editor.defaultFormatter\": \"charliermarsh.ruff\",\n        }\n    },\n    \"extensions\": {\n        \"recommendations\": [\n            \"charliermarsh.ruff\",\n            \"dchanco.vsc-invoke\",\n            \"ms-python.mypy-type-checker\",\n            \"ms-python.python\",\n            \"ms-python.vscode-pylance\",\n            \"redhat.vscode-yaml\"\n        ]\n    }\n}\n</code></pre> <p>This configuration example sets up a project with:</p> <ul> <li>Folders: Defines the project folders included in the workspace.</li> <li>Settings: Customizes editor behavior, such as format on save, default Python interpreter path, and testing configurations specific to Python.</li> <li>Extensions: Recommends extensions beneficial for the project, promoting consistency across the development team.</li> </ul> <p>By tailoring workspace settings, you ensure that every team member has a consistent development environment, which can include specific code formatting rules, linter settings, and extensions that are automatically recommended upon opening the workspace.</p>"},{"location":"3.%20Productionizing/3.6.%20VS%20Code%20Workspace.html#how-can-you-configure-your-vs-code-user-settings-to-become-more-productive","title":"How can you configure your VS Code User Settings to become more productive?","text":"<p>This article from the MLOps Community: How to configure VS Code for AI, ML and MLOps development in Python \ud83d\udee0\ufe0f\ufe0f provide a in-depth guide for configuring VS Code environments. It starts by listing extensions that augments your programming environment. Then, the article shares some settings and keybindings that enhances your development experience. Finally, it provides some tips and tricks to boost your coding efficiency with VS Code.</p>"},{"location":"3.%20Productionizing/3.6.%20VS%20Code%20Workspace.html#vs-code-workspace-additional-resources","title":"VS Code Workspace additional resources","text":"<ul> <li>VS Code Workspace example from the MLOps Python Package</li> <li>What is a VS Code \"workspace\"?</li> </ul>"},{"location":"4.%20Validating/index.html","title":"4.0 Validating","text":"<p>In the fast-paced world of Machine Learning Operations (MLOps), ensuring the quality and integrity of code is crucial for the development of scalable, efficient, and reliable ML systems. This course chapter focuses on essential code validation techniques that bolster the robustness of ML pipelines, from typing to debugging, to elevate code quality and operational efficiency.</p> <ul> <li>4.0. Typing: Learn the significance of typing in Python for early error detection and improved code reliability.</li> <li>4.1. Linting: Explore how linting tools help maintain a clean codebase by highlighting stylistic errors and potential bugs.</li> <li>4.2. Testing: Understand the crucial role of testing in verifying code functionality and ensuring ML models perform as expected.</li> <li>4.3. Logging: Discover the importance of logging for monitoring ML systems in production and aiding in troubleshooting.</li> <li>4.4. Security: Examine strategies for identifying and mitigating security vulnerabilities in your ML applications.</li> <li>4.5. Formatting: Grasp how consistent code formatting enhances readability and collaboration among team members.</li> <li>4.6. Debugging: Delve into debugging practices to efficiently identify and solve issues within your ML codebase.</li> </ul>"},{"location":"4.%20Validating/4.0.%20Typing.html","title":"4.0. Typing","text":""},{"location":"4.%20Validating/4.0.%20Typing.html#what-is-programming-typing","title":"What is programming typing?","text":"<p>Typing in programming involves designating specific data types for variables, functions, and classes within a programming language. This concept is critical for managing how data is stored, processed, and interacted within software applications.</p> <p>Programming languages are categorized into three main types based on how they handle typing:</p> <ul> <li>Static typing: In statically typed languages, the data type of a variable is known at compile time, which means that type checking is done during the compilation of the program. Examples include Java, C++, and Haskell. This approach allows for early detection of type-related errors, contributing to more robust and error-resistant code.</li> <li>Dynamic typing: Dynamically typed languages determine the type of a variable at runtime. This flexibility allows for more rapid development but can introduce type-related errors that are harder to detect early in the development process. Examples of dynamically typed languages are Ruby, JavaScript, PHP, and Python.</li> <li>Gradual typing: Gradual typing offers a blend of static and dynamic typing, allowing developers to choose when to enforce type constraints. This approach provides the flexibility of dynamic typing while still enabling the benefits of static type checking where it's most useful. Languages that support gradual typing include TypeScript, Dart, and Python (from version 3.5 onwards with type annotations).</li> </ul> <p>Additionally, languages can have either a weak or strong type system:</p> <ul> <li>Weak typing: In languages with weak typing, type coercion is common, allowing for more flexibility in operations between different types but at the risk of unexpected behavior or errors (e.g., 1 + \"s\" =&gt; \"1s\").</li> <li>Strong typing: Strongly typed languages enforce stricter rules about interactions between data types, reducing the chances of runtime errors due to unexpected type conversions but requiring more explicit declarations and conversions by the developer (e.g., 1 + \"s\" =&gt; error, str(1) + \"s\" = \"1s\").</li> </ul>"},{"location":"4.%20Validating/4.0.%20Typing.html#why-is-typing-useful-in-programs","title":"Why is typing useful in programs?","text":"<p>The role of typing in programming, especially in complex or large-scale projects, is invaluable for several reasons:</p> <ul> <li>Early Bug Detection: Typing helps in identifying potential type-related issues at the early stages of development, preventing bugs that could become costly and complex to resolve later.</li> <li>Enhanced Code Clarity: Type annotations clarify the expected data types for function inputs and outputs, making the code more readable and understandable.</li> <li>Improved Development Workflow: Adopting typing encourages a disciplined coding practice, resulting in fewer errors and enhanced code quality.</li> <li>Facilitates Collaboration: In team settings, clear type annotations ensure that all members understand the data structures and function interfaces, leading to more effective collaboration.</li> <li>Integration with IDEs: Advanced IDEs utilize type hints to offer superior code completion, error highlighting, and refactoring capabilities.</li> </ul> <p>Although specifying types requires additional effort, this investment significantly improves the codebase's quality.</p>"},{"location":"4.%20Validating/4.0.%20Typing.html#what-is-the-relation-between-python-and-typing","title":"What is the relation between Python and typing?","text":"<p>Python is primarily recognized as a strong and dynamically typed language, allowing programmers to write code without specifying types explicitly. This approach is straightforward but may not be scalable for larger projects. Since Python 3.5, the language has supported gradual typing, enabling developers to annotate types. This feature enhances code clarity and aids in error prevention, especially during development.</p> <p>For instance, a simple function without type annotations in Python might look like this:</p> <pre><code>def print_n_times(message, n):\n    for _ in range(n):\n        print(message)\n</code></pre> <p>However, for better clarity and to take advantage of gradual typing, the same function with type annotations would be:</p> <pre><code>def print_n_times(message: str, n: int) -&gt; None:\n    for _ in range(n):\n        print(message)\n</code></pre> <p>Incorporating type annotations is highly recommended for the benefits they bring in terms of code clarity and early error detection, except in some cases where the effort might not justify the value.</p> <p>It's important to note that Python types are checked during development time, meaning they're used to verify the program's logic and flow rather than affecting runtime performance or optimization.</p> <p>To dive deeper into Python typing, exploring resources such as the Mypy cheatsheet and Python's built-in typing module is beneficial.</p>"},{"location":"4.%20Validating/4.0.%20Typing.html#is-it-possible-to-provide-types-for-a-dataframe","title":"Is it possible to provide types for a dataframe?","text":"<p>It's possible to provide types for dataframes using the Pandera library. Pandera offers a flexible and expressive API for validating data in dataframe-like objects, enhancing the readability and robustness of data processing pipelines.</p> <p>Pandera allows for:</p> <ol> <li>Defining a schema once and validating different dataframe types, including pandas, dask, modin, and pyspark.pandas.</li> <li>Checking the types and properties of columns in a pandas DataFrame or values in a pandas Series.</li> <li>Performing complex statistical validations, such as hypothesis testing.</li> <li>Integrating seamlessly with data analysis and processing pipelines through function decorators.</li> <li>Using a class-based API for dataframe models, similar to pydantic, and validating dataframes with typing syntax.</li> <li>Synthesizing data from schema objects for property-based testing.</li> <li>Validating dataframes lazily to execute all validation rules before raising an error.</li> <li>Integrating with a rich ecosystem of Python tools like pydantic, fastapi, and mypy.</li> </ol> <p>Here's an example schema for validating a dataframe in an MLOps codebase:</p> <pre><code>import pandera as pa\nimport pandera.typing as papd\nimport pandera.typing.common as padt\n\nclass InputsSchema(pa.DataFrameModel):\n    \"\"\"Schema for the project inputs.\"\"\"\n\n    instant: papd.Index[padt.UInt32] = pa.Field(ge=0, check_name=True)\n    dteday: papd.Series[padt.DateTime] = pa.Field()\n    season: papd.Series[padt.UInt8] = pa.Field(isin=[1, 2, 3, 4])\n    yr: papd.Series[padt.UInt8] = pa.Field(ge=0, le=1)\n    mnth: papd.Series[padt.UInt8] = pa.Field(ge=1, le=12)\n    hr: papd.Series[padt.UInt8] = pa.Field(ge=0, le=23)\n    holiday: papd.Series[padt.Bool] = pa.Field()\n    weekday: papd.Series[padt.UInt8] = pa.Field(ge=0, le=6)\n    workingday: papd.Series[padt.Bool] = pa.Field()\n    weathersit: papd.Series[padt.UInt8] = pa.Field(ge=1, le=4)\n    temp: papd.Series[padt.Float16] = pa.Field(ge=0, le=1)\n    atemp: papd.Series[padt.Float16] = pa.Field(ge=0, le=1)\n    hum: papd.Series[padt.Float16] = pa.Field(ge=0, le=1)\n    windspeed: papd.Series[padt.Float16] = pa.Field(ge=0, le=1)\n    casual: papd.Series[padt.UInt32] = pa.Field(ge=0)\n    registered: papd.Series[padt.UInt32] = pa.Field(ge=0)\n</code></pre>"},{"location":"4.%20Validating/4.0.%20Typing.html#is-it-possible-to-provide-better-types-for-classes","title":"Is it possible to provide better types for classes?","text":"<p>Pydantic enhances the native class syntax by validating class attributes and providing a cleaner, more efficient syntax.</p> <p>Features of Pydantic include:</p> <ul> <li>Validation and serialization powered by type hints, integrating seamlessly with IDEs and static analysis tools.</li> <li>High performance due to core validation logic written in Rust.</li> <li>Capability to emit JSON Schema for easy integration with other tools.</li> <li>Support for both strict and lax modes for data validation.</li> <li>Validation for many standard library types, including dataclasses and TypedDicts.</li> <li>Extensive customization options for validators and serializers.</li> <li>A rich ecosystem of integrations with popular libraries like FastAPI and SQLModel.</li> <li>Reliability proven by widespread use across various industries and projects.</li> </ul> <p>Example usage in an MLOps codebase:</p> <pre><code>import pydantic as pdt\n\nclass GridCVSearcher(pdt.BaseModel):\n    \"\"\"Grid searcher with cross-fold validation for better model performance metrics.\"\"\"\n\n    n_jobs: int | None = None\n    refit: bool = True\n    verbose: int = 3\n    error_score: str | float = \"raise\"\n    return_train_score: bool = False\n</code></pre>"},{"location":"4.%20Validating/4.0.%20Typing.html#how-can-you-check-your-types-with-python","title":"How can you check your types with Python?","text":"<p>Mypy is the primary tool for type checking in Python, providing command-line and IDE integration options.</p> <pre><code>uv add --group checkers mypy\nuv run mypy src/ tests/\n</code></pre> <p>Faster alternatives to mypy include:</p> <ul> <li>pyright: Static Type Checker for Python. MIT, Microsoft</li> <li>pyre-check: Performant type-checking for python. MIT, Meta</li> <li>pytype:  A static type analyzer for Python code. Apache-2, Google</li> </ul> <p>Compared to other alternatives, Mypy supports additional plugins as we are doing to see below.</p>"},{"location":"4.%20Validating/4.0.%20Typing.html#how-can-you-configure-mypy-to-improve-your-validation-workflow","title":"How can you configure mypy to improve your validation workflow?","text":"<p>To enhance your validation workflow, you can configure mypy in your project's <code>pyproject.toml</code>. Before committing code, it's advisable to run mypy across your codebase to ensure type correctness. You can ignore the <code>.mypy_cache/</code> folders generated by mypy by adding them to your <code>.gitignore</code>.</p> <p>Example mypy configuration in <code>pyproject.toml</code>:</p> <pre><code>[tool.mypy]\n# improve error messages\npretty = true\n# specify the python version\npython_version = \"3.12\"\n# check untyped definitions\ncheck_untyped_defs = true\n# all missing imports in code\nignore_missing_imports = true\n# enable additional mypy plugins\nplugins = [\"pandera.mypy\", \"pydantic.mypy\"]\n</code></pre> <p>If you need to ignore mypy for entire file or single line, you can add the following comment:</p> <pre><code>def func(a: int, b: int) -&gt; bool:  # type: ignore[empty-body]\n    pass\n</code></pre> <p>More configuration options are available in the mypy documentation.</p>"},{"location":"4.%20Validating/4.0.%20Typing.html#what-are-the-best-practices-for-providing-types-in-python","title":"What are the best practices for providing types in Python?","text":"<ul> <li>Follow the 80-20 rule: Focus on annotating types where it brings the most benefit.</li> <li>Familiarize yourself with the typing module: for further understanding Python types.</li> <li>Use implicit typing judiciously, as not all variables require explicit annotations.</li> <li>Employ <code>typing.Any</code> sparingly when specific types are not necessary or known.</li> <li>Leverage tools like mypy for continuous type checking during development.</li> </ul>"},{"location":"4.%20Validating/4.0.%20Typing.html#typing-additional-resources","title":"Typing additional resources","text":"<ul> <li>Typing configuration from the MLOps Python Package</li> <li>Typing example from the MLOps Python Package</li> <li>Make your MLOps code base SOLID with Pydantic and Python\u2019s ABC</li> <li>Python Type Checking (Guide)</li> </ul>"},{"location":"4.%20Validating/4.1.%20Linting.html","title":"4.1. Linting","text":""},{"location":"4.%20Validating/4.1.%20Linting.html#what-is-software-linting","title":"What is software linting?","text":"<p>Linting involves utilizing a tool to analyze your code for errors and discrepancies against standard coding conventions. Its primary aim is to identify syntax and stylistic issues, along with other potential programming errors that may have been accidentally included.</p>"},{"location":"4.%20Validating/4.1.%20Linting.html#why-should-you-use-linter-tools","title":"Why should you use linter tools?","text":"<p>Linters are indispensable in Python development due to several key reasons:</p> <ul> <li>Code Quality: They ensure adherence to best coding practices, crucial for teamwork.</li> <li>Readability: By enforcing a consistent style, they enhance the readability of the code.</li> <li>Early Bug Detection: They spot potential bugs early, reducing debugging and testing time.</li> <li>Learning and Improvement: They are great for developers, particularly novices, to learn and adhere to best practices, thus refining their coding skills.</li> </ul>"},{"location":"4.%20Validating/4.1.%20Linting.html#which-tools-should-you-use-to-lint-your-code","title":"Which tools should you use to lint your code?","text":"<p>Ruff is a fast, modern linting tool that provides instant feedback, essential for efficient development workflows. The main benefit of ruff is its speed that outpaces many other linters, facilitating a quick fix cycle that doesn't hinder workflow. It also enforces a wide range of linting rules for code quality and consistency. There is also a VS Code Extension for ruff you can use to get linting message while you are typing you code.</p> <pre><code># ruff installation (one shot)\nuv add --group checkers ruff\n# ruff code base linting\nuv run ruff src/ tests/\n</code></pre> <p>Remember to exclude the <code>.ruff_cache/</code> directory in your <code>.gitignore</code> file to prevent the cache folder from being committed.</p> <p>While there are other linting tools like pylint and pyflakes, ruff is highlighted for its speed and comprehensive rule set.</p>"},{"location":"4.%20Validating/4.1.%20Linting.html#how-should-you-configure-your-linting-tools","title":"How should you configure your linting tools?","text":"<p>Configure your linting tools within the <code>pyproject.toml</code> file to tailor their checks to your codebase's specific requirements:</p> <pre><code>[tool.ruff]\n# automatic fix when possible\nfix = true\n# define the default indent width\nindent-width = 4\n# define the default line length\nline-length = 100\n# define the default python version\ntarget-version = \"py312\"\n\n[tool.ruff.lint.per-file-ignores]\n# exceptions for docstrings in tests\n\"tests/*.py\" = [\"D100\", \"D103\"]\n</code></pre> <p>If necessary, you can exclude linting rules either in your <code>pyproject.toml</code> file or inline if they are not relevant for your project:</p> <pre><code># ignore the error code F401 for the line below\nfrom abc.xyz import function_name  # noqa: F401\n</code></pre>"},{"location":"4.%20Validating/4.1.%20Linting.html#what-are-the-best-practices-for-linting-code","title":"What are the best practices for linting code?","text":"<ol> <li>Integrate with Development Workflow: Make linting a staple of your development routine. Set up your IDE or code editor for immediate feedback.</li> <li>Customize Rules as Needed: Tailor the linting rules to meet your project's unique needs, although default settings are a solid starting point.</li> <li>Regular Linting Sessions: Promote frequent linting to prevent the accumulation of issues.</li> <li>Code Reviews and Linting: Use linting reports during code reviews to ensure compliance with coding standards.</li> <li>Educate Team Members: Ensure everyone understands the value of linting and knows how to use the tools efficiently.</li> <li>Balance Between Strictness and Flexibility: Enforce rules on error-prone patterns strictly but allow flexibility for personal coding styles, provided they do not compromise code quality.</li> </ol>"},{"location":"4.%20Validating/4.1.%20Linting.html#linting-additional-resources","title":"Linting additional resources","text":"<ul> <li>Linting configuration from the MLOps Python Package</li> <li>Ruff Tutorial</li> <li>Best of Python Developer Tools</li> </ul>"},{"location":"4.%20Validating/4.2.%20Testing.html","title":"4.2. Testing","text":""},{"location":"4.%20Validating/4.2.%20Testing.html#what-are-software-tests","title":"What are software tests?","text":"<p>Software tests are a set of automated procedures used to ensure that software behaves as intended. They play a critical role in maintaining software reliability, functionality, and in preventing regressions.</p> <p>Tests can be categorized based on their complexity and scope:</p> <ul> <li>Unit Test: Focuses on individual components or functions, verifying that each part works in isolation. For example, testing a single function that calculates the sum of two numbers.</li> <li>Regression Test: Ensures that previously developed and tested software still performs correctly after it has been changed or interfaced with other software. This type of test is crucial for identifying unintended side effects of updates.</li> <li>End-to-End Test: Simulates real user scenarios from start to finish, ensuring the system as a whole operates as expected. It's the most comprehensive form of testing, covering the interaction between various parts of the software and external systems.</li> </ul>"},{"location":"4.%20Validating/4.2.%20Testing.html#why-are-tests-important-in-python-projects","title":"Why are tests important in Python projects?","text":"<p>Testing is indispensable in Python development for various reasons:</p> <ul> <li>Quality Assurance: Confirms that the software fulfills its intended requirements and functions correctly.</li> <li>Regression Prevention: Aids in preventing regressions, where updates unintentionally alter or break existing features.</li> <li>Refactoring Confidence: Empowers developers to refactor and enhance code with the assurance that they won't unknowingly disrupt existing functionality.</li> <li>Serve Documentation: Acts as practical documentation that clarifies how the code is meant to operate.</li> </ul> <p>While developers often use print statements for debugging, testing offers more durable assurances. Once a behavior is validated through tests, it can be continuously verified at each code change, unlike print statements which require manual developer intervention. Testing is especially critical in dynamic languages like Python, where compilers cannot validate the program before execution.</p>"},{"location":"4.%20Validating/4.2.%20Testing.html#which-tool-should-you-use-to-run-python-tests","title":"Which tool should you use to run Python tests?","text":"<p>Although Python includes the <code>unittest</code> module, it tends to be verbose. We suggest using pytest, a contemporary framework that simplifies writing small, clear tests, and scales well for complex application and library testing:</p> <pre><code># content of tests/test_sample.py\ndef inc(x):\n    return x + 1\n\ndef test_answer():\n    assert inc(3) == 4 # assert the function matches the intended behavior\n</code></pre> <p>To run <code>pytest</code> on your code base for behavior validation:</p> <pre><code># pytest installation (one-time)\nuv add pytest\n# pytest execution\nuv run pytest tests/\n</code></pre> <p>You can enhance <code>pytest</code> with additional plugins:</p> <ul> <li>pytest-cov: This plugin generates coverage reports, helping identify untested parts of the codebase:</li> </ul> <pre><code># generate a coverage report\npytest --cov=src/ tests/\n</code></pre> <ul> <li>pytest-xdist: Enables parallel test execution, utilizing all available CPU cores to speed up the testing process:</li> </ul> <pre><code># run pytest using all computer cores\npytest -n auto tests/\n</code></pre>"},{"location":"4.%20Validating/4.2.%20Testing.html#how-should-you-configure-your-code-base-for-testing","title":"How should you configure your code base for testing?","text":"<p>To prevent committing <code>pytest</code> cache files to git, add <code>.pytest_cache</code> to your <code>.gitignore</code> file.</p> <p>VS Code natively supports <code>pytest</code>. Activate this feature in your <code>*.code-workspace</code> file with the following settings:</p> <pre><code>{\n    \"settings\": {\n        \"python.testing.pytestEnabled\": true,\n        \"python.testing.pytestArgs\": [\n            \"tests\"\n        ]\n    }\n}\n</code></pre> <p>Adjust <code>pytest</code> configuration globally in your <code>pyproject.toml</code> file:</p> <pre><code>[tool.coverage.run]\nbranch = true  # report coverage by branch (if)\nsource = [\"src\"]  # set the default source folder\nomit = [\"__main__.py\"]  # exclude certain files from coverage report\n\n[tool.pytest.ini_options]\naddopts = \"--verbosity=2\"  # increase the verbosity level\npythonpath = [\"src\"]  # set the default python path\n</code></pre>"},{"location":"4.%20Validating/4.2.%20Testing.html#how-should-you-structure-your-test-structures","title":"How should you structure your test structures?","text":"<p>Organize your tests in a dedicated <code>tests</code> folder, mirroring the structure of your project. For each module in your project, create a corresponding test file in the <code>tests</code> folder, naming it after the module but prefixed with <code>test_</code>. For instance, tests for a module named <code>models.py</code> should be in a file named <code>test_models.py</code>.</p> <pre><code>src/\n    bikes/\n        models.py\n        metrics.py\n        datasets.py\ntests/\n    test_models.py\n    test_metrics.py\n    test_datasets.py\n</code></pre> <p>Begin each test function with <code>test_</code> to clearly indicate its purpose. This organization helps in maintaining clarity and ease of navigation within the test suite. You can also separate each test case in 3 steps: Given, When, Then:</p> <pre><code>def test_inputs_schema(inputs_reader: datasets.Reader) -&gt; None:\n    # given\n    schema = schemas.InputsSchema\n    # when\n    data = inputs_reader.read()\n    # then\n    assert schema.check(data) is not None, \"Inputs data should be valid!\"\n</code></pre>"},{"location":"4.%20Validating/4.2.%20Testing.html#how-can-you-define-reusable-test-components","title":"How can you define reusable test components?","text":"<p><code>pytest</code> introduces the concept of fixtures, powerful tools for setting up objects that can be reused across multiple tests. You can define fixtures either in the module where they're used or in a <code>tests/conftest.py</code> file to share them throughout your tests. This is particularly useful in MLOps code bases, where you might not want to retrain models or reload datasets before each test. Here's how you can define and use fixtures:</p> <pre><code># in conftest.py\nimport pytest\nimport os\n\n@pytest.fixture(scope=\"session\")\ndef tests_path() -&gt; str:\n    \"\"\"Return the path of the tests folder.\"\"\"\n    file_path = os.path.abspath(__file__)\n    parent_directory = os.path.dirname(file_path)\n    return parent_directory\n\n@pytest.fixture(scope=\"function\")\ndef tmp_outputs_path(tmp_path: str) -&gt; str:\n    \"\"\"Return a tmp path for the outputs dataset.\"\"\"\n    return os.path.join(tmp_path, \"outputs.parquet\")\n</code></pre> <p>These fixtures, especially when utilized in a shared <code>conftest.py</code>, facilitate setting up a common testing environment across the code base, ensuring efficiency and consistency. Fixtures with a <code>session</code> scope will be used across all executions, while <code>function</code> scope will be refreshed before each test.</p>"},{"location":"4.%20Validating/4.2.%20Testing.html#how-can-you-avoid-repetition-in-your-test-scenarios","title":"How can you avoid repetition in your test scenarios?","text":"<p>To minimize repetition in tests and cover a variety of scenarios, <code>pytest.mark.parametrize</code> allows you to run the same test function with different sets of arguments. This approach is akin to calling the function multiple times with various inputs:</p> <pre><code>import pytest\n\n@pytest.mark.parametrize( \"name, interval, greater_is_better\",\n    [\n        (\"mean_squared_error\", [0, float(\"inf\")], False),\n        (\"mean_absolute_error\", [0, float(\"inf\")], False),\n    ], )\ndef test_sklearn_metric( name: str,\n    interval: list,\n    greater_is_better: bool ) -&gt; None:\n    # Example test body\n    assert name in [\"mean_squared_error\", \"mean_absolute_error\"]\n    assert isinstance(interval, list)\n    assert isinstance(greater_is_better, bool)\n</code></pre> <p>In this modified example, <code>test_sklearn_metric</code> will execute twice, verifying the function behaves correctly across both scenarios. This demonstrates how <code>pytest.mark.parametrize</code> effectively broadens test coverage with minimal code duplication.</p>"},{"location":"4.%20Validating/4.2.%20Testing.html#how-can-you-validate-the-output-and-exceptions-of-your-program","title":"How can you validate the output and exceptions of your program?","text":"<p><code>pytest</code> offers several out-of-the-box fixtures for thorough testing, including output capture and exception testing. Here's how you can validate program output and handle exceptions:</p> <pre><code>import json\nimport pytest\n\ndef test_json_print(capsys) -&gt; None:\n    # Example setup (replace with actual command execution)\n    print(json.dumps({\"key\": \"value\"}))  # Simulated program output\n    captured = capsys.readouterr()\n    # Validate\n    assert captured.err == \"\", \"Captured error should be empty!\"\n    assert json.loads(captured.out), \"Captured output should be a valid JSON!\"\n\ndef test_main_no_configs() -&gt; None:\n    # given\n    argv = []\n    # when\n    with pytest.raises(RuntimeError) as error:\n        # Replace with actual function call that should raise an error\n        raise RuntimeError(\"No configs provided.\")\n    # then\n    assert \"No configs provided.\" in str(error.value), \"Expected RuntimeError was not raised!\"\n</code></pre> <p>These examples illustrate capturing program output to verify it's a valid JSON and ensuring the program raises the expected exception under certain conditions, enhancing the robustness of your testing strategy.</p>"},{"location":"4.%20Validating/4.2.%20Testing.html#is-it-easy-to-define-tests-for-mlops-code-bases","title":"Is it easy to define tests for MLOps code bases?","text":"<p>Testing MLOps code bases poses unique challenges compared to other types of code. You must deal with complex data structures, such as dataframes, and the inherent randomness in machine learning models. Furthermore, ML processes like model tuning can be time-consuming, requiring careful design of your tests to facilitate quicker iterations.</p> <p>Gaining proficiency in writing effective tests for ML code bases comes with experience. As you become more familiar with the nuances of your code and ML workflows, you'll develop strategies for efficient and meaningful testing. Ensuring your ML components are not the weakest link in your software applications is crucial, and a solid testing foundation will bolster confidence in your development process.</p>"},{"location":"4.%20Validating/4.2.%20Testing.html#what-are-best-practices-for-writing-unit-tests","title":"What are best practices for writing unit tests?","text":"<ol> <li>Write Readable and Clear Tests: Ensure your tests are straightforward and easy to understand, with descriptive names and necessary comments.</li> <li>Keep Tests Independent: Each test should run independently of others, allowing any order of execution.</li> <li>Use Fixtures for Setup and Teardown: Utilize fixtures for consistent setup and cleanup, reducing redundancy across tests.</li> <li>Regularly Run Your Tests: Integrate testing into your development and CI/CD workflows to catch issues early.</li> <li>Aim for High Test Coverage: Cover as much of your code as possible, especially critical paths, to ensure reliability.</li> <li>Keep Tests Fast: Optimize test speed to maintain efficiency in your development cycle.</li> <li>Review and Update Tests Regularly: As your codebase evolves, ensure your tests remain relevant and reflective of current functionalities.</li> <li>Test for Different Scenarios and Edge Cases: Beyond happy paths, test for potential failure modes and edge conditions to ensure robustness.</li> <li>Strive for at least 80% Code Coverage: This target helps ensure that the majority of your codebase is verified by tests, safeguarding against regressions and encourage a culture of quality.</li> </ol>"},{"location":"4.%20Validating/4.2.%20Testing.html#testing-additional-resources","title":"Testing additional resources","text":"<ul> <li>Testing configuration from the MLOps Python Package</li> <li>Testing modules from the MLOps Python Package</li> <li>Getting Started With Testing in Python</li> <li>Effective Python Testing With Pytest</li> <li>The Test Pyramid</li> <li>A Complete Guide to Pytest Fixtures</li> </ul>"},{"location":"4.%20Validating/4.3.%20Logging.html","title":"4.3. Logging","text":""},{"location":"4.%20Validating/4.3.%20Logging.html#what-is-software-logging","title":"What is software Logging?","text":"<p>Logging is a crucial process in software development, involving the recording of events, operations, and errors that occur within a software application. This process is instrumental in understanding how a program operates and in identifying any problems that may arise. Logging can be accomplished using dedicated logging modules provided by programming languages or through simple mechanisms like print statements.</p> <p>Every program has access to two primary channels for communication:</p> <ul> <li>Standard Output (stdout): This is used for general program output, such as results of operations or status messages. In Python, this can be accessed via <code>sys.stdout</code>.</li> <li>Standard Error (stderr): This channel is reserved for reporting errors and diagnostic messages. It ensures that error messages are kept separate from the main program output, allowing for easier debugging. In Python, <code>sys.stderr</code> is used for this purpose.</li> </ul> <p>To facilitate logging, developers can choose from various logging levels, allowing them to categorize messages by their severity or importance. These levels include:</p> <ul> <li>DEBUG: Detailed information, typically of interest only when diagnosing problems.</li> <li>INFO: Confirmation that things are working as expected.</li> <li>WARNING: An indication that something unexpected happened, or indicative of some problem in the near future (e.g., \u2018disk space low\u2019). The software is still working as expected.</li> <li>ERROR: Due to a more serious problem, the software has not been able to perform some function.</li> <li>CRITICAL: A serious error, indicating that the program itself may be unable to continue running.</li> </ul>"},{"location":"4.%20Validating/4.3.%20Logging.html#why-is-logging-important-in-python-projects","title":"Why is Logging Important in Python Projects?","text":"<p>Logging plays a vital role in both the development and production phases of software projects for several reasons:</p> <ul> <li>Debugging: It facilitates the identification and resolution of issues in the code.</li> <li>Monitoring: Logging allows for the ongoing observation of application behavior and performance, crucial for maintaining operational stability.</li> <li>Audit Trails: Logs serve as a historical record of events, which can be critical for tracing the sequence of actions leading up to a problem.</li> <li>Documentation: They act as a live form of documentation that details the runtime behavior of an application.</li> </ul> <p>Given the myriad of things that can go wrong during program execution \u2013 such as missing files, full disk space, or network connectivity issues \u2013 having detailed logs can make the difference between quickly resolving an issue and a prolonged downtime.</p>"},{"location":"4.%20Validating/4.3.%20Logging.html#which-tool-should-you-use-to-log-in-python","title":"Which tool should you use to log in Python?","text":"<p>Python's standard library includes a robust but somewhat complex <code>logging</code> module. For those seeking simplicity without sacrificing power, the <code>loguru</code> library is an excellent alternative. It streamlines the logging process with easier syntax and a host of convenient features, such as automatic file rotation and structured logging. Below is how you can install and use <code>loguru</code>:</p> <pre><code>from loguru import logger\n\n# logging example for the inference job\ndef run(self)\n    # services\n    logger = self.logger_service.logger()\n    logger.info(\"With logger: {}\", logger)\n    # inputs\n    logger.info(\"Read inputs: {}\", self.inputs)\n    inputs_ = self.inputs.read()  # unchecked!\n    inputs = schemas.InputsSchema.check(inputs_)\n    logger.debug(\"- Inputs shape: {}\", inputs.shape)\n    # model\n    logger.info(\"With model: {}\", self.mlflow_service.registry_name)\n    model_uri = registries.uri_for_model_alias_or_version(\n        name=self.mlflow_service.registry_name,\n        alias_or_version=self.alias_or_version,\n    )\n    logger.debug(\"- Model URI: {}\", model_uri)\n    # loader\n    logger.info(\"Load model: {}\", self.loader)\n    model = self.loader.load(uri=model_uri)\n    logger.debug(\"- Model: {}\", model)\n    # outputs\n    logger.info(\"Predict outputs: {}\", len(inputs))\n    outputs = model.predict(inputs=inputs)  # checked\n    logger.debug(\"- Outputs shape: {}\", outputs.shape)\n    # write\n    logger.info(\"Write outputs: {}\", self.outputs)\n    self.outputs.write(data=outputs)\n    # notify\n    self.alerts_service.notify(\n        title=\"Inference Job Finished\", message=f\"Outputs Shape: {outputs.shape}\"\n    )\n    return locals()\n</code></pre> <p>The above example demonstrates the simplicity of using <code>loguru</code> for logging across various severity levels, from debugging information to error reporting, directly to standard error (stderr) by default.</p> <p></p>"},{"location":"4.%20Validating/4.3.%20Logging.html#how-can-you-configure-loguru","title":"How can you configure loguru?","text":"<p>Configuring <code>loguru</code> in your Python applications is straightforward. You can adjust settings either directly in your code or through a configuration file. Here's an example of how <code>loguru</code> can be configured in a Python project:</p> <pre><code>import pydantic as pdt\n\nclass LoggerService(Service):\n    \"\"\"Service for logging messages.\n\n    https://loguru.readthedocs.io/en/stable/api/logger.html\n\n    Parameters:\n        sink (str): logging output.\n        level (str): logging level.\n        format (str): logging format.\n        colorize (bool): colorize output.\n        serialize (bool): convert to JSON.\n        backtrace (bool): enable exception trace.\n        diagnose (bool): enable variable display.\n        catch (bool): catch errors during log handling.\n    \"\"\"\n\n    sink: str = \"stderr\"\n    level: str = \"DEBUG\"\n    format: str = (\n        \"&lt;green&gt;[{time:YYYY-MM-DD HH:mm:ss.SSS}]&lt;/green&gt;\"\n        \"&lt;level&gt;[{level}]&lt;/level&gt;\"\n        \"&lt;cyan&gt;[{name}:{function}:{line}]&lt;/cyan&gt;\"\n        \" &lt;level&gt;{message}&lt;/level&gt;\"\n    )\n    colorize: bool = True\n    serialize: bool = False\n    backtrace: bool = True\n    diagnose: bool = False\n    catch: bool = True\n\n    @T.override\n    def start(self) -&gt; None:\n        loguru.logger.remove()\n        config = self.model_dump()\n        # use standard sinks or keep the original\n        sinks = {\"stderr\": sys.stderr, \"stdout\": sys.stdout}\n        config[\"sink\"] = sinks.get(config[\"sink\"], config[\"sink\"])\n        loguru.logger.add(**config)\n\n    def logger(self) -&gt; loguru.Logger:\n        \"\"\"Return the main logger.\n\n        Returns:\n            loguru.Logger: the main logger.\n        \"\"\"\n        return loguru.logger\n</code></pre> <p>This example outlines a configurable logging service, illustrating how <code>loguru</code> can be tailored to meet the specific needs of an application, from adjusting the logging level to formatting messages.</p>"},{"location":"4.%20Validating/4.3.%20Logging.html#what-are-best-practices-for-logging-in-python","title":"What are best practices for logging in Python?","text":"<ol> <li>Use Appropriate Logging Levels: Choose the right level for each log statement to ensure the log output is relevant and useful.</li> <li>Provide Contextual Information: Enhance log messages with context to make them more informative and easier to understand.</li> <li>Protect Sensitive Information: Avoid logging sensitive data to safeguard privacy and security.</li> <li>Maintain Consistent Formatting: Standardize log message formats to simplify parsing and analysis.</li> <li>Control Log File Size: Implement strategies like log rotation to manage file sizes and prevent excessive disk space usage.</li> <li>Embrace Asynchronous Logging: For performance-sensitive applications, consider asynchronous logging to minimize impact on application throughput.</li> <li>Centralize Logs for Distributed Systems: Aggregate logs from multiple sources to facilitate comprehensive monitoring and analysis.</li> <li>Utilize Structured Logging: Employ structured formats (e.g., JSON) for complex applications to make logs more queryable and machine-readable.</li> <li>Regularly Review Logging Configuration: Ensure your logging setup remains effective and adjust as necessary to capture critical information.</li> <li>Educate Your Team: Promote awareness and understanding of logging best practices among all team members to enhance the quality and consistency of application logs.</li> </ol> <p>Implementing these practices will help maximize the benefits of logging in your Python projects, providing clear insights into application behavior and aiding in rapid problem resolution.</p>"},{"location":"4.%20Validating/4.3.%20Logging.html#logging-additional-resources","title":"Logging additional resources","text":"<ul> <li>Logging example from the MLOps Python Package</li> <li>A Complete Guide to Logging in Python with Loguru</li> <li>Logging in Python</li> <li>Python Logging: A Stroll Through the Source Code</li> </ul>"},{"location":"4.%20Validating/4.4.%20Security.html","title":"4.4. Security","text":""},{"location":"4.%20Validating/4.4.%20Security.html#what-is-software-security","title":"What is software security?","text":"<p>Software security involves the implementation of protective measures and protocols to safeguard software from malicious attacks and other threats. It encompasses a range of practices designed to keep data safe, protect the integrity of software, and ensure that applications operate as intended without unauthorized access or modifications. Effective software security measures are critical in preventing data breaches, protecting user privacy, and maintaining the trustworthiness of software systems.</p>"},{"location":"4.%20Validating/4.4.%20Security.html#why-is-software-security-important","title":"Why is software security important?","text":"<p>Software security is crucial for several reasons:</p> <ul> <li>Protects Sensitive Data: Security measures help protect sensitive information from unauthorized access and theft, including personal user data, financial information, and intellectual property.</li> <li>Maintains User Trust: By ensuring the confidentiality, integrity, and availability of data, software security maintains and builds trust with users.</li> <li>Compliance with Regulations: Many industries have regulatory requirements for data protection and privacy, making security a legal obligation.</li> <li>Prevents Financial Loss: Security breaches can result in significant financial losses due to theft, legal liabilities, and damage to reputation.</li> <li>Ensures Continuity of Services: Robust security measures help ensure that software applications remain available and operational, preventing disruptions to business operations and services.</li> </ul>"},{"location":"4.%20Validating/4.4.%20Security.html#what-are-the-main-security-risks-in-python-and-mlops","title":"What are the main security risks in Python and MLOps?","text":"<ol> <li> <p>Vulnerable Dependencies: One of the primary security risks in Python projects arises from using libraries and dependencies that contain known vulnerabilities. Regularly updating these dependencies to their latest, secure versions is essential to mitigate this risk.</p> </li> <li> <p>Input Validation: Proper input validation is critical to protect against various forms of attacks, such as SQL injection, cross-site scripting (XSS), and command injection. Ensuring that inputs are validated, sanitized, and securely handled can significantly reduce security risks.</p> </li> <li> <p>Configuration and Secrets Management: Inadequately managed application configurations and secrets, such as API keys and database passwords, can expose sensitive information. Secure storage solutions, like environment variables, secret management services, and encrypted configurations, are vital to safeguard this information.</p> </li> </ol> <p>Despite these risks, the isolation common in MLOps projects provides some level of protection, especially from direct internet-based threats like denial of service (DoS) attacks or the exploitation of vulnerabilities. This isolation primarily benefits backend operations, though online inference systems accessible via the web still require rigorous security measures.</p>"},{"location":"4.%20Validating/4.4.%20Security.html#how-can-security-risks-be-reduced-in-python","title":"How can security risks be reduced in Python?","text":"<p>To mitigate security risks in Python, tools like Bandit offer automated solutions for identifying common security issues within codebases:</p> <pre><code># Bandit installation\nuv add --group checkers bandit\n# Running Bandit to lint the codebase\nuv run bandit src/\n</code></pre> <p>Configuring Bandit through <code>pyproject.toml</code> allows for customization of the linting process, tailoring it to specific project needs:</p> <pre><code>[tool.bandit]\ntargets = [\"src\"]\n</code></pre> <p>For detailed configuration options and best practices, the Bandit documentation is an invaluable resource.</p>"},{"location":"4.%20Validating/4.4.%20Security.html#how-can-security-risks-be-reduced-with-github","title":"How can security risks be reduced with GitHub?","text":"<p>Reducing security risks on GitHub involves leveraging its tools for dependency management and vulnerability scanning. GitHub's Dependabot is an automated tool that monitors your repositories for known vulnerabilities in dependencies and provides updates or patches. Configuring Dependabot and regular code audits can significantly enhance your project's security posture.</p> <p>To set up Dependabot in your GitHub repository, you can add a <code>.github/dependabot.yml</code> file with configurations tailored to your project's specific needs, specifying the frequency of checks, the directories to monitor, and the package managers to use.</p> <p>By staying vigilant with updates, practicing secure coding standards, and utilizing available tools, developers can significantly reduce the security risks associated with Python and MLOps projects.</p>"},{"location":"4.%20Validating/4.4.%20Security.html#security-additional-resources","title":"Security additional resources","text":"<ul> <li>Security configuration from the MLOps Python Package</li> </ul>"},{"location":"4.%20Validating/4.5.%20Formatting.html","title":"4.5. Formatting","text":""},{"location":"4.%20Validating/4.5.%20Formatting.html#what-is-software-formatting","title":"What is software formatting?","text":"<p>Software formatting refers to the process of organizing and structuring code in a way that makes it readable and maintainable. It involves adhering to a set of stylistic rules and conventions that cover aspects such as indentation, line length, variable naming, and the placement of braces and comments. Good formatting improves the clarity of the code, making it easier for developers to understand, navigate, and modify.</p>"},{"location":"4.%20Validating/4.5.%20Formatting.html#why-is-software-formatting-important","title":"Why is software formatting important?","text":"<ol> <li>Enhances Readability: Proper formatting makes code easier to read and understand. Consistent indentation and spacing help delineate code blocks, functions, and logical sections at a glance.</li> <li>Facilitates Collaboration: When working in a team, standardized formatting ensures that everyone can easily work with and understand each other's code, reducing the learning curve for new team members.</li> <li>Improves Maintainability: Well-formatted code is easier to debug, update, and extend. It helps developers identify structural and syntactic errors more quickly, and adhering to common conventions makes the code more predictable.</li> </ol> <p>Since the inception of programming, there have been several debates, such as tabs vs. spaces or single quote vs. double quote. Having common conventions ensures we can standardize the Python codebase without rehashing these debates.</p>"},{"location":"4.%20Validating/4.5.%20Formatting.html#is-there-a-convention-for-formatting-python-codebases","title":"Is there a convention for formatting Python codebases?","text":"<p>The default convention for formatting Python codebases is PEP 8. It provides guidelines on various formatting issues such as naming conventions, indentation, line length, and comments. Following PEP 8 helps Python developers maintain a uniform coding style, enhancing code readability and maintainability.</p> <p>Some conventions of PEP 8 might be adjusted to your environment (e.g., screen size). For instance, the default line length is 79, which may be a bit short for modern monitor widths. You can increase this setting to a more comfortable value, such as 100.</p>"},{"location":"4.%20Validating/4.5.%20Formatting.html#which-tools-should-you-use-to-format-a-codebase-in-python","title":"Which tools should you use to format a codebase in Python?","text":"<p>In recent years, <code>black</code> and <code>isort</code> have been the core tools for formatting Python codebases. Black specializes in code formatting, applying a consistent style to your Python code, while <code>isort</code> focuses on sorting and formatting import statements. However, <code>Ruff</code> offers a faster alternative that combines the benefits of both <code>black</code> and <code>isort</code>.</p> <p>You can format your codebase as follows:</p> <pre><code># ruff installation (one-time)\nuv add --group checkers ruff\n# format the imports\nuv run ruff check --select you --fix src/ tests/\n# format the sources\nuv run ruff format src/ tests/\n</code></pre>"},{"location":"4.%20Validating/4.5.%20Formatting.html#how-can-you-automate-the-formatting-of-your-codebase-as-you-type","title":"How can you automate the formatting of your codebase as you type?","text":"<p>Instead of formatting your codebase periodically from the command line, you can configure your code editor, such as VS Code, to automatically format code when you save a file.</p> <p>Below is an example configuration for your <code>[project].code-workspace</code> file to automatically format files using VS Code's Ruff extension:</p> <pre><code>{\n    \"settings\": {\n        \"editor.formatOnSave\": true,\n        \"[python]\": {\n            \"editor.codeActionsOnSave\": {\n                \"source.organizeImports\": true\n            },\n            \"editor.defaultFormatter\": \"charliermarsh.ruff\",\n        },\n    },\n    \"extensions\": {\n        \"recommendations\": [\n            \"charliermarsh.ruff\",\n        ]\n    }\n}\n</code></pre>"},{"location":"4.%20Validating/4.5.%20Formatting.html#should-you-change-the-default-formatting-proposed-by-the-tools","title":"Should you change the default formatting proposed by the tools?","text":"<p>We recommend keeping the default settings in most cases to avoid debates on how your codebase should be formatted, with few exceptions.</p> <p>In your <code>pyproject.toml</code>, you can configure Ruff formatting, such as the line length and docstring convention, to suit your project:</p> <pre><code>[tool.ruff]\nfix = true\nindent-width = 4\nline-length = 100\ntarget-version = \"py312\"\n\n[tool.ruff.format]\ndocstring-code-format = true\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n</code></pre>"},{"location":"4.%20Validating/4.5.%20Formatting.html#can-you-disable-change-or-suppress-formatting-in-some-code-location","title":"Can you disable change or suppress formatting in some code location?","text":"<p>Sometimes, you might want to change or disable formatting to improve the readability of your code, such as in unit tests. You can do this in two ways:</p> <ul> <li>Use soft instructions to force certain formatting, like adding a comma at the end of an enumeration to force one line per item:</li> </ul> <pre><code>items = {\n    \"a\": 1,\n    \"b\": 2,\n    \"c\": 3,\n}\n# With the last comma, this dict will not be formatted as a single line:\n# items = {\"a\": 1, \"b\": 2, \"c\": 3}\n</code></pre> <ul> <li>Use explicit instructions not to format a block of code with comments:</li> </ul> <pre><code># fmt: off\nnot_formatted=3\nalso_not_formatted=4\n# fmt: on\n</code></pre>"},{"location":"4.%20Validating/4.5.%20Formatting.html#formatting-additional-resources","title":"Formatting additional resources","text":"<ul> <li>Formatting configuration from the MLOps Python Package</li> <li>How to Write Beautiful Python Code With PEP 8</li> </ul>"},{"location":"4.%20Validating/4.6.%20Debugging.html","title":"4.6. Debugging","text":""},{"location":"4.%20Validating/4.6.%20Debugging.html#what-is-software-debugging","title":"What is software debugging?","text":"<p>Software debugging is the process of identifying, analyzing, and fixing bugs or errors in a software program. It involves running the program in a controlled environment, where you can inspect variables, step through code, set breakpoints, and understand the program's execution flow to find where things are going wrong.</p>"},{"location":"4.%20Validating/4.6.%20Debugging.html#why-is-software-debugging-important","title":"Why is software debugging important?","text":"<p>Debugging is crucial for several reasons:</p> <ol> <li>Ensures Code Correctness: It helps ensure that the software behaves as expected and meets its requirements.</li> <li>Improves Code Quality: Through debugging, developers can identify and correct underlying issues that might not be immediately apparent, leading to more robust and error-free code.</li> <li>Saves Time and Resources: Debugging can be a more efficient way to diagnose and fix problems compared to manually scrutinizing code or relying on scattered print statements. It allows for a systematic examination of the program\u2019s execution.</li> </ol> <p>Compared to print statements, debugging offers a quicker and more interactive alternative. You don't have to add print statements, execute your program, and then remove those statements. Instead, you can insert breakpoints and examine the code as much as you want without restarting the program.</p>"},{"location":"4.%20Validating/4.6.%20Debugging.html#what-is-the-benefit-of-debugging-over-logging","title":"What is the benefit of debugging over logging?","text":"<p>While logging provides valuable insights, especially in a production environment, debugging is a more dynamic tool during development to understand code behavior.</p> <p>The two approaches are complementary:</p> <ul> <li>Logging offers initial insights about code execution, helping to pinpoint where issues might occur.</li> <li>Debugging allows for an in-depth investigation of these issues through breakpoints and real-time inspection, without the need for code execution to complete.</li> </ul>"},{"location":"4.%20Validating/4.6.%20Debugging.html#which-tool-should-you-use-to-debug-your-code","title":"Which tool should you use to debug your code?","text":"<p>Python includes <code>pdb</code> (Python Debugger), a powerful but command-line-based tool that can be challenging for those not comfortable with CLI environments.</p> <p>For a more user-friendly experience, Integrated Development Environments (IDEs) like Visual Studio Code (VS Code) offer integrated debugging tools. These tools provide a graphical interface for debugging, making it easier to set breakpoints, step through code, and inspect variables.</p>"},{"location":"4.%20Validating/4.6.%20Debugging.html#how-should-you-debug-your-code-with-vs-code","title":"How should you debug your code with VS Code?","text":"<p>To start debugging in VS Code, follow these steps:</p> <ol> <li>Set a Breakpoint: Click to the left of the line number where you want the execution to pause. A red dot appears, indicating a breakpoint.</li> <li>Launch the Debugger: Open the Run and Debug sidebar (<code>Ctrl+Shift+D</code> on Windows/Linux, <code>Cmd+Shift+D</code> on macOS) and start a debugging session by selecting the appropriate configuration for your project and clicking the green play button.</li> <li>Control Execution: Use the debugging control panel to step over (execute the current line), step into (dive into functions called on the current line), or continue (resume execution until the next breakpoint or the end of the program).</li> <li>Inspect Variables: Hover over variables in the editor to see their current values. The Debug Sidebar also provides a comprehensive view of variables in scope.</li> </ol> <p>Additionally, you can use the Debug Console to execute commands and modify variables on the fly, offering an interactive environment to test fixes or understand behavior without altering the source code directly.</p>"},{"location":"4.%20Validating/4.6.%20Debugging.html#what-are-some-tips-for-improving-your-debugging-experience","title":"What are some tips for improving your debugging experience?","text":"<p>To enhance your debugging experience, consider the following tips:</p> <ul> <li>Use Conditional Breakpoints: Instead of stopping at every iteration of a loop, you can set conditions for breakpoints to pause execution only when certain conditions are met, saving time.</li> <li>Leverage Watch Expressions: Add expressions or variables to the watch list to monitor their values in real-time, helping to quickly identify when and how they change unexpectedly.</li> <li>Understand the Call Stack: The call stack shows you the path execution took to get to the current point. Analyzing it can help identify how different parts of the code interact and where errors propagate from.</li> <li>Experiment in the Debug Console: Use the debug console to test potential fixes or understand complex code behavior without needing to modify and rerun your program.</li> <li>Familiarize Yourself with Debugger Features: Spend time learning the features and shortcuts provided by your IDE's debugger. Efficiency in debugging often comes from knowing how to quickly navigate and use available tools.</li> </ul> <p>Mastering the debugger and incorporating these practices can significantly improve your efficiency in diagnosing and fixing issues, making the debugging process less daunting and more productive.</p>"},{"location":"4.%20Validating/4.6.%20Debugging.html#debugging-additional-resources","title":"Debugging additional resources","text":"<ul> <li>Python debugging in VS Code</li> <li>Python Debugging With Pdb</li> </ul>"},{"location":"5.%20Refining/index.html","title":"5. Refining","text":"<p>In this chapter, we delve into the critical processes and methodologies that enhance the efficiency, reliability, and scalability of MLOps projects. \"Refining\" encompasses a set of practices aimed at streamlining the development pipeline, from code formulation to deployment, ensuring that machine learning models are not only accurately developed but also seamlessly integrated into production environments. These practices are essential for maintaining code quality, automating repetitive tasks, and ensuring consistent environments for development and deployment, thereby reducing errors and increasing productivity.</p> <ul> <li>5.0. Design Patterns: Explore common architectural blueprints that solve recurring problems in software design and development within the MLOps ecosystem.</li> <li>5.1. Task Automation: Learn how to automate mundane and repetitive software development tasks to increase efficiency and reduce the likelihood of human error.</li> <li>5.2. Pre-Commit Hooks: Implement pre-commit hooks to automatically check and enforce code quality standards before code is committed, ensuring a clean and maintainable codebase.</li> <li>5.3. CI/CD Workflows: Discover how Continuous Integration and Continuous Deployment (CI/CD) workflows can be designed to automate the testing and deployment of machine learning models, ensuring rapid and reliable delivery.</li> <li>5.4. Software Containers: Understand the role of software containers in creating consistent environments for developing, testing, and deploying applications, simplifying the complexities of MLOps projects.</li> <li>5.5. AI/ML Experiments: Dive into managing and optimizing AI/ML experiments, focusing on tracking, comparison, and reproducibility of results to accelerate innovation.</li> <li>5.6. Model Registries: Examine how model registries are used to catalog and manage versions of machine learning models, facilitating model sharing, versioning, and deployment.</li> </ul>"},{"location":"5.%20Refining/5.0.%20Design%20Patterns.html","title":"5.0. Design Patterns","text":""},{"location":"5.%20Refining/5.0.%20Design%20Patterns.html#what-is-a-software-design-pattern","title":"What is a software design pattern?","text":"<p>Software design patterns are proven solutions to common problems encountered during software development. These patterns provide a template for how to solve a problem in a way that has been validated by other developers over time. The concept originated from architecture and was adapted to computer science to help developers design more efficient, maintainable, and reliable code. In essence, design patterns serve as blueprints for solving specific software design issues.</p>"},{"location":"5.%20Refining/5.0.%20Design%20Patterns.html#why-do-you-need-software-design-patterns","title":"Why do you need software design patterns?","text":"<ul> <li>Freedom of choice: In the realm of AI/ML, flexibility and adaptability are paramount. Design patterns enable solutions to remain versatile, allowing for the integration of various options and methodologies without locking into a single approach.</li> <li>Code Robustness: Python's dynamic nature demands discipline from developers to ensure code robustness. Design patterns provide a structured approach to coding that enhances stability and reliability.</li> <li>Developer productivity: Employing the right design patterns can significantly boost developer productivity. These patterns facilitate the exploration of diverse solutions, enabling developers to achieve more in less time and enhance the overall value of their projects.</li> </ul> <p>While Python's flexibility is one of its strengths, it can also lead to challenges in maintaining code robustness and reliability. Design patterns help to mitigate these challenges by improving code quality and leveraging proven strategies to refine and enhance the codebase.</p>"},{"location":"5.%20Refining/5.0.%20Design%20Patterns.html#what-are-the-top-design-patterns-to-know","title":"What are the top design patterns to know?","text":"<p>Design patterns are typically categorized into three types:</p>"},{"location":"5.%20Refining/5.0.%20Design%20Patterns.html#strategy-pattern-behavioral","title":"Strategy Pattern (Behavioral)","text":"<p>The Strategy pattern is crucial in MLOps for decoupling the objectives (what to do) from the methodologies (how to do it). For example, it allows for the interchange of different algorithms or frameworks (such as TensorFlow, XGBoost, or PyTorch) for model training without altering the underlying code structure. This pattern upholds the Open/Closed Principle, providing the flexibility needed to adapt to changing requirements, such as switching models or data sources based on runtime conditions.</p> <p></p>"},{"location":"5.%20Refining/5.0.%20Design%20Patterns.html#factory-pattern-creational","title":"Factory Pattern (Creational)","text":"<p>After establishing common interfaces, the Factory pattern plays a vital role in enabling runtime behavior modification of programs. It controls object creation, allowing for dynamic adjustments through external configurations. In MLOps, this translates to the ability to alter AI/ML pipeline settings without code modifications. Python's dynamic features, combined with utilities like Pydantic, facilitate the implementation of the Factory pattern by simplifying user input validation and object instantiation.</p> <p></p>"},{"location":"5.%20Refining/5.0.%20Design%20Patterns.html#adapter-pattern-structural","title":"Adapter Pattern (Structural)","text":"<p>The Adapter pattern is indispensable in MLOps due to the diversity of standards and interfaces in the field. It provides a means to integrate various external components, such as training and inference systems across different platforms (e.g., Databricks and Kubernetes), by bridging incompatible interfaces. This ensures seamless integration and the generalization of external components, allowing for smooth communication and operation between disparate systems.</p> <p></p>"},{"location":"5.%20Refining/5.0.%20Design%20Patterns.html#how-can-you-define-software-interfaces-with-python","title":"How can you define software interfaces with Python?","text":"<p>Python supports two primary methods for defining interfaces: Abstract Base Classes (ABC) and Protocols.</p> <p>ABCs utilize Nominal Typing to establish clear class hierarchies and relationships, such as a RandomForestModel being a subtype of a Model. This approach makes the connection between classes explicit:</p> <pre><code>from abc import ABC, abstractmethod\n\nimport pandas as pd\n\nclass Model(ABC):\n @abstractmethod\n    def fit(self, X: pd.DataFrame, y: pd.DataFrame) -&gt; None:\n        pass\n\n @abstractmethod\n    def predict(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        pass\n\nclass RandomForestModel(Model):\n    def fit(self, X: pd.DataFrame, y: pd.DataFrame) -&gt; None:\n        print(\"Fitting RandomForestModel...\")\n\n    def predict(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        print(\"Predicting with RandomForestModel...\")\n        return pd.DataFrame()\n\nclass SVMModel(Model):\n    def fit(self, X: pd.DataFrame, y: pd.DataFrame) -&gt; None:\n        print(\"Fitting SVMModel...\")\n\n    def predict(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        print(\"Predicting with SVMModel...\")\n        return pd.DataFrame()\n</code></pre> <p>Conversely, Protocols adhere to the Structural Typing principle, embodying Python's duck typing philosophy where a class is considered compatible if it implements certain methods, regardless of its place in the class hierarchy. This means a RandomForestModel is recognized as a Model by merely implementing the expected behaviors.</p> <pre><code>from typing import Protocol, runtime_checkable\nimport pandas as pd\n\n@runtime_checkable\nclass Model(Protocol):\n    def fit(self, X: pd.DataFrame, y: pd.DataFrame) -&gt; None:\n        ...\n\n    def predict(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        ...\n\nclass RandomForestModel:\n    def fit(self, X: pd.DataFrame, y: pd.DataFrame) -&gt; None:\n        print(\"Fitting RandomForestModel...\")\n\n    def predict(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        print(\"Predicting with RandomForestModel...\")\n        return pd.DataFrame()\n\nclass SVMModel:\n    def fit(self, X: pd.DataFrame, y: pd.DataFrame) -&gt; None:\n        print(\"Fitting SVMModel...\")\n\n    def predict(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        print(\"Predicting with SVMModel...\")\n        return pd.DataFrame()\n</code></pre> <p>Choosing between ABCs and Protocols depends on your project's needs. ABCs offer a more explicit, structured approach suitable for applications, while Protocols offer flexibility and are more aligned with library development.</p>"},{"location":"5.%20Refining/5.0.%20Design%20Patterns.html#how-can-you-better-validate-and-instantiate-your-objects","title":"How can you better validate and instantiate your objects?","text":"<p>Pydantic is a valuable tool for defining, validating, and instantiating objects according to specified requirements. It utilizes type annotations to ensure inputs meet predefined criteria, significantly reducing the risk of errors in data-driven operations, such as in MLOps processes.</p>"},{"location":"5.%20Refining/5.0.%20Design%20Patterns.html#validating-objects-with-pydantic","title":"Validating Objects with Pydantic","text":"<p>Pydantic utilizes Python's type hints to validate data, ensuring that the objects you create adhere to your specifications from the get-go. This feature is particularly valuable in MLOps, where data integrity is crucial for the success of machine learning models. Here's how you can leverage Pydantic for object validation:</p> <pre><code>from typing import Optional\nfrom pydantic import BaseModel, Field\n\nclass RandomForestClassifierModel(BaseModel):\n    n_estimators: int = Field(default=100, gt=0)\n    max_depth: Optional[int] = Field(default=None, gt=0, allow_none=True)\n    random_state: Optional[int] = Field(default=None, gt=0, allow_none=True)\n\n# Instantiate the model with validated parameters\nmodel = RandomForestClassifierModel(n_estimators=120, max_depth=5, random_state=42)\n</code></pre> <p>In this example, Pydantic ensures that <code>n_estimators</code> is greater than 0, <code>max_depth</code> is either greater than 0 or <code>None</code>, and similarly for <code>random_state</code>. This kind of validation is essential for maintaining the integrity of your model training processes.</p>"},{"location":"5.%20Refining/5.0.%20Design%20Patterns.html#streamlining-object-instantiation-with-discriminated-union","title":"Streamlining Object Instantiation with Discriminated Union","text":"<p>Pydantic's Discriminated Union feature further simplifies object instantiation, allowing you to dynamically select a class based on a specific attribute (e.g., <code>KIND</code>). This approach can serve as an efficient alternative to the traditional Factory pattern, reducing the need for boilerplate code:</p> <pre><code>from typing import Literal, Union\nfrom pydantic import BaseModel, Field\n\nclass Model(BaseModel):\n    KIND: str\n\nclass RandomForestModel(Model):\n    KIND: Literal[\"RandomForest\"]\n    n_estimators: int = 100\n    max_depth: int = 5\n    random_state: int = 42\n\nclass SVMModel(Model):\n    KIND: Literal[\"SVM\"]\n    C: float = 1.0\n    kernel: str = \"rbf\"\n    degree: int = 3\n\n# Define a Union of model configurations\nModelKind = RandomForestModel | SVMModel\n\nclass Job(BaseModel):\n    model: ModelKind = Field(..., discriminator=\"KIND\")\n\n# Initialize a job from configuration\nconfig = {\n    \"model\": {\n        \"KIND\": \"RandomForest\",\n        \"n_estimators\": 100,\n        \"max_depth\": 5,\n        \"random_state\": 42,\n    }\n}\njob = Job.model_validate(config)\n</code></pre> <p>This pattern not only makes the instantiation of objects based on dynamic input straightforward but also ensures that each instantiated object is immediately validated against its respective schema, further enhancing the robustness of your application.</p> <p>Incorporating these practices into your MLOps projects can significantly improve the reliability and maintainability of your code, ensuring that your machine learning pipelines are both efficient and error-resistant.</p>"},{"location":"5.%20Refining/5.0.%20Design%20Patterns.html#design-pattern-additional-resources","title":"Design pattern additional resources","text":"<ul> <li>Design pattern examples from the MLOps Python Package</li> <li>Stop Building Rigid AI/ML Pipelines: Embrace Reusable Components for Flexible MLOps</li> <li>We need POSIX for MLOps</li> <li>Become the maestro of your MLOps abstractions</li> <li>Make your MLOps code base SOLID with Pydantic and Python\u2019s ABC</li> <li>Design Patterns in Machine Learning Code and Systems</li> <li>Python Protocols: Leveraging Structural Subtyping</li> </ul>"},{"location":"5.%20Refining/5.1.%20Task%20Automation.html","title":"5.1. Task Automation","text":""},{"location":"5.%20Refining/5.1.%20Task%20Automation.html#what-is-task-automation","title":"What is task automation?","text":"<p>Task automation refers to the process of automating repetitive and manual command-line tasks using software tools. This enables tasks to be performed with minimal human intervention, increasing efficiency and accuracy. A common example of task automation in software development is the use of <code>make</code>, a utility that automates the execution of predefined tasks like <code>configure</code>, <code>build</code>, and <code>install</code> within a project repository. By executing a simple command:</p> <pre><code>make configure build install\n</code></pre> <p>developers can streamline the compilation and installation process of software projects, saving time and reducing the likelihood of errors.</p>"},{"location":"5.%20Refining/5.1.%20Task%20Automation.html#why-do-you-need-task-automation","title":"Why do you need task automation?","text":"<p>Task automation is essential for several reasons:</p> <ul> <li>Don't repeat yourself: Automating tasks helps in avoiding the repetition of similar tasks, ensuring that you spend your time on tasks that require your unique skills and insights.</li> <li>Share common actions: It enables teams to share a common set of tasks, ensuring consistency and reliability across different environments and among different team members.</li> <li>Avoid typing mistakes: Automation reduces the chances of errors that can occur when manually typing commands or performing repetitive tasks, leading to more reliable outcomes.</li> </ul> <p>Embracing task automation is a step towards improving efficiency for programmers. The initial effort in setting up automation pays off by saving time and reducing errors, making it a valuable practice in software development.</p>"},{"location":"5.%20Refining/5.1.%20Task%20Automation.html#which-tools-should-you-use-to-automate-your-tasks","title":"Which tools should you use to automate your tasks?","text":"<p>While <code>Make</code> is a ubiquitous and powerful tool for task automation, its syntax can be challenging due to its use of unique symbols (e.g., $*, $%, :=, ...) and strict formatting rules, such as the requirement for tabs instead of spaces. This complexity can make <code>Make</code> intimidating for newcomers.</p> <p>For those seeking a more approachable alternative, <code>PyInvoke</code> offers a simpler, Python-based syntax for defining and running tasks. Here is an example showcasing how to build a Python package (wheel file) using PyInvoke:</p> <pre><code>\"\"\"Package tasks of the project.\"\"\"\n\nfrom invoke.context import Context\nfrom invoke.tasks import task\n\nfrom . import cleans\n\n@task(pre=[cleans.dist])\ndef build(ctx: Context) -&gt; None:\n    \"\"\"Build the python package.\"\"\"\n    ctx.run(\"uv build --wheel\")\n\n\n@task(pre=[build], default=True)\ndef all(_: Context) -&gt; None:\n    \"\"\"Run all package tasks.\"\"\"\n</code></pre> <p>This example illustrates how tasks can be easily defined and automated using Python, making it accessible for those already familiar with the language. Developers can then execute the task from their terminal:</p> <pre><code># execute the build task\ninv build\n</code></pre>"},{"location":"5.%20Refining/5.1.%20Task%20Automation.html#how-can-you-configure-your-task-automation-system","title":"How can you configure your task automation system?","text":"<p>Configuring your task automation system with PyInvoke is straightforward. It can be installed as a Python dependency through:</p> <pre><code>uv add --group dev invoke\n</code></pre> <p>Then, to configure PyInvoke for your project, create an <code>invoke.yaml</code> file in your repository:</p> <pre><code># https://docs.pyinvoke.org/en/latest/index.html\n\nrun:\n  echo: true\nproject:\n  name: bikes\n  package: bikes\n  repository: bikes\n</code></pre> <p>This configuration file allows you to define general settings under <code>run</code> and project-specific variables under <code>project</code>. Detailed documentation and more configuration options can be found on PyInvoke's website.</p>"},{"location":"5.%20Refining/5.1.%20Task%20Automation.html#how-should-you-organize-your-tasks-in-your-project-folder","title":"How should you organize your tasks in your project folder?","text":"<p>For an MLOps project, it's advisable to organize tasks into categories and place them within a <code>tasks/</code> directory at the root of your repository. This directory can include files for different task categories such as cleaning, commits, container management, and more. Here's an example structure:</p> <ul> <li>tasks</li> <li>tasks/__init__.py</li> <li>tasks/checks.py</li> <li>tasks/cleans.py</li> <li>tasks/commits.py</li> <li>tasks/containers.py</li> <li>tasks/docs.py</li> <li>tasks/formats.py</li> <li>tasks/installs.py</li> <li>tasks/mlflow.py</li> <li>tasks/packages.py</li> <li>tasks/projects.py</li> </ul> <p>In the <code>tasks/__init__.py</code> file, you should import and add all task modules to a collection:</p> <pre><code>\"\"\"Task collections for the project.\"\"\"\nfrom invoke import Collection\nfrom . import checks, cleans, commits, containers, docs, formats, installs, mlflow, packages, projects\n\nns = Collection()\n\nns.add_collection(checks)\nns.add_collection(cleans)\nns.add_collection(commits)\nns.add_collection(containers)\nns.add_collection(docs)\nns.add_collection(formats)\nns.add_collection(installs)\nns.add_collection(mlflow)\nns.add_collection(packages)\nns.add_collection(projects, default=True)\n</code></pre> <p>Each module, like <code>checks</code>, can define multiple tasks. For example:</p> <pre><code> \"\"\"Check tasks of the project.\"\"\"\n\n# %% IMPORTS\n\nfrom invoke.context import Context\nfrom invoke.tasks import task\n\n# %% TASKS\n\n\n@task\ndef format(ctx: Context) -&gt; None:\n    \"\"\"Check the formats with ruff.\"\"\"\n    ctx.run(\"uv run ruff format --check src/ tasks/ tests/\")\n\n\n@task\ndef type(ctx: Context) -&gt; None:\n    \"\"\"Check the types with mypy.\"\"\"\n    ctx.run(\"uv run mypy src/ tasks/ tests/\")\n\n\n@task\ndef code(ctx: Context) -&gt; None:\n    \"\"\"Check the codes with ruff.\"\"\"\n    ctx.run(\"uv run ruff check src/ tasks/ tests/\")\n\n\n@task\ndef test(ctx: Context) -&gt; None:\n    \"\"\"Check the tests with pytest.\"\"\"\n    ctx.run(\"uv run pytest --numprocesses=auto tests/\")\n\n\n@task\ndef security(ctx: Context) -&gt; None:\n    \"\"\"Check the security with bandit.\"\"\"\n    ctx.run(\"uv run bandit --recursive --configfile=pyproject.toml src/\")\n\n\n@task\ndef coverage(ctx: Context) -&gt; None:\n    \"\"\"Check the coverage with coverage.\"\"\"\n    ctx.run(\"uv run pytest --numprocesses=auto --cov=src/ --cov-fail-under=80 tests/\")\n\n\n@task(pre=[format, type, code, security, coverage], default=True)\ndef all(_: Context) -&gt; None:\n    \"\"\"Run all check tasks.\"\"\"\n   \"\"\"Run all check tasks.\"\"\"\n</code></pre> <p>These tasks can then be invoked from the command line as needed, providing a structured and efficient way to manage and execute project-related tasks.</p> <pre><code># run the code checker\ninv checks.code\n# run the code and format checker\ninv checks.code checks.format\n# run all the check tasks in the module\ninv checks\n</code></pre>"},{"location":"5.%20Refining/5.1.%20Task%20Automation.html#task-automation-additional-resources","title":"Task automation additional resources","text":"<ul> <li>Task automation examples from the MLOps Python Package</li> <li>PyInvoke Tutorial</li> <li>PyInvoke VS Code extension</li> </ul>"},{"location":"5.%20Refining/5.2.%20Pre-Commit%20Hooks.html","title":"5.2. Pre-Commit Hooks","text":""},{"location":"5.%20Refining/5.2.%20Pre-Commit%20Hooks.html#what-are-pre-commit-hooks","title":"What are pre-commit hooks?","text":"<p>Pre-commit hooks are automated checks that run before a commit or push is completed in your version control system. They serve as a first line of defense to ensure code quality and compliance with coding standards before the code is shared with the team or integrated into the main codebase. These hooks can run a variety of tasks, from syntax checks and code formatting to more complex operations like static analysis.</p>"},{"location":"5.%20Refining/5.2.%20Pre-Commit%20Hooks.html#why-do-you-need-pre-commit-hooks","title":"Why do you need pre-commit hooks?","text":"<ul> <li>Share clean code: Pre-commit hooks help maintain a high code quality by enforcing coding standards and identifying issues early.</li> <li>Fix common issues: By catching common issues before commit, pre-commit hooks save time and effort in debugging and fixing problems that could have been easily avoided.</li> <li>Avoid failed CI/CD jobs: Running checks before committing reduces the likelihood of CI/CD jobs failing due to minor issues that could have been caught early.</li> </ul> <p>Compared to CI/CD workflows, pre-commit hooks have the advantage of running locally on your computer, which makes them faster and easier to debug. You should consider balancing your validation jobs between pre-commit hooks and CI/CD systems based on their unique benefits, leveraging pre-commit hooks for quick, local checks and CI/CD pipelines for more comprehensive tests.</p>"},{"location":"5.%20Refining/5.2.%20Pre-Commit%20Hooks.html#which-tool-should-you-use-to-setup-pre-commit-hooks","title":"Which tool should you use to setup pre-commit hooks?","text":"<p>The most widely used tool for managing pre-commit hooks is <code>pre-commit</code>. It provides a flexible framework for configuring and managing the hooks you want to run before a commit. The tool supports a wide range of hooks and can be easily integrated into any MLOps project.</p> <p>To install <code>pre-commit</code> as part of your uv project, use the following command:</p> <pre><code>uv add --group commits pre-commit\n</code></pre> <p>Then, create a <code>.pre-commit-config.yaml</code> file in your project directory with a configuration similar to the following, which outlines which hooks to run:</p> <pre><code># https://pre-commit.com\n# https://pre-commit.com/hooks.html\n\ndefault_language_version:\n  python: python3.12\nrepos:\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v5.0.0\n    hooks:\n      - id: check-added-large-files\n      - id: check-case-conflict\n      - id: check-merge-conflict\n      - id: check-toml\n      - id: check-yaml\n      - id: debug-statements\n      - id: end-of-file-fixer\n      - id: mixed-line-ending\n      - id: trailing-whitespace\n</code></pre> <p>To install the pre-commit hooks, use the following commands:</p> <pre><code>uv run pre-commit install --hook-type pre-push\nuv run pre-commit install --hook-type commit-msg\n</code></pre> <p>You can now automatically run your hooks before each commit or push, or trigger them manually with:</p> <pre><code>uv run pre-commit run\n</code></pre>"},{"location":"5.%20Refining/5.2.%20Pre-Commit%20Hooks.html#which-hooks-should-you-use-for-an-mlops-project","title":"Which hooks should you use for an MLOps project?","text":"<p>For an MLOps project utilizing Python, it's beneficial to configure your <code>pre-commit-config.yaml</code> to include hooks that validate both the quality and format of your code. Here\u2019s an example configuration that includes the <code>ruff</code> and <code>ruff-format</code> hooks for code quality and formatting, respectively, along with other useful checks:</p> <pre><code># https://pre-commit.com\n# https://pre-commit.com/hooks.html\n\ndefault_language_version:\n  python: python3.12\nrepos:\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v5.0.0\n    hooks:\n      - id: check-added-large-files\n      - id: check-case-conflict\n      - id: check-merge-conflict\n      - id: check-toml\n      - id: check-yaml\n      - id: debug-statements\n      - id: end-of-file-fixer\n      - id: mixed-line-ending\n      - id: trailing-whitespace\n  - repo: https://github.com/astral-sh/ruff-pre-commit\n    rev: v0.8.1\n    hooks:\n      - id: ruff\n      - id: ruff-format\n</code></pre> <p>Additional hooks are available at the pre-commit website, offering a wide range of checks for different needs.</p>"},{"location":"5.%20Refining/5.2.%20Pre-Commit%20Hooks.html#how-can-you-improve-your-commit-messages","title":"How can you improve your commit messages?","text":"<p>Commit messages play a crucial role in software development, offering insights into what changes have been made and why. To enhance the quality of your commit messages and ensure consistency across contributions, Commitizen, a Python tool, can be extremely helpful. It not only formats commit messages but also helps in converting these messages into a comprehensive CHANGELOG. Here's how you can leverage Commitizen to streamline your commit messages:</p> <p>To get started with Commitizen, you can install it using the following command:</p> <pre><code>uv add --group commits commitizen\n</code></pre> <p>This command adds Commitizen to your project as a development dependency, ensuring that it is available for formatting commit messages.</p> <p>Once installed, Commitizen offers several commands to assist with your commits:</p> <pre><code># display a guide to help you format your commit messages\nuv run cz info\n# bump your package version according to semantic versioning\nuv run cz bump\n# interactively create a new commit message following best practices\nuv run cz commit\n</code></pre> <p>These commands are designed to guide you through creating structured and informative commit messages, bumping your project version appropriately, and even updating your CHANGELOG automatically.</p> <p>To configure Commitizen to fit your project needs, you can set it up in your <code>pyproject.toml</code> file as shown below:</p> <pre><code>[tool.commitizen]\nname = \"cz_conventional_commits\" # Uses the conventional commits standard\ntag_format = \"v$version\" # Customizes the tag format\nversion_scheme = \"pep440\" # Follows the PEP 440 version scheme\nversion_provider = \"pep621\" # Uses standard pep for version management\nupdate_changelog_on_bump = true # Automatically updates the CHANGELOG when the version is bumped\n</code></pre> <p>Integrating Commitizen into your pre-commit workflow ensures that all commits adhere to a consistent format, which is crucial for collaborative projects. You can add it to your <code>.pre-commit-config.yaml</code> like this:</p> <pre><code> - repo: https://github.com/commitizen-tools/commitizen\n    rev: v4.0.0 # The version of Commitizen you're using\n    hooks:\n      - id: commitizen # Ensures your commit messages follow the conventional format\n      - id: commitizen-branch\n        stages: [push] # Optionally, enforce commit message checks on push\n</code></pre> <p>By incorporating Commitizen into your development workflow, you not only standardize commit messages across your project but also create a more readable and navigable project history. This practice is invaluable for team collaboration and can significantly improve the maintenance and understanding of your project over time.</p>"},{"location":"5.%20Refining/5.2.%20Pre-Commit%20Hooks.html#is-there-a-way-to-bypass-a-hook-validation","title":"Is there a way to bypass a hook validation?","text":"<p>There are occasions when bypassing a pre-commit hook is necessary, such as when you need to make a quick fix or are confident the commit does not introduce any issues. To bypass the pre-commit hooks, you can use the following commands:</p> <pre><code>git commit -m \"My message\" --no-verify\ngit push --no-verify\n</code></pre>"},{"location":"5.%20Refining/5.2.%20Pre-Commit%20Hooks.html#what-are-some-best-practices-to-set-up-hooks","title":"What are some best practices to set up hooks?","text":"<p>Implementing pre-commit hooks effectively involves considering both the development workflow and the specific needs of your project. Here are some best practices:</p> <ul> <li>Prioritize fast-executing hooks to maintain an interactive development workflow without significant delays.</li> <li>Review available hooks to tailor your pre-commit configuration to your project's technology stack and needs.</li> <li>Collaborate on hook selection by discussing with your team which hooks to enable, ensuring a consensus and uniform coding standards.</li> <li>Use fixed hook versions to avoid unexpected behavior from updates, aligning them with your project's versions.</li> <li>Run complex checks locally, such as unit tests, to catch issues before they reach the CI/CD pipeline, balancing the immediacy of pre-commit hooks with the thoroughness of CI/CD checks.</li> </ul>"},{"location":"5.%20Refining/5.2.%20Pre-Commit%20Hooks.html#pre-commit-hook-additional-resources","title":"Pre-commit hook additional resources","text":"<ul> <li>Pre-commit hook examples from the MLOps Python Package</li> <li>Pre-Commit website</li> </ul>"},{"location":"5.%20Refining/5.3.%20CI-CD%20Workflows.html","title":"5.3. CI/CD Workflows","text":""},{"location":"5.%20Refining/5.3.%20CI-CD%20Workflows.html#what-is-cicd","title":"What is CI/CD?","text":"<p>CI/CD stands for Continuous Integration and Continuous Deployment or Continuous Delivery. It's a method used in software development that focuses on automating the process of integrating code changes from multiple contributors into a single software project, as well as automating the delivery or deployment of the software to production environments. Continuous Integration involves automatically testing and building the software every time a team member commits changes to version control. Continuous Deployment or Delivery takes these integrated changes and automatically deploys them to production environments, ensuring a seamless flow from code commit to deployment.</p>"},{"location":"5.%20Refining/5.3.%20CI-CD%20Workflows.html#what-is-a-cicd-workflow","title":"What is a CI/CD workflow?","text":"<p>A CI/CD workflow refers to the automated sequence of steps that software undergoes from development to deployment. This workflow typically includes stages such as building the application, running automated tests to verify code quality and functionality, and deploying the code to a production or staging environment. The goal of a CI/CD workflow is to enable developers to release new changes to customers quickly and with high confidence by automating the release process.</p>"},{"location":"5.%20Refining/5.3.%20CI-CD%20Workflows.html#why-do-you-need-cicd-workflows","title":"Why do you need CI/CD workflows?","text":"<ul> <li>Guardrail the code base: CI/CD workflows act as a safeguard for the codebase, ensuring that all changes meet quality standards and pass all tests before merging. This protects the main branch from breaking changes, keeping the software in a deployable state.</li> <li>Automate publication tasks: They streamline the process of getting software from version control into the hands of users by automating the build, test, and deployment tasks. This automation reduces the manual work involved in software releases and speeds up the delivery process.</li> <li>Report code quality to others: CI/CD systems often integrate with other tools to provide visibility into the health of the codebase. They can generate reports on code quality, test coverage, and other metrics, making it easier to maintain high standards and improve the software over time.</li> </ul> <p>By automating these processes, a CI/CD system reduces the load on development teams, ensures a consistent level of quality, and facilitates faster, more reliable software releases.</p>"},{"location":"5.%20Refining/5.3.%20CI-CD%20Workflows.html#which-solution-should-you-use-for-your-cicd","title":"Which solution should you use for your CI/CD?","text":"<p>There are many CI/CD solutions available, each with its own set of features and integrations. GitHub Actions is one of the most popular and accessible solutions, especially for projects hosted on GitHub. It integrates deeply with GitHub repositories, offering a seamless experience for building, testing, and deploying software directly from GitHub.</p> <p>To get started with GitHub Actions for your CI/CD workflows, you'll need to:</p> <ol> <li>Create a <code>.github/workflows</code> directory in your repository if it doesn't already exist.</li> <li>Define your workflow in a YAML file within this directory. The file should specify triggers for the workflow, jobs to run, and steps within those jobs.</li> <li>Use predefined actions from the GitHub Marketplace or define your own steps to install dependencies, run tests, build your software, and deploy it.</li> </ol>"},{"location":"5.%20Refining/5.3.%20CI-CD%20Workflows.html#which-workflows-should-you-set-up-for-your-mlops-project","title":"Which workflows should you set up for your MLOps project?","text":"<p>For MLOps projects, setting up both Verification and Publication workflows is crucial to ensure the quality and seamless deployment of machine learning models and related software.</p>"},{"location":"5.%20Refining/5.3.%20CI-CD%20Workflows.html#verification-workflow","title":"Verification Workflow","text":"<p>This workflow runs on pull requests on any branch, verifying code quality and functionality before merging:</p> <pre><code>name: Check\non:\n  pull_request:\n    branches:\n      - '*'\nconcurrency:\n  cancel-in-progress: true\n  group: ${{ github.workflow }}-${{ github.ref }}\njobs:\n  checks:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - uses: ./.github/actions/setup\n      - run: uv sync --group=checks\n      - run: uv run invoke checks.format\n      - run: uv run invoke checks.type\n      - run: uv run invoke checks.code\n      - run: uv run invoke checks.security\n      - run: uv run invoke checks.coverage\n</code></pre> <p>Here is a breakdown of each attribute in the workflow:</p> <ul> <li>name: Sets the name of the workflow as \"Check\", identifying it in the GitHub Actions UI.</li> <li>on: Specifies the event that triggers the workflow, in this case, a pull request.</li> <li>concurrency: Manages how workflow runs are handled concurrently. If <code>cancel-in-progress</code> is set to <code>true</code>, any in-progress runs of the workflow will be canceled when a new run is triggered.</li> <li>jobs: Defines the jobs to be run as part of the workflow.<ul> <li>checks: Identifies a job within the workflow, named \"checks\".<ul> <li>runs-on: Specifies the type of virtual host machine to run the job on.</li> <li>steps: Lists the steps to be executed as part of the job.<ul> <li>- uses: actions/checkout@v4: Utilizes the <code>checkout</code> action to access the repository code within the job.</li> <li>- uses: ./.github/actions/setup: Applies a custom action located in the repository to set up the environment.</li> <li>- run: uv sync --group=checks: Executes the command to install dependencies specified under the \"checks\" group with uv.</li> <li>- run: uv run invoke checks.format: Invokes a task to check code formatting.</li> <li>- run: uv run invoke checks.type: Invokes a task to check type annotations.</li> <li>- run: uv run invoke checks.code: Invokes a task to check code quality.</li> <li>- run: uv run invoke checks.security: Invokes a task to check security vulnerabilities.</li> <li>- run: uv run invoke checks.coverage: Invokes a task to check test coverage.</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"5.%20Refining/5.3.%20CI-CD%20Workflows.html#publication-workflow","title":"Publication Workflow","text":"<p>This workflow is triggered by a release event and is responsible for publishing the software or model:</p> <pre><code>name: Publish\non:\n  release:\n    types:\n      - edited\n      - published\nenv:\n  DOCKER_IMAGE: ghcr.io/fmind/mlops-python-package\nconcurrency:\n  cancel-in-progress: true\n  group: publish-workflow\njobs:\n  pages:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - uses: ./.github/actions/setup\n      - run: uv sync --group=docs\n      - run: uv run invoke docs\n      - uses: JamesIves/github-pages-deploy-action@v4\n        with:\n          folder: docs/\n          branch: gh-pages\n  packages:\n    permissions:\n      packages: write\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - uses: ./.github/actions/setup\n      - run: uv sync --only-dev\n      - run: uv run invoke packages\n      - uses: docker/login-action@v3\n        with:\n          registry: ghcr.io\n          username: ${{ github.actor }}\n          password: ${{ secrets.GITHUB_TOKEN }}\n      - uses: docker/setup-buildx-action@v3\n      - uses: docker/build-push-action@v6\n        with:\n          push: true\n          context: .\n          cache-to: type=gha\n          cache-from: type=gha\n          tags: |\n            ${{ env.DOCKER_IMAGE }}:latest\n            ${{ env.DOCKER_IMAGE }}:${{ github.ref_name }}\n</code></pre> <p>This workflow details steps for deploying documentation and building, tagging, and pushing Docker images, ensuring that releases are systematically handled.</p>"},{"location":"5.%20Refining/5.3.%20CI-CD%20Workflows.html#how-can-you-avoid-repeating-some-steps-between-cicd-workflows","title":"How can you avoid repeating some steps between CI/CD workflows?","text":"<p>To avoid repetition and maintain consistency across workflows, common steps can be abstracted into reusable actions. By creating a <code>setup</code> action under <code>.github/actions</code>, you encapsulate common setup tasks:</p> <pre><code>name: Setup\ndescription: Setup for project workflows\nruns:\n  using: composite\n  steps:\n    - name: Install uv\n      uses: astral-sh/setup-uv@v4\n      with:\n        enable-cache: true\n    - name: Setup Python\n      uses: actions/setup-python@v5\n      with:\n        python-version-file: .python-version\n</code></pre> <p>This composite action can then be referenced in multiple workflows, ensuring a DRY (Don't Repeat Yourself) approach to CI/CD configuration.</p> <p>You can also find more actions to use in your workflows from GitHub Marketplace: https://github.com/marketplace?type=actions</p>"},{"location":"5.%20Refining/5.3.%20CI-CD%20Workflows.html#what-are-some-tips-and-tricks-for-using-cicd-workflows-for-mlops","title":"What are some tips and tricks for using CI/CD workflows for MLOps?","text":"<ul> <li>Automate Regular Tasks: Identify and automate manual tasks to increase efficiency and reduce the potential for human error.</li> <li>Master GitHub Actions Syntax: Understanding the capabilities and triggers available in GitHub Actions allows for more dynamic and responsive workflows.</li> <li>Use Concurrency Wisely: The <code>concurrency</code> attribute helps to manage workflow runs by canceling or queuing multiple runs, optimizing resource usage.</li> <li>Leverage GitHub CLI: The GitHub Command Line Interface can streamline workflow executions: <code>gh workflow run ...</code>.</li> <li>Implement Branch Protection: Enforcing branch protection rules ensures that merges can only occur after successful workflow completions, maintaining code quality and stability.</li> </ul>"},{"location":"5.%20Refining/5.3.%20CI-CD%20Workflows.html#ci-cd-workflow-additional-resources","title":"CI-CD Workflow additional resources","text":"<ul> <li>CI-CD Workflow example from the MLOps Python Package</li> <li>GitHub Actions documentation</li> </ul>"},{"location":"5.%20Refining/5.4.%20Software%20Containers.html","title":"5.4. Software Containers","text":""},{"location":"5.%20Refining/5.4.%20Software%20Containers.html#what-are-software-containers","title":"What are software containers?","text":"<p>Software containers are lightweight, stand-alone packages that include everything needed to run a piece of software, including the code, runtime, system tools, system libraries, and settings. Containers are isolated from each other and the host system, yet they share the OS kernel, making them more efficient and faster than traditional virtual machines. Containers ensure consistency across different development and deployment environments, addressing the \"it works on my machine\" headache.</p>"},{"location":"5.%20Refining/5.4.%20Software%20Containers.html#why-do-you-need-software-containers","title":"Why do you need software containers?","text":"<ul> <li>Package system dependencies: Containers encapsulate all dependencies required by an application, such as specific versions of libraries and other software, ensuring that it can run in any environment without issues.</li> <li>Share reusable software artifacts: Containers allow for the creation and sharing of reusable software artifacts. For instance, you can create a container image with a configured environment that can be shared with other developers, ensuring everyone works with the same setup.</li> <li>Make your package runnable from any system: Containers abstract away the underlying operating system, allowing applications to run uniformly across different environments. This is particularly useful in heterogeneous environments where systems may be running different versions of operating systems or have different configurations.</li> </ul> <p>While Python packages can be challenging to install across different systems, Docker ensures consistency between operating systems and environments. Running a Docker image is often more straightforward and faster than managing Python packages directly on the system.</p>"},{"location":"5.%20Refining/5.4.%20Software%20Containers.html#which-tool-should-you-use-for-creating-containers","title":"Which tool should you use for creating containers?","text":"<p>The most common tool for creating containers is Docker. Docker simplifies the process of building, running, and managing containers. It uses Dockerfile to automate the deployment of applications in lightweight and portable containers.</p> <p>To install Docker, follow these steps:</p> <ol> <li>Visit the Docker website and download the Docker Desktop application for your operating system.</li> <li>Follow the installation instructions provided on the website.</li> <li>Once installed, open a terminal or command prompt and verify the installation by running <code>docker --version</code>.</li> </ol> <p>Docker offers some paid services, such as Docker Hub private repositories and Docker Enterprise Edition, which provide additional features like enhanced security, user management, and automated image builds. It's important to contact your IT administrators to see if these solutions are available in your organization and to explore potential alternatives.</p>"},{"location":"5.%20Refining/5.4.%20Software%20Containers.html#which-tool-should-you-use-for-hosting-containers","title":"Which tool should you use for hosting containers?","text":"<p>Containers are software packages that must be hosted on a container registry. You can use either Docker Hub or GitHub Packages to host your containers.</p> <p>To use GitHub Packages, follow these steps:</p> <ol> <li>Create a GitHub account and sign in.</li> <li>Navigate to your repository where you want to host your container.</li> <li>In the repository, go to \"Packages\" and follow the instructions to publish your container image.</li> </ol> <p>You must then login to the system from the command-line and tag with image by adding <code>ghcr.io/[user]</code> at the beginning, such in: <code>ghcr.io/fmind/mlops-python-package</code>.</p>"},{"location":"5.%20Refining/5.4.%20Software%20Containers.html#which-image-should-you-create-for-your-mlops-projects","title":"Which image should you create for your MLOps projects?","text":"<p>To start using Docker for your MLOps projects, you can define a simple image in a <code>Dockerfile</code> in your project repository as follows:</p> <pre><code># https://docs.docker.com/engine/reference/builder/\n\nFROM ghcr.io/astral-sh/uv:python3.12-bookworm\nCOPY dist/*.whl .\nRUN uv pip install --system *.whl\nCMD [\"bikes\", \"--help\"]\n</code></pre> <p>You can then build and push this image with the following commands:</p> <pre><code>uv build --wheel # build the wheel file\ndocker build --tag=bikes:latest .\ndocker run --rm bikes:latest\n</code></pre>"},{"location":"5.%20Refining/5.4.%20Software%20Containers.html#which-tips-and-tricks-should-you-use-to-optimize-your-container-workflow","title":"Which tips and tricks should you use to optimize your container workflow?","text":"<ul> <li>Enable Multi-platform with docker buildx: Use Docker Buildx, a Docker CLI plugin, to build your images once and use them across multiple platforms. This is especially useful for creating images that need to run on both AMD64 and ARM architectures.</li> <li>Enable cache on your local laptop and CI/CD: Utilize Docker's caching mechanisms to speed up your builds. Configure your Dockerfile and CI/CD pipelines to reuse layers from previously built images, significantly reducing build time and bandwidth consumption.</li> <li>Clean up cache directories on your container image: After installing dependencies, ensure to remove cache directories and temporary files within your Dockerfile. This reduces the image size, making deployments faster and more efficient.</li> <li>Run container image linters like hadolint: Use tools like Hadolint to analyze your Dockerfile for best practices and common mistakes. This helps in optimizing your Dockerfile and ensuring security and efficiency.</li> <li>Install CUDA drivers if you need to use GPU: For projects requiring GPU acceleration, include CUDA drivers and libraries in your container images. This allows your applications to leverage GPU resources for compute-intensive tasks, essential for many machine learning and deep learning workflows.</li> </ul>"},{"location":"5.%20Refining/5.4.%20Software%20Containers.html#software-container-additional-resources","title":"Software container additional resources","text":"<ul> <li>Dockerfile example from the MLOps Python Package</li> <li>Containerize a Python application</li> <li>Docker in Visual Studio Code</li> </ul>"},{"location":"5.%20Refining/5.5.%20AI-ML%20Experiments.html","title":"5.5. AI/ML Experiments","text":""},{"location":"5.%20Refining/5.5.%20AI-ML%20Experiments.html#what-is-an-aiml-experiment","title":"What is an AI/ML experiment?","text":"<p>An AI/ML experiment is a structured process where data scientists and machine learning engineers systematically apply various algorithms, tune parameters, and manipulate datasets to develop predictive models. The goal is to find the most effective model that solves a given problem or enhances the performance of existing solutions. These experiments are iterative, involving trials of different configurations to evaluate their impact on model accuracy, efficiency, and other relevant metrics.</p>"},{"location":"5.%20Refining/5.5.%20AI-ML%20Experiments.html#why-do-you-need-aiml-experiment","title":"Why do you need AI/ML experiment?","text":"<ul> <li>Improve reproducibility: Reproducibility is crucial in AI/ML projects to ensure that experiments can be reliably repeated and verified by others. Experiment tracking helps in documenting the setup, code, data, and outcomes, making it easier to replicate results.</li> <li>Find the best hyperparameters: Hyperparameter tuning is a fundamental step in enhancing model performance. AI/ML experiments allow you to systematically test different hyperparameter configurations to identify the ones that yield the best results.</li> <li>Assign tags to organize your experiments: Tagging helps in categorizing experiments based on various criteria such as the type of model, dataset used, or the objective of the experiment. This organization aids in navigating and analyzing experiments efficiently.</li> <li>Track the performance improvement during a run: Continuous monitoring of model performance metrics during experiment runs helps in understanding the impact of changes and guiding further adjustments.</li> <li>Integrate with several AI/ML frameworks: Experiment tracking tools support integration with popular AI/ML frameworks, streamlining the experimentation process across different environments and tools.</li> </ul> <p>AI/ML experimentation is distinct in MLOps due to the inherent complexity and non-deterministic nature of machine learning tasks. Leveraging experiment tracking tools equips teams with a structured approach to manage this complexity, akin to how scientists document their research findings.</p>"},{"location":"5.%20Refining/5.5.%20AI-ML%20Experiments.html#which-aiml-experiment-solution-should-you-use","title":"Which AI/ML experiment solution should you use?","text":"<p>There are various AI/ML experiment tracking solutions available, each offering unique features. Major cloud providers like Google Cloud (Vertex AI), Azure (Azure ML) and AWS (SageMaker) offer integrated MLOps platforms that include experiment tracking capabilities. There are also vendor-specific tools such as Weights &amp; Biases and Neptune AI that specialize in experiment tracking. Among the open-source options, MLflow stands out as a robust and versatile choice for tracking experiments, integrating with a wide range of ML libraries and frameworks.</p> <p>To install MLflow, execute:</p> <pre><code>uv add mlflow\n</code></pre> <p>To verify the installation and start the MLflow server:</p> <pre><code>uv run mlflow doctor\nuv run mlflow server\n</code></pre> <p>For Docker Compose users, the following configuration can launch an MLflow server from a <code>docker-compose.yml</code> file:</p> <pre><code>services:\n  mlflow:\n    image: ghcr.io/mlflow/mlflow:v2.19.0\n    ports:\n      - 5000:5000\n    environment:\n      - MLFLOW_HOST=0.0.0.0\n    command: mlflow server\n</code></pre> <p>Run <code>docker compose up</code> to start the server.</p> <p>Further deployment details for broader access can be found in MLflow's documentation.</p>"},{"location":"5.%20Refining/5.5.%20AI-ML%20Experiments.html#how-should-you-configure-mlflow-in-your-project","title":"How should you configure MLflow in your project?","text":"<p>Configuring MLflow in your project enables efficient tracking of experiments. Initially, set the tracking and registry URIs to a local folder like <code>./mlruns</code>, and specify the experiment name. Enabling autologging will automatically record metrics, parameters, and models without manual instrumentation.</p> <pre><code>import mlflow\n\nmlflow.set_tracking_uri(\"file://./mlruns\")\nmlflow.set_registry_uri(\"file://./mlruns\")\nmlflow.set_experiment(experiment_name=\"Bike Sharing Demand Prediction\")\nmlflow.autolog()\n</code></pre> <p>To begin tracking, wrap your code in an MLflow run context, specifying details like the run name and description:</p> <pre><code>with mlflow.start_run(\n    run_name=\"Demand Forecast Model Training\",\n    description=\"Training with enhanced feature set\",\n    log_system_metrics=True,\n) as run:\n    # Your model training code here\n</code></pre>"},{"location":"5.%20Refining/5.5.%20AI-ML%20Experiments.html#which-information-can-you-track-in-an-aiml-experiment","title":"Which information can you track in an AI/ML experiment?","text":"<p>MLflow's autologging capability simplifies the tracking of experiments by automatically recording several information. You can complement autologging by manually logging additional information:</p> <ul> <li>Parameters with <code>mlflow.log_param()</code> for individual key-value pairs, or <code>mlflow.log_params()</code> for multiple parameters.</li> <li>Metrics using <code>mlflow.log_metric()</code> for single key-value metrics, capturing the evolution of metrics over time, or []<code>mlflow.log_metrics()</code>](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.log_metrics) for multiple metrics.</li> <li>Input datasets and context with <code>mlflow.log_input()</code>, including tags for detailed categorization.</li> <li>Tags for the active run through <code>mlflow.set_tag()</code> for single tags or <code>mlflow.set_tags()</code> for multiple tags.</li> <li>Artifacts such as files or directories with <code>mlflow.log_artifact()</code> or <code>mlflow.log_artifacts()</code> for logging multiple files.</li> </ul>"},{"location":"5.%20Refining/5.5.%20AI-ML%20Experiments.html#how-can-you-compare-aiml-experiments-in-your-project","title":"How can you compare AI/ML experiments in your project?","text":"<p>Comparing AI/ML experiments is crucial for identifying the most effective models and configurations. MLflow offers two primary methods for comparing experiments: through its web user interface (UI) and programmatically. Here's how you can use both methods:</p>"},{"location":"5.%20Refining/5.5.%20AI-ML%20Experiments.html#comparing-experiments-via-the-mlflow-web-ui","title":"Comparing Experiments via the MLflow Web UI","text":"<ol> <li> <p>Launch the MLflow Tracking Server: Start the MLflow tracking server if it isn't running already.</p> </li> <li> <p>Navigate to the Experiments Page: Navigate to the experiments page where all your experiments are listed.</p> </li> <li> <p>Select Experiments to Compare: Find the experiments you're interested in comparing and use the checkboxes to select them. You can select multiple experiments for comparison.</p> </li> <li> <p>Use the Compare Button: After selecting the experiments, click on the \"Compare\" button. This will take you to a comparison view where you can see the runs side-by-side.</p> </li> <li> <p>Analyze the Results: The comparison view will display key metrics, parameters, and other logged information for each run. Use this information to analyze the performance and characteristics of each model or configuration.</p> </li> </ol>"},{"location":"5.%20Refining/5.5.%20AI-ML%20Experiments.html#comparing-experiments-programmatically","title":"Comparing Experiments Programmatically","text":"<p>Comparing experiments programmatically offers more flexibility and can be integrated into your analysis or reporting tools.</p> <ol> <li>Search Runs: Use the <code>mlflow.search_runs()</code> function to query the experiments you want to compare. You can filter experiments based on experiment IDs, metrics, parameters, and tags. For example:</li> </ol> <pre><code>import mlflow\n\n# Assuming you know the experiment IDs or names\nexperiment_ids = [\"1\", \"2\"]\nruns_df = mlflow.search_runs(experiment_ids)\n</code></pre> <ol> <li> <p>Filter and Sort: Once you have the dataframe with runs, you can use pandas operations to filter, sort, and manipulate the data to focus on the specific metrics or parameters you're interested in comparing.</p> </li> <li> <p>Visualize the Comparison: For a more intuitive comparison, consider visualizing the results using libraries such as Matplotlib or Seaborn. For example, plotting the performance metrics of different runs can help in visually assessing which configurations performed better.</p> </li> </ol> <pre><code>import matplotlib.pyplot as plt\n\n# Example: Comparing validation accuracy of different runs\nplt.figure(figsize=(10, 6))\nfor _, row in runs_df.iterrows():\n    plt.plot(row['metrics.validation_accuracy'], label=f\"Run {row['run_id'][:7]}\")\n\nplt.title(\"Comparison of Validation Accuracy Across Runs\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Validation Accuracy\")\nplt.legend()\nplt.show()\n</code></pre> <p>These methods enable you to conduct thorough comparisons between different experiments, helping guide your decisions on model improvements and selections.</p>"},{"location":"5.%20Refining/5.5.%20AI-ML%20Experiments.html#what-are-some-tips-and-tricks-for-using-aiml-experiments","title":"What are some tips and tricks for using AI/ML experiments?","text":"<p>To maximize the efficacy of AI/ML experiments:</p> <ul> <li>Align logged information with relevant business metrics to ensure experiments are focused on meaningful outcomes.</li> <li>Use nested runs to structure experiments hierarchically, facilitating organized exploration of parameter spaces. <pre><code>with mlflow.start_run() as parent_run:\n    param = [0.01, 0.02, 0.03]\n\n    # Create a child run for each parameter setting\n    for p in param:\n        with mlflow.start_run(nested=True) as child_run:\n            mlflow.log_param(\"p\", p)\n            ...\n            mlflow.log_metric(\"val_loss\", val_loss)\n</code></pre></li> <li>Employ tagging extensively to enhance the searchability and categorization of experiments.</li> <li>Track detailed progress by logging steps and timestamps, providing insights into the evolution of model performance. <pre><code>mlflow.log_metric(key=\"train_loss\", value=train_loss, step=epoch, timestamp=now)\n</code></pre></li> <li>Regularly log models to the model registry for versioning and to facilitate deployment processes.</li> </ul>"},{"location":"5.%20Refining/5.5.%20AI-ML%20Experiments.html#ai-ml-experiment-additional-resources","title":"AI-ML Experiment additional resources","text":"<ul> <li>AI-ML Experiment integration from the MLOps Python Package</li> <li>MLflow Tracking</li> <li>Experiment Tracking with MLflow in 10 Minutes</li> <li>How We Track Machine Learning Experiments with MLFlow</li> </ul>"},{"location":"5.%20Refining/5.6.%20Model%20Registries.html","title":"5.6. Model Registries","text":""},{"location":"5.%20Refining/5.6.%20Model%20Registries.html#what-is-a-model-registry","title":"What is a model registry?","text":"<p>A model registry is a central storage and management system for machine learning models. It serves as a repository for models at various stages of their lifecycle, from development to deployment. This system supports version control, model tracking, and collaboration across teams, making it a crucial tool for efficient model management.</p>"},{"location":"5.%20Refining/5.6.%20Model%20Registries.html#why-do-you-need-a-model-registry","title":"Why do you need a model registry?","text":"<ul> <li>Storing and versioning AI/ML models: Model registries provide a structured way to store and manage multiple versions of models, allowing users to easily roll back to earlier versions if necessary.</li> <li>Track the origin of AI/ML models: They help track the development history of models, including who trained them, on what data, and the changes made over time.</li> <li>Make your model easily deployable: With a model registry, models can be moved smoothly from development to production, supporting continuous integration and delivery workflows.</li> <li>Promote models ready for production: Model registries facilitate the staging and promotion of models, ensuring only verified and tested models make it to production.</li> </ul> <p>Using a model registry acknowledges the unique nature of machine learning models, which are not just code, but include data, configuration, and dependencies.</p>"},{"location":"5.%20Refining/5.6.%20Model%20Registries.html#which-model-registry-solution-should-you-use","title":"Which model registry solution should you use?","text":"<p>There are various solutions available, ranging from cloud-based platforms like Google Vertex AI, AWS SageMaker, Azure ML. Third-party solutions such as Weights &amp; Biases and Neptune AI offer ready-to-use model management features. For those looking for an open-source option, MLflow Model Registry is popular and accessible, supporting a range of ML frameworks.</p> <p>To get started with MLflow, install it via uv:</p> <pre><code>uv add mlflow\n</code></pre> <p>Check its installation and start the server with:</p> <pre><code>uv run mlflow doctor\nuv run mlflow server\n</code></pre>"},{"location":"5.%20Refining/5.6.%20Model%20Registries.html#what-is-an-mlflow-model-and-a-registered-model","title":"What is an MLflow model and a registered model?","text":"<p>An MLflow Model is a model logged during an MLflow experiment, using methods such as <code>mlflow.&lt;model_flavor&gt;.log_model()</code>. After logging, these models can be registered in the MLflow Model Registry. A registered model in MLflow has a unique name and contains various versions, each with its own set of aliases, tags, and metadata.</p>"},{"location":"5.%20Refining/5.6.%20Model%20Registries.html#how-should-you-integrate-mlflow-registry-in-your-project","title":"How should you integrate MLflow Registry in your project?","text":"<p>The integration of MLflow Model Registry happens in 4 steps: Initializing, Saving, registry, and loading.</p>"},{"location":"5.%20Refining/5.6.%20Model%20Registries.html#initializing","title":"Initializing","text":"<p>Configure your MLflow environment before any operations:</p> <pre><code>import mlflow\nmlflow.set_tracking_uri(\"file://./mlruns\")\nmlflow.set_registry_uri(\"file://./mlruns\")\nclient.create_registered_model(\"bikes\")\n</code></pre> <p>This will store models and their metadata in the local <code>./mlruns</code> folder.</p>"},{"location":"5.%20Refining/5.6.%20Model%20Registries.html#saving","title":"Saving","text":"<p>Log your model using MLflow, either automatically with autologging or manually:</p> <pre><code>import mlflow\nwith mlflow.start_run(run_name=\"training\"):\n    model = ...  # your model training logic here\n    mlflow.sklearn.log_model(model, \"models\") # optional if you use autologging\n</code></pre>"},{"location":"5.%20Refining/5.6.%20Model%20Registries.html#registring","title":"Registring","text":"<p>Register your model to manage its lifecycle with this command following the saving step:</p> <pre><code>model_version = mlflow.register_model(name=\"bikes\", model_uri=model_info.model_uri)\n</code></pre>"},{"location":"5.%20Refining/5.6.%20Model%20Registries.html#loading","title":"Loading","text":"<p>Load a model for use in production or further testing:</p> <pre><code>model_uri = \"models:/bikes/1\"\nmodel = mlflow.sklearn.load_model(model_uri)\npredictions = model.predict(data)\n</code></pre>"},{"location":"5.%20Refining/5.6.%20Model%20Registries.html#how-can-you-sign-the-model-to-provide-its-inputs-and-outputs","title":"How can you sign the model to provide its inputs and outputs?","text":"<p>Create a model signature to clearly define what input the model expects and what output it produces:</p> <pre><code>import mlflow\nfrom mlflow.models.signature import infer_signature\ninputs, outputs = ...  # define your model inputs and outputs here\nsignature = infer_signature(inputs, outputs)\nmlflow.sklearn.log_model(model, artifact_path=\"models\", signature=signature, input_example=inputs)\n</code></pre>"},{"location":"5.%20Refining/5.6.%20Model%20Registries.html#how-should-you-access-the-models-in-the-registry","title":"How should you access the models in the registry?","text":"<p>Access models either through MLflow's UI for a visual interface or programmatically for automation and integration into other applications:</p> <pre><code>import mlflow\nclient = mlflow.tracking.MlflowClient()\nmodel_versions = client.search_model_versions(\"name='bikes'\")\nfor model_version in model_versions:\n    print(model_version)\n</code></pre>"},{"location":"5.%20Refining/5.6.%20Model%20Registries.html#how-should-you-mark-your-model-as-ready-to-production","title":"How should you mark your model as ready to production?","text":"<p>To effectively manage MLflow models, especially when marking a model as ready for production, using aliases is a highly practical approach. This method allows for a flexible handling of model versions, which can change frequently with new training runs or during model rollbacks.</p> <p>Model aliases in MLflow are mutable, named references that can be assigned to specific versions of a registered model. This allows you to reference a model version using a model URI or through the model registry API without constantly updating the version number. For example, you can create an alias named \"champion\" that points to version 1 of a model called \"MyModel.\" Subsequently, this model version can be referred to with the URI models:/MyModel@champion.</p>"},{"location":"5.%20Refining/5.6.%20Model%20Registries.html#assigning-an-alias","title":"Assigning an Alias","text":"<p>You can assign an alias through the MLflow UI or programmatically. Here\u2019s how you can do it programmatically:</p> <pre><code>from mlflow.tracking import MlflowClient\n\nclient = MlflowClient(tracking_uri=\"./mlruns\", registry_uri=\"./mlruns\")\nclient.set_registered_model_alias(name=\"bikes\", alias=\"Champion\", version=1)\n</code></pre>"},{"location":"5.%20Refining/5.6.%20Model%20Registries.html#retrieving-a-model-using-an-alias","title":"Retrieving a Model Using an Alias","text":"<p>To use the model assigned to an alias in your applications, you can retrieve it as follows:</p> <pre><code>import mlflow\n\nmodel_uri = \"models:/bikes@Champion\"\nmodel = mlflow.pyfunc.load_model(model_uri=model_uri)\npredictions = model.predict(inputs)\n</code></pre>"},{"location":"5.%20Refining/5.6.%20Model%20Registries.html#automating-alias-assignment","title":"Automating Alias Assignment","text":"<p>To automate the assignment of an alias to the latest model version, you can use:</p> <pre><code>versions = client.search_model_versions(\"name='bikes'\", max_results=1, order_by=[\"version_number DESC\"])\nlast_version = versions[0].version\nclient.set_registered_model_alias(name=\"bikes\", alias=\"Champion\", version=last_version)\n</code></pre>"},{"location":"5.%20Refining/5.6.%20Model%20Registries.html#how-can-you-rollback-your-model-if-something-goes-wrong","title":"How can you rollback your model if something goes wrong?","text":"<p>To rollback a model version in MLflow, you can reassign the alias to a previous stable version either through the UI or programmatically. Remember, after rolling back, you'll need to refresh the model loading in your application either manually or set it up to automatically detect changes.</p>"},{"location":"5.%20Refining/5.6.%20Model%20Registries.html#how-can-you-define-custom-logic-associated-with-your-model","title":"How can you define custom logic associated with your model?","text":"<p>MLflow supports a customizable model format called \"PyFunc\" (Python Function), which allows you to define custom logic that executes in conjunction with your model. Here's how you can utilize PyFunc to integrate custom logic with your model:</p> <pre><code>import mlflow.pyfunc\n\nclass CustomModel(mlflow.pyfunc.PythonModel):\n\n    def load_context(self, context):\n        # Load artifacts or dependencies here\n        pass\n\n    def predict(self, context, model_input):\n        # Apply custom logic here\n        return model_input.apply(some_transformation)\n\n# Example of saving the model\nmlflow.pyfunc.save_model(path=\"path/to/save\", python_model=CustomModel())\n</code></pre> <p>This example defines a custom model by subclassing <code>mlflow.pyfunc.PythonModel</code> and implementing the necessary methods to integrate custom behavior during model predictions.</p>"},{"location":"5.%20Refining/5.6.%20Model%20Registries.html#model-registry-additional-resources","title":"Model Registry additional resources","text":"<ul> <li>Model Registry integration from the MLOps Python Package</li> <li>MLflow Model Registry</li> <li>MLflow Model Registry Example</li> </ul>"},{"location":"6.%20Sharing/index.html","title":"6. Sharing","text":"<p>In this chapter, we will explore the essential practices and tools that facilitate the sharing and distribution of machine learning operations (MLOps) projects. Sharing not only enhances collaboration among data scientists and developers but also promotes the reuse and adaptation of existing models and workflows, crucial for the efficient scaling of machine learning solutions. By the end of this chapter, you will understand how to effectively organize, document, and disseminate your MLOps projects to make them accessible and useful to others.</p> <ul> <li>6.0. Repository: Learn how to set up and structure a repository for MLOps projects, which serves as the foundation for version control and collaboration.</li> <li>6.1. License: Understand the importance of choosing the right license for your project, which governs how others can use, modify, and distribute your work.</li> <li>6.2. Readme: Discover the key elements of crafting an effective README file that provides a comprehensive overview and guides users on how to use your project.</li> <li>6.3. Releases: Discuss the process of managing project versions through releases, which help in tracking iterations and ensuring stability for end users.</li> <li>6.4. Templates: Explore the use of templates to standardize project components, such as data pipelines and model training routines, enhancing consistency and reducing errors.</li> <li>6.5. Workstations: Delve into setting up cloud workstations, including configuring environments and tools that are essential for contributors to efficiently work on your project.</li> <li>6.6. Contributions: Examine strategies for managing contributions, including guidelines for submitting issues and pull requests, which are vital for collaborative development and project improvement.</li> </ul>"},{"location":"6.%20Sharing/6.0.%20Repository.html","title":"6.0. Repository","text":""},{"location":"6.%20Sharing/6.0.%20Repository.html#what-is-a-code-repository","title":"What is a code repository?","text":"<p>A code repository serves as a centralized platform that supports collaboration in software development. It offers tools for version control, managing contribution guidelines, and establishing automation workflows. The key elements defining a code repository include the host platform, the owner (which can be an individual or an organization), and the repository name. For example, a project might be identified by a URL such as https://github.com/fmind/mlops-python-package where additional details on project specifics or documentation could be appended.</p> <p>Popular code repositories include GitHub, GitLab, and Bitbucket, each providing features tailored to different collaboration needs and complexities. GitHub is renowned for its robust support for public repositories, while GitLab and BitBucket are preferred by both private and public entities. Moreover, cloud providers like Google Cloud Platform, Azure, and AWS offer integrated code repository for public and private organizations.</p>"},{"location":"6.%20Sharing/6.0.%20Repository.html#why-do-you-need-to-configure-a-code-repository","title":"Why do you need to configure a code repository?","text":"<ul> <li>Share your code with others: Code repositories facilitate easy sharing of projects, promoting collaborative development and feedback.</li> <li>Reference your project: They provide a stable environment for hosting your code, ensuring it is always accessible and traceable.</li> <li>Setup collaboration: Code repositories support multiple developers working on the same project simultaneously without conflict, utilizing features such as branches and pull requests.</li> </ul> <p>Configuring a code repository is essential for both organizational and personal projects, as it secures projects from being confined to a single machine and enhances collaboration, security, and reliability.</p>"},{"location":"6.%20Sharing/6.0.%20Repository.html#which-information-should-you-provide-in-a-code-repository","title":"Which information should you provide in a code repository?","text":"<p>On the repository's main page, you should include:</p> <ul> <li>Name: Use a concise and descriptive name to clearly identify your project.</li> <li>Description: Provide a detailed description outlining the project's purpose, scope, and functionality.</li> <li>Tags: Employ tags to help categorize your repository under relevant topics, facilitating easier discovery based on certain technologies or functionalities.</li> </ul> <p>Maintain organizational consistency by following any existing naming conventions which might include team names or technical stacks. For example, <code>forecasting-bikes-ml</code> includes the team name (forecasting), project domain (bikes), and tech stack (machine learning). Such conventions avoid collisions if another teams want to create a <code>bikes</code> project, or if the same team wants to used another technology stack (e.g., spark for data processing).</p>"},{"location":"6.%20Sharing/6.0.%20Repository.html#what-are-the-main-concepts-and-structures-related-to-a-code-repository","title":"What are the main concepts and structures related to a code repository?","text":"<p>Commits, branches, and tags are essential for version management and ensuring changes are recorded and retrievable. Commits are useful for logging new changes, branches for isolating work during development, and tags for organizing project releases.</p>"},{"location":"6.%20Sharing/6.0.%20Repository.html#creating-a-commit","title":"Creating a Commit","text":"<p>A git commit represents a snapshot of your project's history at a particular point in time. Here are the steps to create a commit using Git:</p> <ol> <li>Modify your files or add new ones within your project directory.</li> <li>Stage the changes you want to include in your commit by running: <pre><code>git add &lt;filename&gt;\n</code></pre> To add all changes in the directory, you can use: <pre><code>git add .\n</code></pre></li> <li>Check the status to see what changes are staged for the next commit: <pre><code>git status\n</code></pre></li> <li>Commit the staged changes by running: <pre><code>git commit -m \"Your commit message\"\n</code></pre></li> </ol> <p>Here, replace \"Your commit message\" with a brief description of what changes were made in this commit.</p>"},{"location":"6.%20Sharing/6.0.%20Repository.html#creating-a-branch","title":"Creating a Branch","text":"<p>A git branch allows you to develop features, fix bugs, or safely experiment with new ideas in a contained area of your repository.</p> <ol> <li>Switch to the branch from which you want to base your new branch (commonly the main branch): <pre><code>git checkout main\n</code></pre></li> <li>Create and switch to a new branch by running: <pre><code>git checkout -b &lt;branch-name&gt;\n</code></pre> Replace <code>&lt;branch-name&gt;</code> with a descriptive name for your branch, such as <code>feat/add-login</code>.</li> </ol>"},{"location":"6.%20Sharing/6.0.%20Repository.html#creating-tags","title":"Creating Tags","text":"<p>A git tag is used to mark specific points in repository history as important, typically for release versions.</p> <ol> <li>Check your commit history to find the commit to which you want to attach a tag: <pre><code>git log\n</code></pre></li> <li>Create an annotated tag on your current commit by running: <pre><code>git tag -a &lt;tag-name&gt; -m \"Your tag message\"\n</code></pre> Replace <code>&lt;tag-name&gt;</code> with your version or release identifier, such as <code>v1.0.0</code>, and \"Your tag message\" with a description of what this tag represents.</li> <li>Push the tag to your remote repository: <pre><code>git push origin &lt;tag-name&gt;\n</code></pre></li> </ol>"},{"location":"6.%20Sharing/6.0.%20Repository.html#what-is-the-best-method-to-clone-a-code-repository","title":"What is the best method to clone a code repository?","text":"<p>To clone a repository, you can use either HTTPS or SSH, with SSH being the preferred method due to its security benefits. SSH does not require credential re-authentication for each push or pull operation, unlike HTTPS.</p> <p>To create an SSH key:</p> <pre><code>ssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"\n</code></pre> <p>After generating the SSH key, add it to your repository host account settings. It's crucial to remember never to share your private SSH key as it serves as your secure credential. Always keep it confidential. You can identify the public key, which is safe to share, by its <code>.pub</code> file extension. For instance, if your private key is named <code>id_rsa</code>, your public key will be named <code>id_rsa.pub</code> and this is the key you upload to your repository host for authentication.</p> <p>To clone a repository with SSH (recommended):</p> <pre><code>git clone git@hostname:user/repository.git\n</code></pre> <p>To clone a repository with HTTPS:</p> <pre><code>git clone https://hostname/user/repository.git\n</code></pre>"},{"location":"6.%20Sharing/6.0.%20Repository.html#is-it-possible-to-restrict-the-visibility-of-a-code-repository","title":"Is it possible to restrict the visibility of a code repository?","text":"<p>You can adjust the visibility of your code repository to be either public or private:</p> <ul> <li>Public repositories are accessible to everyone and are ideal for open-source projects.</li> <li>Private repositories are only accessible to specific individuals or teams and are suitable for sensitive or proprietary projects.</li> </ul> <p>Choose the visibility setting that best aligns with your project needs and organizational policies.</p>"},{"location":"6.%20Sharing/6.0.%20Repository.html#what-is-the-difference-between-forking-and-branching-a-code-repository","title":"What is the difference between forking and branching a code repository?","text":"<p>Forking a repository involves creating a complete copy of the repository under your account, enabling you to work independently without affecting the original repository. Branching creates a separate line within the same repository, allowing for isolated development that can be merged back into the main code base later.</p> <p>Forking is typically used when public collaborators wish to develop their versions of a project, whereas branching is suited for ongoing collaboration within the same project direction, recommended for both public and private organizational use.</p>"},{"location":"6.%20Sharing/6.0.%20Repository.html#is-it-possible-to-setup-automation-at-the-repository-level","title":"Is it possible to setup automation at the repository level?","text":"<p>Automation can be effectively implemented at the repository level through the use of webhooks, third-party apps, and integrations. These tools can help automate tasks such as testing, deployment, and integration workflows, which enhances productivity and ensures consistency across operations.</p> <ul> <li> <p>Webhooks: Webhooks can be configured to trigger automated workflows in response to events within the repository, such as pushes, pull requests, or merges. For example, a webhook could trigger an automated build or test in a continuous integration system whenever code is pushed to a repository.</p> </li> <li> <p>Third-party apps: Many third-party applications are available on platforms like GitHub Apps or GitLab Integrations that can be used to extend the functionality of your repository. For instance, applications like CircleCI, Travis CI, or Jenkins can be integrated for continuous integration and continuous deployment (CI/CD) pipelines. Another popular choice is SonarQube, which can be integrated for automatic code quality reviews.</p> </li> <li> <p>Integrations: Most repository platforms offer a range of built-in integrations that connect with other tools and services. For example, GitHub can integrate directly with project management tools like Jira or Trello to link commits and pull requests to tasks. Similarly, it can integrate with deployment platforms like Heroku or AWS, enabling automatic deployments when changes are made to specific branches.</p> </li> </ul>"},{"location":"6.%20Sharing/6.0.%20Repository.html#how-can-you-protect-the-code-of-the-main-branch","title":"How can you protect the code of the main branch?","text":"<p>To safeguard the integrity of your main branch, you can implement branch protection rules. These might include requirements for pull request reviews, status checks before merging, and restrictions on who can push to these crucial parts of your repository, ensuring all changes are thoroughly checked.</p> <ol> <li>Navigate to Your Repository Settings: Go to your repository on GitHub, click on \"Settings,\" and then select \"Branches\" from the sidebar.</li> <li>Add Branch Protection Rules: Click on \"Add rule\" in the \"Branch protection rules\" section. Enter the name of the branch you want to protect, typically <code>main</code> or <code>master</code>.</li> <li>Configure the Protection Rules:<ul> <li>Require pull request reviews before merging: This setting requires one or more reviews before a contributor can merge changes into the protected branch, which helps ensure that code is reviewed and approved by other team members.</li> <li>Require status checks to pass before merging: Enabling this ensures that all required CI tests pass before the branch can be merged, which is crucial for maintaining code quality and functionality.</li> <li>Include administrators: You can enforce these rules on repository administrators as well, ensuring that all changes, regardless of who makes them, undergo the same level of scrutiny.</li> <li>Restrict who can push to matching branches: This setting allows you to specify which users or teams can push to the branch, adding an additional layer of security.</li> </ul> </li> <li>Save Changes: Once configured, save the changes to enforce the rules on the branch.</li> </ol>"},{"location":"6.%20Sharing/6.0.%20Repository.html#repository-additional-resources","title":"Repository additional resources","text":"<ul> <li>Repository example from the MLOps Python Package</li> <li>GitHub Repository documentation</li> </ul>"},{"location":"6.%20Sharing/6.1.%20License.html","title":"6.1. License","text":""},{"location":"6.%20Sharing/6.1.%20License.html#what-is-a-software-license","title":"What is a software license?","text":"<p>A software license is a legal agreement that details the conditions under which a software product can be accessed and used. This agreement is essential for defining the rights and responsibilities of both the developers and the users. It covers aspects like software modification, sharing, and usage in other projects, thus protecting the intellectual property of the creators and the legal rights of the users.</p> <p>Software licenses are typically categorized into two main types: open-source and proprietary. Open-source licenses generally allow the software to be freely used, modified, and shared, while proprietary licenses impose restrictions on these activities.</p>"},{"location":"6.%20Sharing/6.1.%20License.html#why-do-you-need-a-software-license","title":"Why do you need a software license?","text":"<p>Implementing a software license is crucial for several reasons:</p> <ul> <li>Legal Clarity: Establishes a legal framework that dictates how the software can be used, preventing disputes.</li> <li>Copyright Protection: Protects your intellectual property from unauthorized use or distribution.</li> <li>Control Over Distribution: Dictates the terms of how your software is distributed, whether it is sold, given away, or integrated into other products.</li> <li>Open Source Integrity: In open-source projects, a clear license preserves the ethos of free use and collaborative development, and governs contributions.</li> </ul> <p>In an organizational setting, it's vital to choose a license that aligns with institutional policies and objectives. Consultation with legal and management teams is recommended to ensure compliance with these policies.</p>"},{"location":"6.%20Sharing/6.1.%20License.html#example-mit-vs-gplv3","title":"Example: MIT vs GPLv3","text":"<p>The MIT License and the GNU General Public License v3 (GPLv3) are two prevalent open-source licenses that differ significantly:</p> <ul> <li>MIT License: Highly permissive, allowing almost unrestricted use, including in proprietary projects, provided the original copyright and license notice are included in copies of the software.</li> <li>GNU General Public License v3 (GPLv3): This copyleft license requires that any derivatives of the software also be distributed under the same license, ensuring the preservation of usage, modification, and sharing rights.</li> </ul>"},{"location":"6.%20Sharing/6.1.%20License.html#case-study-elasticsearch-vs-opensearch","title":"Case Study: ElasticSearch vs OpenSearch","text":"<p>ElasticSearch switched from the very permissive Apache 2.0 license to a dual-license model due to concerns about cloud providers using their software without contributing back. This led Amazon Web Services (AWS) to fork ElasticSearch, creating OpenSearch under the Apache 2.0 license to maintain its openness and community contribution. This case underscores the strategic role of licensing in software development and community engagement.</p>"},{"location":"6.%20Sharing/6.1.%20License.html#how-to-choose-your-software-license","title":"How to choose your software license?","text":"<p>Choosing the right software license involves careful consideration of your project\u2019s objectives and the legal landscape:</p> <ul> <li>For Open Source Projects: Utilize tools like ChooseALicense.com to select a license that fits your preferences regarding how your software is used and shared.</li> <li>For Proprietary Project: Work closely with your company\u2019s legal team to ensure the license aligns with business strategies and regulatory requirements.</li> </ul>"},{"location":"6.%20Sharing/6.1.%20License.html#how-to-define-the-license-for-your-project","title":"How to define the license for your project?","text":"<p>To implement a license in your project:</p> <ol> <li>Create a LICENSE.txt file in the root directory.</li> <li>Include the Full License Text: Ensure the complete, unaltered text of the license is included to make it legally binding.</li> </ol> <p>You can find open-source license text on this website: https://opensource.org/license.</p>"},{"location":"6.%20Sharing/6.1.%20License.html#should-you-choose-a-different-license-for-aiml-models","title":"Should you choose a different license for AI/ML models?","text":"<p>Licensing AI and ML models necessitates addressing unique considerations related to the nature of data usage, model training, and deployment:</p> <ul> <li>Data Privacy and Usage: It's crucial to ensure that the licensing agreement covers the handling of potentially sensitive or personal information in compliance with applicable privacy laws, such as GDPR.</li> <li>Model Reproducibility: The license should specify if and how the model can be used to generate derivative models or reproduce results.</li> <li>Commercial Use: You need to decide whether the model can be used for commercial purposes. Some licenses may restrict commercial use to promote free academic and research utilization.</li> <li>Attribution Requirements: If required, the license should mandate users to credit the model creators or reference the original research.</li> </ul> <p>Here is a list of some popular licenses used for AI/ML models that address these considerations:</p> <ul> <li>Apache License 2.0: Permits almost unrestricted use, including commercial use, while still requiring attribution.</li> <li>GNU General Public License (GPL) v3.0: Ensures that derivative works are distributed under the same license terms, suitable for models where sharing improvements is desired.</li> <li>MIT License: A permissive license that allows almost any use with minimal restrictions, provided that the license and copyright notice are included.</li> <li>Creative Commons (CC BY 4.0): Suitable for datasets rather than models, requiring attribution but allowing for commercial and derivative uses.</li> </ul>"},{"location":"6.%20Sharing/6.1.%20License.html#how-should-you-integrate-the-license-of-your-software-dependencies","title":"How should you integrate the license of your software dependencies?","text":"<p>Effectively managing the licenses of software dependencies ensures legal compliance and supports community and user trust. Here are streamlined steps for integrating these licenses:</p> <ol> <li>Inventory Dependencies: Use tools like SPDX to list and document all third-party components and their licenses.</li> <li>Review License Terms: Analyze what is permitted or restricted under each license.</li> <li>Assess Compatibility: Check for conflicts between dependency licenses and your project\u2019s intended use.</li> <li>Automate License Compliance: Employ tools like WhiteSource or Black Duck to automate license management, integrating them into your CI/CD pipeline.</li> </ol> <p>These steps help maintain legal integrity while leveraging the benefits of diverse software components.</p>"},{"location":"6.%20Sharing/6.1.%20License.html#license-additional-resources","title":"License additional resources","text":"<ul> <li>License example from the MLOps Python Package</li> <li>Choose an Open Source License</li> </ul>"},{"location":"6.%20Sharing/6.2.%20Readme.html","title":"6.2. Readme","text":""},{"location":"6.%20Sharing/6.2.%20Readme.html#what-is-a-readme-file","title":"What is a README file?","text":"<p>A README.md file acts as the front page of your project repository. Think of it as the first point of contact for anyone encountering your project. It goes beyond just a basic description; it\u2019s your guidebook, introducing the project\u2019s purpose, usage, and how to contribute effectively.</p>"},{"location":"6.%20Sharing/6.2.%20Readme.html#why-do-you-need-a-readme-file","title":"Why do you need a README file?","text":"<ul> <li>Advertise your project: A compelling README attracts potential users and contributors by highlighting the value and uniqueness of your project.</li> <li>Explain the purpose and usage: It clarifies what the project does, who it\u2019s for, and how it can be utilized, thereby increasing user engagement and adoption.</li> <li>Include some visuals and statistics: Including images, badges, and usage statistics can make the README visually appealing and informative, offering quick insights at a glance.</li> </ul> <p>A well-crafted README is crucial as it forms the first impression of your project. Investing time in developing a clear, informative, and engaging README can significantly impact the project's visibility and usability.</p>"},{"location":"6.%20Sharing/6.2.%20Readme.html#which-format-should-you-use-for-the-readme-file","title":"Which format should you use for the README file?","text":"<p>While READMEs can be written in plain text, Markdown, or RestructuredText. Markdown is the most popular due to its straightforward and intuitive syntax.</p> <p>You can learn more about Markdown format at this link: Markdown Guide.</p>"},{"location":"6.%20Sharing/6.2.%20Readme.html#what-should-you-put-in-your-readme-file","title":"What should you put in your README file?","text":"<p>Typically, your README should include the following information:</p> <ul> <li>Detailed project overview: Provide a comprehensive description of what the project does and the problems it solves.</li> <li>Key features and functionalities: Highlight the main features and what makes your project stand out.</li> <li>Visual elements like logos or screenshots: Graphics can help make the README more engaging and easier to understand at a glance.</li> <li>How to install/set up/use the project: Offer clear, step-by-step instructions to get the project running and how to use it effectively.</li> <li>Badges: Include badges from platforms like Travis CI, Codecov, and others to show build status, test coverage, etc.</li> </ul>"},{"location":"6.%20Sharing/6.2.%20Readme.html#when-should-you-write-your-readme-file","title":"When should you write your README file?","text":"<p>Ideally, you should start your README early in the development process and update it as your project evolves.</p> <p>Begin with a basic structure:</p> <pre><code># Your AI/ML Project\n\nExplore the innovative applications of AI and ML with our project.\n</code></pre> <p>As your project develops, expand the README to include:</p> <ul> <li>Capabilities: Detail the unique solutions your project offers.</li> <li>Installation Guide: Provide comprehensive instructions for setting up.</li> <li>Usage Instructions: Include examples to demonstrate how to use the project effectively.</li> </ul>"},{"location":"6.%20Sharing/6.2.%20Readme.html#how-can-you-get-more-tips-on-writing-a-readme-file","title":"How can you get more tips on writing a README file?","text":"<p>For additional tips on crafting an effective README.md, consider these resources:</p> <ul> <li>Make a README: Offers best practices and examples.</li> <li>Awesome README: Features a collection of well-crafted READMEs for inspiration.</li> <li>README Template: Provides a basic template to start your README.</li> </ul> <p>To enhance your README writing, consider using VS Code Extensions like:</p> <ul> <li>MarkdownLint: Helps keep your Markdown files clean and consistent.</li> <li>Markdown Preview Enhanced: Provides a real-time preview of your Markdown document.</li> <li>Markdown All in One: Assists in maintaining a high standard of documentation.</li> </ul> <p>Include headers in your file to facilitate navigation, especially using VS Code extensions like Markdown All in One to automatically generate a Table of Contents.</p>"},{"location":"6.%20Sharing/6.2.%20Readme.html#what-should-you-do-if-my-readme-file-becomes-too-big","title":"What should you do if my README file becomes too big?","text":"<p>If your README grows too extensive, it's advisable to transition to a full-fledged documentation site. This can be achieved using:</p> <ul> <li>Documentation generators: Tools like Sphinx or mkDocs can help structure and generate detailed documentation.</li> <li>Documentation hosting: Platforms such as GitHub Pages or ReadTheDocs offer hosting solutions to make your documentation accessible online.</li> </ul> <p>As your project scales, ensuring that the documentation remains manageable and navigable is crucial for maintaining user engagement and developer contributions.</p>"},{"location":"6.%20Sharing/6.2.%20Readme.html#readme-additional-resources","title":"README additional resources","text":"<ul> <li>README example from the MLOps Python Package</li> <li>Make a README: Offers best practices and examples.</li> <li>Awesome README: Features a collection of well-crafted READMEs for inspiration.</li> <li>README Template: Provides a basic template to start your README.</li> </ul>"},{"location":"6.%20Sharing/6.3.%20Releases.html","title":"6.3. Releases","text":""},{"location":"6.%20Sharing/6.3.%20Releases.html#what-is-a-project-release","title":"What is a project release?","text":"<p>A project release is a specific version of your project that stabilizes the code and its associated artifacts. It documents a list of changes, bug fixes, and new features aimed at achieving the project's goals. A release serves as a reference point for contributors, enabling them to pinpoint specific software versions that can be utilized in other projects.</p>"},{"location":"6.%20Sharing/6.3.%20Releases.html#why-do-you-need-to-create-project-releases","title":"Why do you need to create project releases?","text":"<ul> <li>Communicate changes to others: Creating a release provides a structured way to inform users and developers about new features, fixes, and improvements. This communication is crucial for managing expectations and helping users to understand how the software has evolved.</li> <li>Divide the project into milestones: Releases help in setting and achieving milestones throughout the development process. This organization allows teams to track progress, evaluate the effectiveness of the project timeline, and adjust goals as needed.</li> <li>Ensure each release is stable and consistent: Each release acts as a quality checkpoint, ensuring that the software meets certain standards of stability and consistency. This reduces the risk of regressions and maintains a reliable user experience.</li> </ul> <p>Project releases act as a contract between the contributors and end users. Referencing a specific version, like v8.1.2, guarantees a consistent set of behaviors and interfaces. By maintaining clear and transparent communication about releases, you can build and maintain trust with your user base, especially as the project grows.</p>"},{"location":"6.%20Sharing/6.3.%20Releases.html#at-which-frequency-should-you-release-a-new-project","title":"At which frequency should you release a new project?","text":"<p>The frequency of new releases varies depending on the project. Some projects benefit from a rapid release cycle, as short as every two or three weeks, while others may opt for longer intervals, possibly every few months. It's crucial to balance the needs of both contributors and users and to establish a clear naming scheme to prevent compatibility issues and conflicts.</p>"},{"location":"6.%20Sharing/6.3.%20Releases.html#which-git-workflow-should-you-use-for-your-project","title":"Which git workflow should you use for your project?","text":"<p>Choosing an effective git workflow is crucial for streamlined team collaboration and efficient project management. Below are three popular workflows, each with its own strengths and suited for different types of projects.</p>"},{"location":"6.%20Sharing/6.3.%20Releases.html#github-flow","title":"GitHub Flow","text":"<p>Ideal for projects with frequent releases, GitHub Flow is simple:</p> <ul> <li>Main branch is always deployable.</li> <li>Feature branches for each new feature or fix.</li> <li>Use Pull Requests (PRs) for reviews.</li> <li>Merge into <code>main</code> after review.</li> </ul>"},{"location":"6.%20Sharing/6.3.%20Releases.html#git-flow","title":"Git Flow","text":"<p>More structured, Git Flow is suited for projects with scheduled releases:</p> <ul> <li>Maintain two main branches: <code>master</code> and <code>develop</code>.</li> <li>Feature branches merge into <code>develop</code>.</li> <li>Release branches prepare new production releases.</li> <li>Hotfix branches allow for quick fixes to production.</li> </ul>"},{"location":"6.%20Sharing/6.3.%20Releases.html#forking-workflow","title":"Forking Workflow","text":"<p>Ideal for open-source projects where multiple external contributors work independently:</p> <ul> <li>Contributors fork the main repo and work independently.</li> <li>Changes are submitted via PRs from their own forks.</li> <li>Central maintainers review and merge PRs into the official repository.</li> </ul>"},{"location":"6.%20Sharing/6.3.%20Releases.html#comparison","title":"Comparison","text":"<ul> <li>GitHub Flow is best for small teams or projects needing fast, continuous updates.</li> <li>Git Flow suits large, complex projects with multiple simultaneous development streams.</li> <li>Forking Workflow is optimal for projects with broad, diverse contributor bases, particularly in open source.</li> </ul> <p>Choose a workflow based on your project's size, complexity, release frequency, and the nature of your team's collaboration.</p>"},{"location":"6.%20Sharing/6.3.%20Releases.html#which-versioning-schema-should-you-use-for-a-project-release","title":"Which versioning schema should you use for a project release?","text":"<p>The two most popular versioning schemes are:</p> <ul> <li>SemVer (Semantic Versioning): SemVer is based on three numerical identifiers: major, minor, and patch (e.g., 1.4.2). Major changes include breaking changes, minor changes add functionality in a backwards-compatible manner, and patches are for backwards-compatible bug fixes.</li> <li>CalVer (Calendar Versioning): CalVer uses the calendar date to inform the version number, which can be useful for projects where time and the state of the project are closely linked, such as in continuously delivered software.</li> </ul> <p>Python also adopts its own version specification scheme known as PEP440, which provides guidelines for version identifiers based on public releases, pre-releases, post-releases, and development releases.</p> <p>SemVer is recommended for its simplicity and the possibility of communication various level of changes.</p>"},{"location":"6.%20Sharing/6.3.%20Releases.html#how-can-you-create-a-new-project-release-on-github","title":"How can you create a new project release on GitHub?","text":"<p>Creating a new release on GitHub involves the following steps:</p> <ol> <li>Navigate to your GitHub repository and go to the \"Releases\" section.</li> <li>Click on \"Draft a new release\" or use the tag version from your git command line.</li> <li>Fill in the tag version, which should match your versioning schema, add release title and description that includes the highlights of this release such as new features, bug fixes, and any deprecations.</li> <li>Attach binaries or additional relevant files if necessary.</li> <li>Publish the release, which will then be available to users and can be downloaded from the repository.</li> </ol>"},{"location":"6.%20Sharing/6.3.%20Releases.html#how-can-you-prepare-and-implement-a-project-release-with-other-developers","title":"How can you prepare and implement a project release with other developers?","text":"<p>Preparing and implementing a release with other developers involves collaboration tools provided by platforms like GitHub:</p> <ul> <li>Issues: Use GitHub Issues to track individual tasks or bugs that need to be addressed before the release.</li> <li>Labels: Apply labels to issues to categorize them by priority, type (e.g., bug, feature), or status (e.g., in-progress, completed).</li> <li>Milestones: Group related issues under a milestone corresponding to the release. This helps in tracking progress towards the release and ensures all intended features and fixes are included.</li> </ul> <p>These tools facilitate organization and communication among team members, ensuring that everyone is aligned and aware of the release objectives and timelines.</p>"},{"location":"6.%20Sharing/6.3.%20Releases.html#how-can-you-review-and-include-the-work-of-others-in-the-project-release","title":"How can you review and include the work of others in the project release?","text":"<p>Contributors should work on a feature or fix branch specifically created for their task. Once the task is completed:</p> <ol> <li>Feature or fix branch: Developers create branches off the main branch to develop new features or fixes.</li> <li>Pull Request (PR): They submit a Pull Request to merge their branch into the main branch. The PR should include a clear description of the changes and any necessary documentation updates.</li> <li>Code Review: Other contributors review the PR, providing feedback and requesting changes if needed. This ensures that the code meets the project\u2019s standards and integrates well with the existing codebase.</li> </ol> <p>After approval, the branch is merged into the main branch, and the feature or fix becomes part of the next release. The feature branch can then be archived.</p>"},{"location":"6.%20Sharing/6.3.%20Releases.html#which-steps-should-be-performed-prior-to-a-new-project-release","title":"Which steps should be performed prior to a new project release?","text":"<p>Before releasing, perform these steps:</p> <ul> <li>Testing: Conduct thorough testing, possibly automated using tools like GitHub Actions, to ensure the software works as intended and is free from major bugs.</li> <li>Publication: Set up a workflow to automatically handle tasks such as documentation updates and container builds upon release.</li> <li>Changelog: Update the CHANGELOG.md using tools like commitizen, detailing all changes, fixes, and features in this release.</li> </ul> <p>These steps ensure that the release is polished and ready for public use.</p>"},{"location":"6.%20Sharing/6.3.%20Releases.html#how-can-you-communicate-the-changes-related-to-a-project-release","title":"How can you communicate the changes related to a project release?","text":"<p>Effective communication of changes involves several platforms:</p> <ul> <li>Release page on GitHub: Provides detailed release notes and links to downloadable assets or binaries.</li> <li>CHANGELOG.md: Maintains a cumulative record of all changes made across releases using the keep a changelog format.</li> <li>GitHub Pages: Utilizes a version-specific page to provide documentation corresponding to each release.</li> </ul>"},{"location":"6.%20Sharing/6.3.%20Releases.html#how-long-should-you-support-previous-releases-of-your-project","title":"How long should you support previous releases of your project?","text":"<p>The duration for supporting previous releases depends on the project's scope and user base. For instance, projects like Python provide Long Term Support (LTS) versions that are supported for several years. Define the level of support for each release, which might include backporting critical bug fixes or providing security updates.</p> <p>A well-documented support policy reassures users that they can rely on your software for critical applications over time.</p>"},{"location":"6.%20Sharing/6.3.%20Releases.html#release-additional-resources","title":"Release additional resources","text":"<ul> <li>Release page from the MLOps Python Package</li> <li>Managing releases in a GitHub repository</li> </ul>"},{"location":"6.%20Sharing/6.4.%20Templates.html","title":"6.4. Templates","text":""},{"location":"6.%20Sharing/6.4.%20Templates.html#what-is-a-code-template","title":"What is a code template?","text":"<p>A code template provides a standardized framework for initiating new projects, particularly beneficial in environments where multiple projects share common elements. This structure usually includes predefined configurations for common tools like linters, unit testing frameworks, and code formatters. By doing so, each new project can be customized with specific details such as the project name, description, and operating environment, while maintaining a consistent approach to software development.</p> <p>For instance, the authors of this course provide the Cookiecutter MLOps Package for free to quickly generate new MLOps Projects based on the principles described in this course. This section explains how to leverage similar code templates and adapt them for your organization.</p>"},{"location":"6.%20Sharing/6.4.%20Templates.html#why-do-you-need-to-create-code-templates","title":"Why do you need to create code templates?","text":"<ul> <li>Align code base and best practices: Creating a code template helps enforce uniformity and adherence to best practices across all projects within an organization</li> <li>Augment your productivity: Templates streamline the project setup process, significantly reducing the time and effort needed to start new projects</li> <li>Focus on the main problem: Code templates allow developers to concentrate on solving the business problem rather than getting bogged down by repetitive setup tasks. The template maintainers can focus on enhancing the template itself, ensuring it incorporates the latest and most efficient solutions.</li> </ul> <p>As AI/ML projects become more standardized and akin to an assembly line production in factories, automating their creation ensures faster deployment and a higher standard of initial setup quality.</p>"},{"location":"6.%20Sharing/6.4.%20Templates.html#which-tools-should-you-use-to-create-code-templates","title":"Which tools should you use to create code templates?","text":""},{"location":"6.%20Sharing/6.4.%20Templates.html#cookiecutter","title":"Cookiecutter","text":"<p>Cookiecutter is a widely-used tool in the Python community for creating project templates. It uses a straightforward command-line interface to generate new projects from user-defined templates.</p> <pre><code>cookiecutter [template-directory]\n</code></pre> <p>This command processes the template directory or repository containing a <code>cookiecutter.json</code> file and potentially other template files, prompting the user for input on defined variables.</p>"},{"location":"6.%20Sharing/6.4.%20Templates.html#cruft","title":"Cruft","text":"<p>Cruft complements Cookiecutter by helping manage updates to projects created from a Cookiecutter template. It tracks changes in the template and can apply these changes to existing projects, helping maintain consistency and up-to-date practices across all projects.</p> <p>Initializing a new project with Cruft:</p> <pre><code>cruft create [template-repository-url]\n</code></pre> <p>Updating the project afterwards:</p> <pre><code>cruft update\n</code></pre>"},{"location":"6.%20Sharing/6.4.%20Templates.html#how-can-you-pass-variables-to-replace-inside-a-code-template","title":"How can you pass variables to replace inside a code template?","text":"<p>Variables in Cookiecutter templates are managed using Jinja2, a template engine for Python. Jinja2 allows for dynamic content generation using placeholders in template files, which are replaced by actual values at runtime based on user input or default values defined in <code>cookiecutter.json</code>.</p> <p>Example of using Cookiecutter variables in code:</p> <pre><code># Define a variable in your Python script that uses Cookiecutter variables\nproject_name = \"{{ cookiecutter.project_name }}\"\n</code></pre> <p>The <code>cookiecutter.json</code> file is where all default values for the variables in a template are defined. When a new project is generated, Cookiecutter prompts the user to input values for these variables or accept the defaults as specified in the JSON file.</p> <p>Here is an example of defining variables in the <code>cookiecutter.json</code> file, which includes project metadata and configuration defaults:</p> <pre><code>{\n    \"user\": \"fmind\",\n    \"name\": \"MLOps Project\",\n    \"repository\": \"{{cookiecutter.name.lower().replace(' ', '-')}}\",\n    \"package\": \"{{cookiecutter.repository.replace('-', '_')}}\",\n    \"license\": \"MIT\",\n    \"version\": \"0.1.0\",\n    \"description\": \"TODO\",\n    \"python_version\": \"3.12\",\n    \"mlflow_version\": \"2.19.0\"\n}\n</code></pre>"},{"location":"6.%20Sharing/6.4.%20Templates.html#how-should-you-structure-a-cookiecutter-template-repository","title":"How should you structure a cookiecutter template repository?","text":"<p>A cookiecutter template repository typically consists of two primary levels of structure:</p> <ol> <li> <p>Template Files and Folders: These are the directories and files that will be generated and are identifiable by their names containing cookiecutter variables (e.g., <code>{{cookiecutter.project_slug}}</code>. All such template files and folders are contained within a single root directory (e.g, <code>{{cookiecutter.repository}}</code>) to facilitate easy generation.</p> </li> <li> <p>Supporting Files: These include additional resources such as documentation, scripts for automating setup tasks, or configuration files necessary for the template itself but not part of the generated project files. These files reside outside the main template folder.</p> </li> </ol> <p>Refer to the cookiecutter-mlops-package template by this course authors for a practical example of how to structure a template repository.</p> <p>Initializing this template package:</p> <pre><code>cookiecutter gh:fmind/cookiecutter-mlops-package\n</code></pre> <p>For advanced structuring techniques and best practices, refer to the Advanced Usage section of Cookiecutter documentation.</p>"},{"location":"6.%20Sharing/6.4.%20Templates.html#what-should-you-include-and-exclude-from-the-code-template","title":"What should you include and exclude from the code template?","text":"<p>What to Include:</p> <ul> <li>Automation tasks: Tools like PyInvoke to automate common development tasks.</li> <li>Linters: Tools such as Ruff to ensure code quality and style consistency.</li> <li>Unit test configurations: Setup configurations for tools like pytest to facilitate testing.</li> <li>Project configuration files: Such as <code>pyproject.toml</code> for managing project dependencies and settings.</li> </ul> <p>What to Exclude:</p> <ul> <li>Specific source code or tests: Avoid including actual code or tests that imply a specific programming style or architecture. This allows developers to apply their preferred coding practices without constraints.</li> </ul>"},{"location":"6.%20Sharing/6.4.%20Templates.html#how-should-you-keep-the-project-updated-with-the-code-template","title":"How should you keep the project updated with the code template?","text":"<p>To maintain alignment with the original template as it evolves, initiate your project with cruft and periodically run:</p> <pre><code>cruft update\n</code></pre> <p>This will help integrate enhancements and bug fixes from the template into your project, utilizing Git's capabilities to manage any conflicts that arise.</p>"},{"location":"6.%20Sharing/6.4.%20Templates.html#how-should-you-illustrate-the-usage-of-a-code-template","title":"How should you illustrate the usage of a code template?","text":"<p>Creating one or several demo repositories based on the template serves multiple purposes:</p> <ul> <li>Demonstrates practical application: Show how the template can be used in a real-world scenario.</li> <li>Encourages experimentation: Provides a base for others to experiment with different coding styles or architectural approaches.</li> <li>Facilitates specific feature development: Use the demo to develop features that might not be universally required but are useful for some projects.</li> </ul>"},{"location":"6.%20Sharing/6.4.%20Templates.html#how-should-you-change-and-improve-the-code-template","title":"How should you change and improve the code template?","text":"<p>The most effective approach to develop and refine a code template is to:</p> <ol> <li>Generate a new project using the code template.</li> <li>Implement and test your changes in the generated project.</li> <li>Once validated, backport these changes to the template itself.</li> </ol> <p>This iterative approach helps ensure that the template remains robust and functional across different use cases.</p>"},{"location":"6.%20Sharing/6.4.%20Templates.html#how-can-you-test-the-generation-of-a-code-template-automatically","title":"How can you test the generation of a code template automatically?","text":"<p>Utilizing pytest-cookies, you can automatically test the generation process of your Cookiecutter template:</p> <pre><code>def test_bake_project(cookies):\n    result = cookies.bake(extra_context={\"repo_name\": \"helloworld\"})\n    assert result.exit_code == 0\n    assert result.exception is None\n    assert result.project_path.name == \"helloworld\"\n    assert result.project_path.is_dir()\n</code></pre> <p>Additionally, pytest-shell-utilities can be used to run shell commands post-generation to validate setup tasks:</p> <pre><code>def test_assert_good_exitcode(shell):\n    ret = shell.run(\"exit\", \"0\")\n    assert ret.returncode == 0\n\ndef test_assert_bad_exitcode(shell):\n    ret = shell.run(\"exit\", \"1\")\n    assert ret.returncode == 1\n</code></pre>"},{"location":"6.%20Sharing/6.4.%20Templates.html#how-can-you-perform-automated-actions-after-the-code-generation","title":"How can you perform automated actions after the code generation?","text":"<p>Cookiecutter's hook mechanism allows the execution of Python or shell scripts after the project generation. This functionality is crucial for performing setup tasks or cleaning up unnecessary files based on the user's choices.</p> <p>Example of a post-generation hook script:</p> <pre><code>import os\n\nREMOVE_PATHS = [\n    \"{% if cookiecutter.packaging != 'pip' %}requirements.txt{% endif %}\",\n]\n\nfor path in REMOVE_PATHS:\n    path = path.strip()\n    if path and os.path.exists(path):\n        os.unlink(path) if os.path.isfile(path) else os.rmdir(path)\n</code></pre>"},{"location":"6.%20Sharing/6.4.%20Templates.html#templates-additional-resources","title":"Templates additional resources","text":"<ul> <li>Cookiecutter MLOps Package</li> <li>Creating a GitHub template repository</li> <li>Python Package template with cookiecutter</li> </ul>"},{"location":"6.%20Sharing/6.5.%20Workstations.html","title":"6.5. Workstations","text":""},{"location":"6.%20Sharing/6.5.%20Workstations.html#what-is-a-cloud-workstation","title":"What is a cloud workstation?","text":"<p>A cloud workstation is a virtual computing environment hosted on remote servers, providing users with access to powerful computing resources over the internet. These virtual workstations typically include all necessary software and hardware capabilities required for specific tasks, which are maintained by the cloud provider. Users can access these resources from any device capable of connecting to the internet, ensuring flexibility and mobility.</p>"},{"location":"6.%20Sharing/6.5.%20Workstations.html#why-should-you-use-cloud-workstations","title":"Why should you use cloud workstations?","text":"<ul> <li>Improved Security: Cloud workstations benefit from the robust security measures implemented by cloud providers, which include data encryption, application firewalls, intrusion detection systems, and regular security audits.</li> <li>Start in Minutes: Cloud workstations can be provisioned rapidly, often within minutes. This quick deployment allows users to bypass the lengthy installations and configurations typically required for local setups.</li> <li>Share Common Setup: Cloud workstations enables teams to share a common development environment, ensuring consistency across all team members' setups. This homogeneity avoids the pitfalls of environment-specific issues and reduces the need for individual support.</li> <li>Improved Collaboration: Cloud workstations enhance collaboration among team members by allowing multiple users to access and work on the same environment simultaneously. This feature is particularly useful for remote teams, making it easier to share progress, troubleshoot issues, and pair program in real-time.</li> <li>Provide More Hardware Choice: Cloud providers typically offer a range of hardware configurations that users can choose from, according to their specific needs. This flexibility allows users to scale their resources up or down based on the project's demands without the need for physical hardware upgrades is particularly useful for MLOps.</li> </ul> <p>Compared to local systems, cloud workstations offer greater scalability, reliability, and accessibility. They eliminate the need for upfront hardware investments and reduce the ongoing maintenance and upgrading costs. Moreover, they can provide access to high-performance computing resources that might not be feasible or cost-effective to maintain locally.</p>"},{"location":"6.%20Sharing/6.5.%20Workstations.html#which-platforms-should-you-use-for-cloud-workstations","title":"Which platforms should you use for cloud workstations?","text":"<p>Several platforms offer robust cloud workstation services:</p> <ul> <li>GitHub Codespaces: Ideal for developers, it provides a complete dev environment within GitHub that can run in your browser or integrate with Visual Studio Code.</li> <li>Cloud Workstation (GCP): Offers customizable compute instances that can be tailored for high-performance environments.</li> <li>Amazon WorkSpaces (AWS): Provides a broad range of cloud computing options that can be optimized for different workloads, from lightweight tasks to compute-intensive applications.</li> </ul>"},{"location":"6.%20Sharing/6.5.%20Workstations.html#how-can-you-edit-code-together-using-cloud-workstations","title":"How can you edit code together using cloud workstations?","text":"<p>Collaborative coding in cloud workstations can be enhanced through various extensions and tools. For instance,Visual Studio Code Live Share allows real-time collaboration, letting participants from different locations edit and debug code together.</p>"},{"location":"6.%20Sharing/6.5.%20Workstations.html#workstation-additional-resources","title":"Workstation additional resources","text":"<ul> <li>GitHub Codespaces</li> <li>Cloud Workstation (GCP)</li> <li>Amazon WorkSpaces (AWS)</li> </ul>"},{"location":"6.%20Sharing/6.6.%20Contributions.html","title":"6.6. Contributions","text":""},{"location":"6.%20Sharing/6.6.%20Contributions.html#what-is-a-code-of-conduct","title":"What is a code of conduct?","text":"<p>A code of conduct is a document that establishes expectations for behavior for community members. It serves as a guideline to help create a safe, respectful, and collaborative environment. This document typically addresses issues such as harassment, discrimination, and conflict resolution. By having a clear code of conduct, projects can encourage an inclusive atmosphere that encourages participation from diverse groups of people.</p> <p>You can add a code of conduct for your project by following this guide from GitHub or adding a <code>CODE_OF_CONDUCT.md</code> file directly at the root of your repository. You can find examples on the Open Source Guides: Establishing a code of conduct.</p>"},{"location":"6.%20Sharing/6.6.%20Contributions.html#what-are-contribution-guidelines","title":"What are contribution guidelines?","text":"<p>Contribution guidelines are instructions provided by a project to help contributors understand how they can effectively participate. These guidelines often include standards for coding practices, the submission process for patches or features, and criteria for acceptable behavior when interacting with the project\u2019s community. By following these guidelines, contributors can ensure their efforts align with the project goals and requirements, streamlining the collaboration process.</p> <p>You can define the contributing guidelines for your project by creating a <code>CONTRIBUTING.md</code> file at the top of your repository.</p>"},{"location":"6.%20Sharing/6.6.%20Contributions.html#how-can-you-build-a-good-software-community","title":"How can you build a good software community?","text":"<p>Building a good software community involves several key strategies:</p> <ol> <li>Open Communication: Encourage open and transparent communication where all members feel heard. Utilize forums, chats, and regular meetings to facilitate discussions.</li> <li>Recognition and Rewards: Recognize and reward contributions, whether they are code submissions, documentation, or community help. This can promote a sense of value and appreciation among members.</li> <li>Inclusivity: Promote an inclusive environment by actively engaging with diverse groups and considering different perspectives in decision-making processes.</li> <li>Learning and Development: Provide opportunities for learning and personal development through workshops, seminars, and mentoring programs. This helps members grow their skills and stay engaged with the community.</li> <li>Leadership and Governance: Establish clear leadership and governance structures that define roles and responsibilities. This helps in managing the community efficiently and ensuring that everyone knows how to contribute effectively.</li> </ol>"},{"location":"6.%20Sharing/6.6.%20Contributions.html#how-should-you-invite-others-to-your-repository","title":"How should you invite others to your repository?","text":"<p>Inviting others to your repository can be effectively managed through the use of permissions. Here are some steps to consider:</p> <ol> <li>Clear Roles and Permissions: Define roles within your repository (e.g., Viewer, Contributor, Maintainer) and assign appropriate permissions that align with these roles.</li> <li>Contribution Guidelines: Provide clear contribution guidelines to help new contributors understand how to get started.</li> <li>Welcoming Message: Send a welcoming message explaining the project\u2019s goals, the importance of each role, and how new contributors can get involved.</li> <li>Use of Tools: Utilize tools like GitHub\u2019s issue tracker and pull requests to facilitate collaboration and communication.</li> </ol>"},{"location":"6.%20Sharing/6.6.%20Contributions.html#how-can-you-help-people-become-better-contributors","title":"How can you help people become better contributors?","text":"<p>Helping people become better contributors primarily involves mentorship and effective code reviews. Here\u2019s how you can implement these strategies:</p> <ol> <li>Code Reviews: Conduct thorough code reviews that not only focus on the correctness of code but also on best practices and design principles. Provide constructive feedback that helps contributors understand what they did well and where they can improve.</li> <li>Documentation: Ensure your project has comprehensive documentation that is easy to understand. This can help contributors get up to speed quickly and reduce misunderstandings.</li> <li>Regular Feedback: Offer regular feedback sessions where contributors can discuss their progress and receive advice on overcoming challenges.</li> <li>Mentoring: Establish a mentoring program where experienced contributors can guide new members. This can significantly enhance the learning curve and encourage more meaningful contributions.</li> </ol>"},{"location":"6.%20Sharing/6.6.%20Contributions.html#contribution-additional-resources","title":"Contribution additional resources","text":"<ul> <li>Code of conduct from the MLOps Python Package</li> <li>Code Review Guidelines for Data Science Teams</li> </ul>"},{"location":"7.%20Observability/index.html","title":"7. Observability","text":"<p>Observability in Machine Learning Operations (MLOps) is crucial for gaining insights into the performance, behavior, and health of AI/ML models and their supporting infrastructure in production environments. It encompasses practices and tools that enable teams to understand how models are performing, detect issues early on, and make data-driven decisions to optimize and maintain these systems. This chapter delves into several key aspects of observability, equipping you with the knowledge and strategies to build more reliable and effective MLOps pipelines.</p> <ul> <li>7.0. Reproducibility: Explore how to make machine learning experiments and pipelines more reproducible using MLflow Projects, enabling others to verify findings, share knowledge, and build upon existing work.</li> <li>7.1. Monitoring: Learn the fundamental principles and tools for monitoring AI/ML models, focusing on tracking key metrics, setting up alerts, and understanding changes in model behavior using MLflow Evaluate API and Evidently.</li> <li>7.2. Alerting: Understand how to design effective alert systems to promptly notify stakeholders of potential issues with models or infrastructure using tools like Slack, Discord, Datadog, and PagerDuty.</li> <li>7.3. Lineage: Delve into data and model lineage, discovering how to track the origin and transformation of data and models throughout the ML lifecycle using MLflow Dataset.</li> <li>7.4. Costs and KPIs: Explore techniques for managing costs associated with running AI/ML workloads and for defining and tracking key performance indicators (KPIs) aligned with business goals, using MLflow Tracking for analysis.</li> <li>7.5. Explainability: Explore the concept of explainable AI, focusing on techniques like SHAP to understand model predictions and build trust in AI systems.</li> <li>7.6. Infrastructure: Discover the importance of infrastructure monitoring, learning how to track resource usage and performance metrics to optimize efficiency and costs through MLflow System Metrics.</li> </ul>"},{"location":"7.%20Observability/0.%20Reproducibility.html","title":"7.0. Reproducibility","text":""},{"location":"7.%20Observability/0.%20Reproducibility.html#what-is-reproducibility-in-mlops","title":"What is reproducibility in MLOps?","text":"<p>Reproducibility in MLOps means being able to reliably recreate the results of an AI/ML experiment or workflow. This capability is crucial for validating findings, debugging models, and ensuring consistent behavior across different environments and over time. Reproducibility helps build trust and transparency in AI/ML projects, allowing for independent verification and accelerating future development by providing a stable foundation to build upon.</p>"},{"location":"7.%20Observability/0.%20Reproducibility.html#why-is-reproducibility-important-in-mlops","title":"Why is reproducibility important in MLOps?","text":"<p>Reproducibility is a cornerstone in any scientific endeavor, and machine learning is no exception. It ensures that results are not due to chance or specific environmental configurations. This rigor builds trust in the models, making them more reliable for deployment. Additionally, reproducibility is crucial for debugging and fixing issues. If a model's performance degrades unexpectedly, having a reproducible setup allows you to isolate the changes that caused the issue and quickly restore the model's effectiveness.</p>"},{"location":"7.%20Observability/0.%20Reproducibility.html#how-can-you-implement-reproducibility-in-your-mlops-projects","title":"How can you implement reproducibility in your MLOps projects?","text":"<p>Implementing reproducibility in MLOps projects necessitates a combination of tools and practices:</p> <ul> <li>Code Versioning: Utilizing tools like Git to track code changes and revert to specific versions allows you to precisely reproduce the code that generated particular results. This is essential for understanding the evolution of a model and recreating previous experiments.</li> <li>Environment Management: Ensuring that the environment (e.g., Python version, libraries, dependencies) in which an experiment is conducted is consistent is vital. Employing tools like Docker or uv to encapsulate dependencies and manage environments promotes consistency and portability.</li> <li>Dataset Versioning: Tracking changes to the dataset used for training or evaluation is crucial. This could involve storing multiple versions of the dataset or logging metadata about the dataset's source with MLflow Data.</li> <li>Randomness Control: Inherently, AI/ML tasks often involve randomness in model initialization, data shuffling, or algorithm execution. To achieve reproducibility, you must control this randomness by fixing random seeds, which ensures that random number generators produce the same sequence of numbers, thereby leading to consistent results.</li> <li>Experiment Tracking: Employing tools like MLflow to log experiment parameters, metrics, and artifacts allows you to systematically document your experiments. This meticulous logging enables you to review past experiments, compare results, and identify the precise configurations that led to certain outcomes.</li> </ul>"},{"location":"7.%20Observability/0.%20Reproducibility.html#how-can-you-fix-randomness-in-aiml-frameworks","title":"How can you fix randomness in AI/ML frameworks?","text":"<p>By setting a specific seed, you ensure that the generator always produces the same sequence of \"random\" numbers, leading to consistent results across different executions of your code, even if those executions occur on different machines or at different times.</p> <p>Here is how you can fix the randomness in your project for several popular machine learning frameworks.</p>"},{"location":"7.%20Observability/0.%20Reproducibility.html#python","title":"Python","text":"<pre><code>import random\n\nrandom.seed(42)\n</code></pre>"},{"location":"7.%20Observability/0.%20Reproducibility.html#numpy","title":"NumPy","text":"<pre><code>import numpy as np\n\nnp.random.seed(42)\n</code></pre>"},{"location":"7.%20Observability/0.%20Reproducibility.html#scikit-learn","title":"Scikit-learn","text":"<pre><code>from sklearn.ensemble import RandomForestRegressor\n\nmodel = RandomForestRegressor(random_state=42)\n</code></pre>"},{"location":"7.%20Observability/0.%20Reproducibility.html#pytorch","title":"PyTorch","text":"<pre><code>import torch\n\ntorch.manual_seed(42)\n</code></pre> <p>You can also fix the randomness for CUDA operations by using:</p> <pre><code>if torch.cuda.is_available():\n    torch.cuda.manual_seed_all(42)\n</code></pre> <p>For additional reproducibility in multi-GPU environments, consider setting:</p> <pre><code>torch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n</code></pre>"},{"location":"7.%20Observability/0.%20Reproducibility.html#tensorflow","title":"TensorFlow","text":"<pre><code>import tensorflow as tf\n\ntf.random.set_seed(42)\n</code></pre>"},{"location":"7.%20Observability/0.%20Reproducibility.html#how-can-you-use-mlflow-projects-to-improve-the-reproducibility-of-your-project","title":"How can you use MLflow Projects to improve the reproducibility of your project?","text":"<p>MLflow Projects is a component of MLflow that provides a standard format for packaging data science code in a reusable and reproducible way. An MLflow Project is defined by an <code>MLproject</code> file that specifies the project's dependencies, environment, and entry points. This standardized format makes it easier to share and execute projects across different environments and platforms, promoting both collaboration and consistency in project execution.</p>"},{"location":"7.%20Observability/0.%20Reproducibility.html#defining-an-mlflow-project","title":"Defining an MLflow Project","text":"<p>To define an MLflow project, you can create an <code>MLproject</code> file in your project's root directory. This file uses YAML syntax to define the project structure. Below is an example of an <code>MLproject</code> file that specifies the project name, environment, and entry point:</p> <pre><code># https://mlflow.org/docs/latest/projects.html\n\nname: bikes\npython_env: python_env.yaml\nentry_points:\n  main:\n    parameters:\n      conf_file: path\n    command: \"PYTHONPATH=src python -m bikes {conf_file}\"\n</code></pre> <p>In this example:</p> <ul> <li><code>name</code> defines the project name as \"bikes\".</li> <li><code>python_env</code> specifies the path to the python environment file.</li> <li><code>entry_points</code> defines entry points, which specify how to run parts of the project.<ul> <li><code>main</code> is an entry point that accepts one parameters: <code>conf_file</code> as a file path.</li> <li>The <code>command</code> specifies how to execute the entry point, which in this case runs the <code>bikes</code> module with the provided parameters.</li> </ul> </li> </ul>"},{"location":"7.%20Observability/0.%20Reproducibility.html#executing-an-mlflow-project","title":"Executing an MLflow Project","text":"<p>To run an MLflow Project:</p> <pre><code>mlflow run --experiment-name=bikes --run-name=Training -P conf_file=confs/training.yaml .\"\n</code></pre> <p>This command instructs MLflow to run the current directory (<code>.</code>) as a project. The <code>-P</code> flag allows you to pass parameters to the entry points defined in your <code>MLproject</code> file. In this case, it passes <code>confs/training.yaml</code> as the main configuration file.</p>"},{"location":"7.%20Observability/0.%20Reproducibility.html#benefits-of-using-mlflow-projects","title":"Benefits of Using MLflow Projects","text":"<ul> <li>Simplified Sharing: It's easier to share and distribute projects.</li> <li>Consistent Execution: Ensures consistent execution across different environments.</li> <li>Reduced Setup Time: Minimizes the time and effort required to set up and run projects.</li> <li>Collaboration: Facilitates collaboration among team members.</li> </ul> <p>By leveraging MLflow Projects, you can significantly enhance the reproducibility of your MLOps projects, making it easier to share, execute, and validate your experiments, contributing to the overall robustness and trustworthiness of your ML solutions.</p>"},{"location":"7.%20Observability/0.%20Reproducibility.html#reproducibility-additional-resources","title":"Reproducibility additional resources","text":"<ul> <li>MLflow Project example from the MLOps Python Package</li> <li>MLflow Project execution from the MLOps Python Package</li> <li>MLflow Projects</li> </ul>"},{"location":"7.%20Observability/1.%20Monitoring.html","title":"7.1. Monitoring","text":""},{"location":"7.%20Observability/1.%20Monitoring.html#what-is-aiml-monitoring","title":"What is AI/ML Monitoring?","text":"<p>AI/ML Monitoring is the continuous process of overseeing the performance, behavior, and health of machine learning models in production environments. It's an essential aspect of MLOps that extends beyond the initial training and deployment stages, ensuring that models remain effective and reliable throughout their operational lifecycle. Effective monitoring involves several key tasks, including:</p> <ul> <li>Tracking Metrics: Capturing and analyzing key performance indicators (KPIs) that reflect the model's accuracy, precision, recall, and other relevant metrics over time.</li> <li>Setting up Alerts: Establishing trigger mechanisms that alert stakeholders when specific conditions are met, such as a significant drop in model accuracy or the detection of data drift.</li> <li>Gaining Insights: Providing a means to understand and diagnose issues through visualizations, logs, and other diagnostic tools.</li> </ul> <p>Effective AI/ML monitoring helps prevent model decay, which occurs when model performance deteriorates over time, by quickly identifying issues, enabling timely interventions. It is crucial for both technical teams and business stakeholders to maintain confidence in the accuracy and value of AI/ML solutions.</p>"},{"location":"7.%20Observability/1.%20Monitoring.html#how-does-aiml-monitoring-differ-from-traditional-software-monitoring","title":"How does AI/ML Monitoring differ from Traditional Software Monitoring?","text":"<p>While AI/ML monitoring shares similarities with traditional software monitoring, it also presents unique challenges:</p> <ul> <li>Non-Deterministic Behavior: Machine learning models can exhibit unpredictable behavior due to their reliance on data patterns. These patterns may change over time, leading to performance degradation or unexpected outputs.</li> <li>Complex Dependencies: AI/ML applications often depend on multiple external factors, including data sources, feature engineering pipelines, and serving infrastructure. Monitoring must encompass all these dependencies to identify potential sources of issues.</li> <li>Black Box Nature: The internal workings of some machine learning models can be opaque, making it harder to directly diagnose the reasons for incorrect predictions or behavior changes.</li> </ul> <p>These unique challenges call for specialized tools and strategies, as traditional monitoring systems may not be adequately equipped to address the complexities inherent in AI/ML applications.</p>"},{"location":"7.%20Observability/1.%20Monitoring.html#what-are-the-benefits-of-aiml-monitoring","title":"What are the Benefits of AI/ML Monitoring?","text":"<ol> <li>Early Problem Detection: By continuously monitoring metrics and setting up alerts, teams can detect problems such as model drift, data quality issues, or biases before they significantly impact business outcomes.</li> <li>Improved Model Performance: Tracking metrics helps identify opportunities for model retraining, hyperparameter tuning, or other optimizations to enhance model performance over time.</li> <li>Increased Reliability: Effective monitoring safeguards against model failures and downtime, ensuring that AI/ML solutions remain stable and dependable, maintaining business continuity.</li> <li>Enhanced User Trust: By ensuring model accuracy and fairness, monitoring practices build trust and confidence in AI/ML solutions among users, promoting their adoption and use.</li> </ol> <p>AI/ML monitoring plays a crucial role in bridging the gap between model development and production, ensuring that these solutions remain valuable and reliable assets for businesses.</p>"},{"location":"7.%20Observability/1.%20Monitoring.html#which-metrics-should-you-track-for-aiml-monitoring","title":"Which Metrics Should You Track for AI/ML Monitoring?","text":"<p>The metrics tracked for AI/ML monitoring should align with the specific objectives and performance requirements of the model:</p>"},{"location":"7.%20Observability/1.%20Monitoring.html#common-metrics","title":"Common Metrics","text":"<ul> <li>Accuracy, Precision, Recall, F1 Score: These metrics assess the model's overall performance and its ability to correctly classify or predict outcomes.</li> <li>Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE): These metrics quantify the magnitude of prediction errors, crucial for regression problems.</li> <li>Area Under the Receiver Operating Characteristic Curve (AUC-ROC): This metric evaluates the model's ability to distinguish between different classes, particularly useful for binary classification problems.</li> </ul>"},{"location":"7.%20Observability/1.%20Monitoring.html#business-metrics","title":"Business Metrics","text":"<p>Beyond technical metrics, aligning monitoring with business goals through relevant metrics is crucial:</p> <ul> <li>Conversion Rate, Customer Churn, Revenue Impact: These metrics measure the direct impact of the model on business outcomes, offering a clear view of its effectiveness.</li> </ul>"},{"location":"7.%20Observability/1.%20Monitoring.html#data-quality-metrics","title":"Data Quality Metrics","text":"<ul> <li>Data Drift, Missing Values, Outliers: Monitoring data quality metrics helps detect changes in input data distribution or format that could impact model performance.</li> </ul>"},{"location":"7.%20Observability/1.%20Monitoring.html#how-can-you-implement-aiml-monitoring-with-mlflow","title":"How can you implement AI/ML Monitoring with MLflow?","text":"<p>The MLOps Python Package utilizes Mlflow's <code>evaluate</code> API for comprehensive model evaluation, including the validation of results with user-defined thresholds. This capability allows for a standardized approach to model monitoring, ensuring that model quality is consistently assessed and monitored.</p> <p>Here's how you can implement this monitoring functionality:</p> <ol> <li> <p>Define Metrics:  First, define the metrics you want to track using a class from the <code>bikes.core.metrics</code> module. This class allows you to specify the metric name and whether a higher or lower score indicates better performance.</p> <pre><code>from bikes.core import metrics\n\nmetrics = [\n    metrics.SklearnMetric(name=\"mean_squared_error\", greater_is_better=False),\n    metrics.SklearnMetric(name=\"r2_score\", greater_is_better=True),\n]\n</code></pre> </li> <li> <p>Set Thresholds (Optional): If you want to establish thresholds for specific metrics, use the <code>Threshold</code> class, again from <code>bikes.core.metrics</code>, to define the absolute threshold value and whether a higher or lower score is desired. These thresholds serve as benchmarks for model performance, potentially triggering alerts if violated.</p> <pre><code>thresholds = {\n    \"r2_score\": metrics.Threshold(threshold=0.5, greater_is_better=True)\n}\n</code></pre> </li> <li> <p>Integrate with the <code>EvaluationsJob</code>: The <code>EvaluationsJob</code> in the <code>bikes.jobs.evaluations</code> module is responsible for loading the registered model, reading the evaluation dataset, and calculating the specified metrics. You can configure this job to use the defined metrics and thresholds.</p> <pre><code>from bikes import jobs\nfrom bikes.io import datasets\n\nevaluations_job = jobs.EvaluationsJob(\n    inputs=datasets.ParquetReader(path=\"data/inputs_test.parquet\"),\n    targets=datasets.ParquetReader(path=\"data/targets_test.parquet\"),\n    metrics=metrics,\n    thresholds=thresholds,\n)\n</code></pre> </li> <li> <p>Execute the Job: Run the <code>EvaluationsJob</code> to compute the metrics and assess them against the thresholds. If any thresholds are violated, MLflow will raise a <code>ModelValidationFailedException</code>, which can be handled appropriately in your workflow.</p> <p><pre><code>with evaluations_job as runner:\n    runner.run()\n</code></pre> </p> </li> </ol>"},{"location":"7.%20Observability/1.%20Monitoring.html#how-to-integrate-aiml-monitoring-to-your-data-infrastructure","title":"How to integrate AI/ML Monitoring to your data infrastructure?","text":"<p>You can use the Evidently library to generate interactive reports for model monitoring and analysis.  Evidently supports data and target drift detection, model performance monitoring, and the creation of visual reports to understand changes and issues within an ML pipeline. It simplifies the tracking and analysis of changes in model behavior, making the monitoring process more efficient and effective.</p> <p>Here are the steps to integrate Evidently into your Jupyter notebooks for model monitoring:</p> <ol> <li>Install the Evidently library:</li> </ol> <pre><code>pip install evidently\n</code></pre> <ol> <li>Import necessary modules:</li> </ol> <pre><code>import pandas as pd\nfrom evidently.report import Report\nfrom evidently.metric_preset import DataDriftPreset\n</code></pre> <ol> <li>Load your reference and current data: These datasets represent the data the model was trained on (reference) and the data the model is currently making predictions on (current).</li> </ol> <pre><code>reference_data = pd.read_csv('reference.csv')\ncurrent_data = pd.read_csv('current.csv')\n</code></pre> <ol> <li>Generate an Evidently report:</li> </ol> <pre><code>report = Report(metrics=[DataDriftPreset()])\nreport.run(reference_data=reference_data, current_data=current_data)\nreport.show() # or report.save_html('my_report.html')\n</code></pre> <p>This example will generate a report highlighting any data drift between the reference and current datasets, which can be crucial in identifying why a model's performance might be declining.</p>"},{"location":"7.%20Observability/1.%20Monitoring.html#what-are-the-best-practices-for-aiml-monitoring","title":"What are the Best Practices for AI/ML Monitoring?","text":"<ol> <li>Establish Clear Monitoring Goals: Define the objectives of your monitoring efforts, which might include detecting drift, maintaining performance, or ensuring fairness.</li> <li>Choose Relevant Metrics: Select metrics that align with the specific objectives of your model and your business goals.</li> <li>Set up Meaningful Alerts: Design alerts that are actionable and relevant, avoid alert fatigue, and ensure timely responses to critical issues.</li> <li>Integrate Monitoring into CI/CD Pipelines: Incorporate monitoring steps into your continuous integration and continuous deployment (CI/CD) workflows to ensure that changes are thoroughly evaluated and monitored.</li> <li>Visualize Results: Utilize visual dashboards and reports to present monitoring results in an easily understandable format for both technical and business stakeholders.</li> <li>Regularly Review and Update: Periodically review your monitoring setup and adjust metrics, thresholds, and alerts as needed based on experience and changing requirements.</li> </ol>"},{"location":"7.%20Observability/1.%20Monitoring.html#monitoring-additional-resources","title":"Monitoring Additional Resources","text":"<ul> <li>Example from the MLOps Python Package</li> <li>MLflow Evaluate API</li> <li>EvidentlyAI</li> <li>Is AI/ML Monitoring just Data Engineering? \ud83e\udd14</li> <li>Model Monitoring: What it is and why it's so hard</li> </ul>"},{"location":"7.%20Observability/2.%20Alerting.html","title":"7.2. Alerting","text":""},{"location":"7.%20Observability/2.%20Alerting.html#what-is-aiml-alerting","title":"What is AI/ML Alerting?","text":"<p>AI/ML Alerting is a crucial aspect of monitoring that involves sending notifications to relevant stakeholders when specific conditions are met, signifying potential issues or deviations in the behavior or performance of a deployed machine learning model. These alerts act as early warning systems, enabling timely intervention and mitigation of problems before they significantly impact business outcomes or user experience.</p> <p>Effective alerting relies on:</p> <ul> <li>Defining Alert Conditions: Establishing clear criteria that trigger alerts, such as significant drops in model accuracy, exceeding error thresholds, or the detection of data drift.</li> <li>Identifying Recipients: Specifying individuals or teams responsible for responding to alerts, ensuring that notifications reach the right people.</li> <li>Selecting Communication Channels: Choosing appropriate methods for delivering alerts, whether it's email, Slack messages, system notifications, or specialized alerting platforms.</li> </ul>"},{"location":"7.%20Observability/2.%20Alerting.html#why-do-you-need-alerting-for-aiml","title":"Why do you need Alerting for AI/ML?","text":"<p>Alerting plays a critical role in maintaining the reliability and value of AI/ML solutions in production environments. Its main benefits include:</p> <ol> <li>Rapid Response to Issues: Alerts provide a mechanism for quickly informing stakeholders of problems, reducing the time it takes to diagnose and mitigate them.</li> <li>Proactive Issue Mitigation: By detecting issues early, alerting systems enable proactive interventions, preventing minor problems from escalating into major disruptions or failures.</li> <li>Improved Decision Making: Alerts can provide valuable data and insights that help teams make informed decisions on model retraining, hyperparameter adjustments, or other optimizations.</li> <li>Increased System Uptime: By minimizing downtime and ensuring model accuracy, alerting contributes to the overall stability and reliability of AI/ML applications.</li> </ol> <p>Effective AI/ML alerting acts as a safety net, safeguarding against model decay and unforeseen circumstances that could impact the performance or behavior of deployed models.</p>"},{"location":"7.%20Observability/2.%20Alerting.html#which-conditions-should-trigger-an-alert","title":"Which conditions should trigger an alert?","text":"<p>Deciding which conditions should trigger an alert involves considering the specific context of the model and its impact on business goals. Here are some common scenarios:</p> <ul> <li>Performance Degradation: When a model's accuracy, precision, recall, or other key metrics drop significantly below predefined thresholds.</li> <li>Data Drift: When the distribution or characteristics of the input data change significantly compared to the data used for training the model, which might signal a need for retraining.</li> <li>Bias Detection: When the model demonstrates bias towards certain groups or categories, which could lead to unfair or unethical outcomes.</li> <li>System Errors: When errors or exceptions occur within the model serving infrastructure or dependencies, potentially impacting service availability.</li> <li>Resource Utilization: When the model's resource consumption, such as CPU, memory, or network bandwidth, exceeds predefined limits, which might signal performance bottlenecks.</li> </ul>"},{"location":"7.%20Observability/2.%20Alerting.html#which-platforms-can-send-alerts","title":"Which platforms can send alerts?","text":"<p>To implement an effective alerting system, you need to choose communication channels that align with your team\u2019s workflow and preferences. Here are some common options for sending AI/ML alerts:</p> <ul> <li>Slack and Discord: Suitable for real-time team communication, these messaging platforms allow for instant notifications, discussions, and collaboration among team members.</li> <li>Datadog: A popular monitoring and observability platform, it provides comprehensive alerting capabilities for various system and application metrics, including those related to AI/ML models.</li> <li>Statuspal: This platform specializes in status page monitoring and incident communication, making it useful for notifying users about any disruptions or downtime related to AI/ML services.</li> <li>PagerDuty: A popular incident management platform that can be used for routing AI/ML alerts to the right team members, escalating issues if necessary, and ensuring that incidents are addressed promptly.</li> </ul>"},{"location":"7.%20Observability/2.%20Alerting.html#how-can-you-implement-alerting-local-demo","title":"How can you implement Alerting (local demo)?","text":"<p>The MLOps Python Package includes a simple alerting service based on the <code>plyer</code> library, which provides cross-platform notifications. This service can be used to send alerts to the user's desktop, offering a basic but effective way to notify developers about events during model training, tuning, or deployment.</p> <p>Here's how you can use the alerting service:</p> <ol> <li> <p>Configure the <code>AlertsService</code>: Set the <code>enable</code> parameter to <code>True</code> to activate the alerting feature. You can also customize the <code>app_name</code> and <code>timeout</code> parameters as needed.</p> <pre><code>from bikes.io import services\n\nalerts_service = services.AlertsService(enable=True, app_name=\"Bikes\", timeout=5)\n</code></pre> </li> <li> <p>Integrate with a Job: Include the <code>alerts_service</code> as a parameter within a job definition. For instance, you can add it to the <code>TrainingJob</code> to receive a notification when model training is complete.</p> <pre><code>from bikes import jobs\n\ntraining_job = jobs.TrainingJob(\n    ...,\n    alerts_service=alerts_service,\n)\n</code></pre> </li> <li> <p>Send Alerts: Use the <code>alerts_service.notify()</code> method within the job's <code>run()</code> method to trigger notifications based on specific conditions.</p> <pre><code># Within the TrainingJob's run() method\n# ... (training logic)\nself.alerts_service.notify(title=\"Training Complete\", message=f\"Model version: {model_version.version}\")\n</code></pre> </li> <li> <p>Disable for Production: Remember to disable desktop notifications (<code>enable=False</code>) in production settings to avoid disrupting users. Instead, integrate with more appropriate alerting platforms as mentioned earlier.</p> </li> </ol>"},{"location":"7.%20Observability/2.%20Alerting.html#what-are-the-best-practices-for-aiml-alerting","title":"What are the Best Practices for AI/ML Alerting?","text":"<ol> <li>Prioritize Alerts: Categorize alerts by severity to ensure that critical issues receive immediate attention, while less urgent notifications can be handled later.</li> <li>Provide Context: Include relevant information in alert messages, such as the affected model, specific metrics, and potential causes.</li> <li>Avoid Alert Fatigue: Strike a balance between informing stakeholders and overwhelming them with too many notifications.</li> <li>Automate Actions: Where possible, automate responses to alerts, such as triggering model retraining, scaling infrastructure, or rolling back to a previous model version.</li> <li>Regularly Review and Update: Periodically assess the effectiveness of your alerting system and adjust alert conditions, recipients, and channels as needed based on feedback and experience.</li> </ol>"},{"location":"7.%20Observability/2.%20Alerting.html#alerting-additional-resources","title":"Alerting Additional Resources","text":"<ul> <li>Example from the MLOps Python Package</li> <li>Alerting in Datadog</li> <li>Slack API Documentation</li> <li>Discord Developer Documentation</li> <li>PagerDuty</li> <li>Statuspal</li> <li>Plyer</li> </ul>"},{"location":"7.%20Observability/3.%20Lineage.html","title":"7.3. Lineage","text":""},{"location":"7.%20Observability/3.%20Lineage.html#what-is-data-and-model-lineage","title":"What is Data and Model Lineage?","text":"<p>Data and Model Lineage in Machine Learning Operations (MLOps) refers to the comprehensive tracking of data and model origins, transformations, and dependencies throughout the machine learning lifecycle. It provides a historical record of how data flows through various stages, from its initial source to the final model outputs, which is crucial for understanding, debugging, and ensuring the reliability of AI/ML systems.</p> <p>Lineage tracking encompasses:</p> <ul> <li>Data Origin: Identifying the initial source of data, including databases, APIs, or files.</li> <li>Data Transformations: Recording all processing steps applied to the data, such as cleaning, feature engineering, and aggregations.</li> <li>Model Training: Capturing details about the model training process, including hyperparameters, training data used, and resulting model versions.</li> <li>Model Deployment: Tracking where and how the model is deployed, including serving infrastructure and deployment timestamps.</li> </ul>"},{"location":"7.%20Observability/3.%20Lineage.html#why-do-you-need-data-and-model-lineage","title":"Why do you need Data and Model Lineage?","text":"<p>Lineage information offers multiple benefits for managing and deploying machine learning models effectively:</p> <ol> <li>Debugging and Troubleshooting: When an issue arises, lineage information helps trace the origin of the problem by providing a clear picture of the data flow and model dependencies.</li> <li>Data and Model Governance: It enables auditing and compliance by providing a transparent record of data usage, transformations, and model versions.</li> <li>Reproducibility: It enhances reproducibility by documenting the exact steps involved in creating a model, making it easier to recreate results or validate findings.</li> <li>Impact Analysis: Lineage data can be used to assess the impact of changes to data or models on downstream applications or business processes.</li> </ol>"},{"location":"7.%20Observability/3.%20Lineage.html#what-are-the-main-data-lineage-use-cases","title":"What are the main Data Lineage use cases?","text":"<p>Data Lineage provides critical insights into the flow and usage of data within your ML projects, which can be invaluable for various purposes. Here are some common use cases:</p>"},{"location":"7.%20Observability/3.%20Lineage.html#data-discovery","title":"Data Discovery","text":"<p>Lineage information makes it easier to discover and locate relevant datasets within your MLOps system. By tracing the lineage of models or predictions, you can identify the source datasets used, understand their characteristics, and explore their relationships with other datasets. This discovery process simplifies finding the right data for new projects or validating the suitability of existing datasets for specific tasks.</p>"},{"location":"7.%20Observability/3.%20Lineage.html#impact-analysis","title":"Impact Analysis","text":"<p>Understanding the impact of changes to data or models on downstream processes is crucial for maintaining system reliability. Lineage information facilitates this analysis by providing a visual representation of data dependencies. For example, if you make changes to a feature engineering step, you can use lineage tracking to determine which models rely on those features, allowing you to assess the potential impact on their performance and take necessary actions, such as retraining or retesting affected models.</p>"},{"location":"7.%20Observability/3.%20Lineage.html#data-governance-and-compliance","title":"Data Governance and Compliance","text":"<p>Meeting regulatory requirements and maintaining data quality often necessitate a clear understanding of data usage and its transformations. Lineage tracking offers a robust audit trail for data provenance, which can be used for compliance reporting and to ensure adherence to data governance policies. For example, tracking the lineage of personal data used in ML models can help demonstrate compliance with privacy regulations like GDPR, ensuring that data is processed and used according to established guidelines.</p>"},{"location":"7.%20Observability/3.%20Lineage.html#how-to-implement-lineage-tracking-with-mlflow-datasets","title":"How to Implement Lineage Tracking with MLflow Datasets?","text":"<p>The MLOps Python Package leverages MLflow's Dataset API to log dataset information during the execution of various jobs, such as training or tuning. This provides a streamlined method for integrating lineage tracking directly into the MLflow tracking system, ensuring that dataset details are captured alongside model metrics and parameters.</p>"},{"location":"7.%20Observability/3.%20Lineage.html#implementing-lineage-tracking-in-the-mlops-python-package","title":"Implementing Lineage Tracking in the MLOps Python Package","text":"<p>Here's how lineage is implemented within the MLOps Python Package:</p> <ol> <li> <p>Defining Dataset Readers: Within the <code>bikes.io.datasets</code> module, the <code>Reader</code> class serves as an abstract base class for all dataset readers. It defines the <code>lineage()</code> method, which is responsible for generating lineage information.</p> <pre><code>import abc\n\nclass Reader(abc.ABC):\n    @abc.abstractmethod\n    def lineage(\n        self, name: str, data: pd.DataFrame, targets: str | None = None, predictions: str | None = None,\n    ) -&gt; Lineage:\n</code></pre> </li> <li> <p>Implementing Lineage in Concrete Readers: Concrete readers, such as the <code>ParquetReader</code>, implement the <code>lineage()</code> method, utilizing the <code>mlflow.data.pandas_dataset.from_pandas</code> function to create an MLflow Dataset object. This object captures the dataset's name, source, schema, and profile information.</p> <pre><code>import mlflow.data.pandas_dataset as lineage\nimport pandas as pd\n\nclass ParquetReader(Reader):\n    # ... (other methods)\n\n    @T.override\n    def lineage(\n        self, name: str, data: pd.DataFrame, targets: str | None = None, predictions: str | None = None,\n    ) -&gt; Lineage:\n        return lineage.from_pandas(\n            df=data, name=name, source=self.path, targets=targets, predictions=predictions\n        )\n</code></pre> </li> <li> <p>Logging Lineage in Jobs: Jobs, like the <code>TrainingJob</code> or <code>TuningJob</code>, utilize the <code>reader.lineage()</code> method to generate lineage information for input datasets and then log this information to MLflow using <code>mlflow.log_input()</code>.</p> <pre><code># Within the TrainingJob's run() method\ninputs_lineage = self.inputs.lineage(data=inputs, name=\"inputs\")\nmlflow.log_input(dataset=inputs_lineage, context=self.run_config.name)\n</code></pre> </li> </ol>"},{"location":"7.%20Observability/3.%20Lineage.html#visualizing-lineage-in-mlflow","title":"Visualizing Lineage in MLflow","text":"<p>The MLflow UI provides a visual representation of lineage information, enabling users to see the flow of data from its source to the models and predictions. This view simplifies understanding how different datasets are used within the project and facilitates debugging by tracing the lineage of specific models or predictions.</p> <p></p>"},{"location":"7.%20Observability/3.%20Lineage.html#enhancing-lineage-tracking","title":"Enhancing Lineage Tracking","text":"<p>To further enrich lineage information, consider these practices:</p> <ul> <li>Log Transformations: During data preprocessing, include logging statements to record specific transformations applied. This can be done with custom tags or attributes within the lineage object.</li> <li>Utilize Data Versioning: Version control systems for data, like DVC, can be integrated to track different dataset versions used for model training or evaluation.</li> <li>Capture Feature Engineering Steps: Document the logic behind feature creation or selection to understand how features impact model outcomes.</li> <li>Track Model Deployment Lineage: Integrate deployment platforms with your lineage tracking system to capture where and how models are deployed.</li> </ul> <p>By embracing these practices, you can significantly improve the transparency and auditability of your AI/ML systems.</p>"},{"location":"7.%20Observability/3.%20Lineage.html#lineage-additional-resources","title":"Lineage Additional Resources","text":"<ul> <li>Example from the MLOps Python Package</li> <li>MLflow Tracking Dataset</li> <li>OpenLineage and Marquez</li> </ul>"},{"location":"7.%20Observability/4.%20Costs-KPIs.html","title":"7.4. Costs and KPIs","text":""},{"location":"7.%20Observability/4.%20Costs-KPIs.html#what-are-costs-and-kpis-in-mlops","title":"What are costs and KPIs in MLOps?","text":"<p>Costs in the context of Machine Learning Operations (MLOps) refer to the financial expenditures associated with developing, deploying, and maintaining machine learning models. These costs encompass various factors including:</p> <ul> <li>Infrastructure: This covers the costs of using cloud computing resources (e.g., virtual machines, storage, databases), on-premise servers, and specialized hardware like GPUs, essential for training and deploying models.</li> <li>Data: Obtaining, cleaning, labeling, and storing data can incur significant costs. The size, complexity, and source of the data influence these costs.</li> <li>Personnel: Hiring and managing data scientists, machine learning engineers, data engineers, DevOps professionals, and other personnel involved in the MLOps lifecycle constitutes a significant portion of costs.</li> <li>Software and Tools: Licensing fees for machine learning libraries, frameworks, data processing tools, monitoring platforms, and other software tools contribute to overall costs.</li> <li>Operational Expenses: Day-to-day running costs, such as electricity, network bandwidth, security measures, and maintenance for hardware and software, fall under operational expenses.</li> </ul> <p>Key Performance Indicators (KPIs) are metrics used to assess the success and effectiveness of MLOps projects. These KPIs vary based on the project's goals but generally include:</p> <ul> <li>Model Accuracy: Metrics like precision, recall, F1-score, and AUC measure how well the model predicts outcomes.</li> <li>Model Performance: This covers metrics related to the model's speed, such as inference latency and throughput, crucial for real-time applications.</li> <li>Data Quality: Metrics like data completeness, accuracy, and timeliness assess the reliability of the data used to train and evaluate models.</li> <li>Deployment Efficiency: This measures how smoothly and rapidly models are deployed, including metrics like deployment time and frequency.</li> <li>Operational Stability: Uptime, error rates, and resource utilization track the stability and reliability of deployed models.</li> <li>Business Impact: These metrics directly measure the model's impact on the business, such as increased revenue, reduced costs, improved customer satisfaction, or enhanced decision-making.</li> </ul> <p>By monitoring and optimizing both costs and KPIs, organizations can ensure that their MLOps projects are not only technically successful but also financially viable and deliver tangible business value.</p>"},{"location":"7.%20Observability/4.%20Costs-KPIs.html#why-should-you-track-costs-and-kpis","title":"Why should you track costs and KPIs?","text":"<p>Tracking costs and KPIs is essential for several reasons:</p> <ul> <li>Optimize Resource Allocation: By understanding where costs are incurred, organizations can identify areas for improvement and make informed decisions about resource allocation, maximizing Return on Investment (ROI).</li> <li>Measure Progress and Success: Tracking KPIs helps teams evaluate the progress and effectiveness of their efforts, ensuring they stay aligned with business objectives.</li> <li>Identify Bottlenecks:  Tracking performance metrics can help pinpoint bottlenecks in the MLOps pipeline, such as slow data processing, inefficient model training, or deployment delays, enabling targeted improvements.</li> <li>Support Data-Driven Decision-Making:  Accurate data on costs and performance metrics empowers teams to make data-driven decisions, driving continuous improvement and innovation.</li> </ul>"},{"location":"7.%20Observability/4.%20Costs-KPIs.html#how-can-you-perform-a-back-of-the-envelope-calculation","title":"How can you perform a back-of-the-envelope calculation?","text":"<p>Back-of-the-envelope calculations are a quick and simple way to estimate costs and KPIs, providing a preliminary understanding of the financial and performance aspects of an MLOps project. These estimates, while not precise, offer valuable insights for early planning and resource allocation decisions.</p> <p>For example, to estimate the cost of using a cloud computing instance for model training:</p> <ol> <li>Identify Resource Requirements: Determine the type of instance needed (e.g., CPU-based, GPU-based), the number of instances, and the duration of training time.</li> <li>Obtain Pricing Information: Consult the cloud provider's pricing information to determine the hourly rate for the chosen instance type.</li> <li>Estimate Total Cost: Multiply the hourly rate by the estimated training time and the number of instances to get an approximate cost.</li> </ol> <p>Similarly, you can estimate KPIs based on available data and historical information. For instance, if you have past data on model accuracy and deployment frequency, you can use these values to estimate the expected performance and deployment efficiency of a new model.</p> <p>Back-of-the-envelope calculations can be refined as you gain more information about the project's specifics. Remember, these estimates are primarily for initial planning and should be revisited and updated as the project progresses.</p>"},{"location":"7.%20Observability/4.%20Costs-KPIs.html#how-can-you-find-good-kpi-metrics-for-your-mlops-project","title":"How can you find good KPI metrics for your MLOps project?","text":"<p>Identifying appropriate KPIs for your MLOps project involves aligning them with your business goals:</p> <ul> <li>Start with Business Objectives: Define clear business objectives that the MLOps project aims to achieve. For instance, if the goal is to improve customer satisfaction through personalized product recommendations, the relevant KPI might be click-through rate or conversion rate.</li> <li>Consider the MLOps Lifecycle: Identify key stages in the MLOps lifecycle, such as data preparation, model training, deployment, and monitoring, and choose KPIs that reflect the performance and efficiency of these stages.</li> <li>Balance Between Technical and Business Metrics: Include both technical metrics (e.g., model accuracy, latency) and business metrics (e.g., revenue impact, customer churn reduction) to gain a holistic view of the project's success.</li> <li>Prioritize Actionable Metrics: Focus on KPIs that are actionable, meaning they can be used to make improvements or drive decision-making.</li> <li>Regularly Review and Update KPIs: As the project evolves and business priorities change, revisit and update the selected KPIs to ensure they remain relevant and valuable.</li> </ul> <p>Here's an example of aligning KPIs with business goals:</p> Business Goal MLOps KPI Measurement Increase online sales Click-through rate on product recommendations Number of clicks on recommendations divided by the number of times recommendations were shown Reduce customer churn Customer churn rate after implementing a churn prediction model Number of customers who churned divided by the total number of customers Improve operational efficiency Model deployment time Time taken to deploy a new model version to production"},{"location":"7.%20Observability/4.%20Costs-KPIs.html#how-can-you-compute-cost-and-kpi-metrics-from-your-mlflow-server","title":"How can you compute cost and KPI metrics from your MLflow server?","text":"<p>Computing technical costs and KPIs can be streamlined using experiment tracking tools like MLflow. The following notebook from the MLOps Python Package demonstrates how to extract valuable insights from an MLflow server, providing a foundation for understanding project performance and costs.</p>"},{"location":"7.%20Observability/4.%20Costs-KPIs.html#notebook-code-breakdown","title":"Notebook Code Breakdown","text":"<p>The provided notebook showcases how to retrieve and analyze data from an MLflow server to compute technical metrics related to experiments, runs, models, and versions:</p> <p>1. Imports: The notebook begins by importing necessary libraries: <code>mlflow</code> for interacting with the MLflow server, <code>pandas</code> for data manipulation, and <code>plotly.express</code> for visualization.</p> <p>2. Options and Configs: This section defines display settings for pandas dataframes and sets configuration parameters like <code>MAX_RESULTS</code> to limit the number of items retrieved and <code>TRACKING_URI</code> and <code>REGISTRY_URI</code> to specify the MLflow server location.</p> <p>3. Clients: It establishes an <code>MlflowClient</code> object to communicate with the MLflow server based on the specified tracking and registry URIs.</p> <p>4. Indicators:  This is the core section where the code retrieves data about experiments, runs, models, and versions from the MLflow server. Each retrieval operation is followed by data processing and transformation steps using pandas:</p> <ul> <li>Experiments: It fetches experiment information, converts timestamps to datetime objects, and displays the first few rows of the dataframe.</li> <li>Runs: Retrieves run data, including start and end times, tags, and other logged information, then prepares the data for analysis and visualization.</li> <li>Models: Fetches information about registered models, converts timestamps to datetime objects, and removes the <code>latest_versions</code> column.</li> <li>Versions: Gets data for model versions, extracts the first alias (if any), converts timestamps to datetime objects, and displays the first few rows.</li> </ul> <p>5. Dashboards:  This section demonstrates the use of <code>plotly.express</code> to create interactive visualizations that offer insights into the collected data:</p> <ul> <li>Experiment Creation Time: Plots a strip chart showing experiment creation times, categorized by lifecycle stage.</li> <li>Model Creation Timestamp: Creates a strip chart displaying the creation timestamps of registered models.</li> <li>Version Creation Timestamp: Generates a strip chart illustrating version creation timestamps, grouped by model name.</li> <li>Run Run Time: Draw a strip chart of the MLflow run time per run name (training, tuning, ...).</li> </ul> <p></p> <ul> <li>Run Start Time: Plots a strip chart showcasing run start times, colored by experiment ID.</li> </ul> <p></p> <ul> <li>Run Estimator Class Distribution:  Creates a bar chart showing the distribution of estimator classes used in runs.</li> </ul> <p></p>"},{"location":"7.%20Observability/4.%20Costs-KPIs.html#interpreting-technical-cost-and-kpi-metrics","title":"Interpreting Technical Cost and KPI Metrics","text":"<p>While this notebook focuses on retrieving and visualizing technical information from MLflow, it provides a basis for deriving cost and KPI metrics. Here's how you can interpret and extend the provided data to gain insights into costs and KPIs:</p> <ul> <li>Compute Usage: By analyzing run start and end times, you can estimate the duration of each run and the total compute time for a project. This information, combined with knowledge of your infrastructure costs, helps in estimating compute costs.</li> <li>Data Processing Time:  Further logging of data processing steps within runs can reveal how much time is spent on data-related tasks, providing insights for optimization.</li> <li>Model Training Efficiency: Track the number of runs and their durations to understand how efficiently models are trained. This can identify bottlenecks and guide improvements in model development workflows.</li> <li>Deployment Frequency: Analyze the creation timestamps of model versions to track deployment frequency, a key indicator of agility and responsiveness.</li> <li>Model Performance Metrics:  Retrieve logged metrics like accuracy, precision, recall, and AUC from runs to assess model performance. Compare these metrics across different runs and models to identify the best performing configurations.</li> </ul>"},{"location":"7.%20Observability/4.%20Costs-KPIs.html#extending-the-notebook-for-cost-analysis","title":"Extending the Notebook for Cost Analysis","text":"<p>To delve deeper into cost analysis, you can:</p> <ol> <li>Incorporate Infrastructure Costs:  Introduce variables representing your infrastructure costs, such as the hourly rate for cloud computing instances or the cost of maintaining on-premise hardware.</li> <li>Compute Resource Utilization: Calculate resource utilization by multiplying run durations by the cost of the used resources.</li> <li>Aggregate Costs:  Sum up the utilization costs across all runs to estimate the total cost of an experiment or project.</li> <li>Visualize Cost Distributions: Create visualizations to show the breakdown of costs across different stages of the MLOps lifecycle.</li> </ol>"},{"location":"7.%20Observability/4.%20Costs-KPIs.html#implementing-business-specific-kpis","title":"Implementing Business-Specific KPIs","text":"<p>Extending the notebook to track business-specific KPIs involves:</p> <ol> <li>Define Relevant Metrics:  Identify the key business metrics that align with your project's objectives.</li> <li>Log These Metrics in MLflow:  Use <code>mlflow.log_metric()</code> to record these metrics during your model training and evaluation runs.</li> <li>Retrieve and Visualize Business KPIs:  Retrieve the logged business KPIs from MLflow and create visualizations to analyze trends and identify improvements.</li> </ol> <p>By incorporating cost and KPI tracking into your MLflow workflows, you gain a comprehensive understanding of your project's performance, enabling informed decision-making and optimized resource allocation for maximum business impact.</p>"},{"location":"7.%20Observability/4.%20Costs-KPIs.html#cost-and-kpi-additional-resources","title":"Cost and KPI Additional Resources","text":"<ul> <li>Example from the MLOps Python Package</li> <li>MLflow Tracking</li> <li>How to Estimate ROI for AI and ML Projects</li> </ul>"},{"location":"7.%20Observability/5.%20Explainability.html","title":"7.5. Explainability","text":""},{"location":"7.%20Observability/5.%20Explainability.html#what-is-explainability-in-aiml","title":"What is Explainability in AI/ML?","text":"<p>Explainability in Artificial Intelligence and Machine Learning (AI/ML) refers to the ability to understand and interpret the reasoning behind a model's predictions or decisions. It's a crucial aspect of responsible AI, particularly as models become more complex and their applications extend to critical domains such as healthcare, finance, and law. Explainability promotes transparency, accountability, and trust in AI systems by making their internal workings accessible to humans, enabling better evaluation of their fairness, bias, and potential risks.</p>"},{"location":"7.%20Observability/5.%20Explainability.html#why-do-you-need-explainability-in-aiml","title":"Why do you need Explainability in AI/ML?","text":"<ul> <li>Build trust in AI systems: Explainable AI promotes confidence in model predictions by providing insights into the factors influencing decisions.</li> <li>Identify and mitigate bias:  Explainability allows for the detection and correction of biases that may be embedded in data or amplified by models.</li> <li>Improve model debugging and development: Understanding how a model works can aid in identifying errors or improving its performance.</li> <li>Meet regulatory requirements: In regulated industries like banking and insurance, explainability is crucial for demonstrating compliance with fair lending or insurance practices.</li> </ul>"},{"location":"7.%20Observability/5.%20Explainability.html#what-is-the-difference-between-local-and-global-explainability","title":"What is the difference between Local and Global Explainability?","text":"<p>Explainability methods can be broadly categorized into two types based on their scope of interpretation:</p> <ul> <li>Local Explainability: Focuses on understanding individual predictions, providing insights into the specific factors influencing a single output. This approach answers questions like \"Why was this loan application rejected?\" or \"Why did the model predict this patient is at high risk?\"<ul> <li>Example: SHAP (SHapley Additive exPlanations) is a widely used technique for local explainability. It assigns a contribution score to each feature for a given prediction, quantifying how each feature influences the outcome.</li> </ul> </li> <li>Global Explainability: Seeks to understand the overall behavior of the model, providing insights into the general relationship between input features and predictions across the entire dataset. This helps answer questions such as \"Which features are most important for the model's predictions overall?\" or \"How does the model generally behave for different customer segments?\"<ul> <li>Example: Model feature importance scores, often provided by tree-based models or linear models, represent a form of global explainability. These scores indicate the relative influence of each feature on the model's predictions, averaged across the entire dataset.</li> </ul> </li> </ul> <p>Choosing between local and global explainability depends on the specific goals and questions you are trying to answer. In some scenarios, you might require both types of explanations to gain a complete understanding of your model's behavior and its predictions.</p>"},{"location":"7.%20Observability/5.%20Explainability.html#how-can-you-implement-explainability-with-shap-and-other-aiml-frameworks","title":"How can you implement Explainability with SHAP and other AI/ML frameworks?","text":"<p>The MLOps Python Package integrates SHAP (SHapley Additive exPlanations) to provide local explainability and Random Forest feature importances for global explainability. It includes a dedicated job, <code>ExplanationsJob</code>, that generates explanations for a registered model using a sample of data inputs.</p> <p>Here's how the package implements explainability:</p> <ol> <li> <p><code>explain_model()</code>: This method, defined within the <code>Model</code> abstract base class in <code>bikes.core.models</code>, is responsible for providing global explainability. For the <code>BaselineSklearnModel</code>, it leverages the <code>feature_importances_</code> attribute of the underlying RandomForestRegressor to generate a DataFrame outlining feature importances.</p> <pre><code>class BaselineSklearnModel(Model):\n    # ... (other methods)\n\n    @T.override\n    def explain_model(self) -&gt; schemas.FeatureImportances:\n    model = self.get_internal_model()\n    regressor = model.named_steps[\"regressor\"]\n    transformer = model.named_steps[\"transformer\"]\n    feature = transformer.get_feature_names_out()\n    feature_importances = schemas.FeatureImportances(\n        data={\n            \"feature\": feature,\n            \"importance\": regressor.feature_importances_,\n        }\n    )\n    return feature_importances\n</code></pre> </li> </ol> <p></p> <ol> <li> <p><code>explain_samples()</code>: This method, also within the <code>Model</code> base class, focuses on local explainability. For <code>BaselineSklearnModel</code>, it utilizes a SHAP TreeExplainer to compute SHAP values for a given set of input samples. These values are then stored in a DataFrame, enabling analysis of feature contributions to individual predictions.</p> <pre><code>class BaselineSklearnModel(Model):\n    # ... (other methods)\n\n    @T.override\n    def explain_samples(self, inputs: schemas.Inputs) -&gt; schemas.SHAPValues:\n    model = self.get_internal_model()\n    regressor = model.named_steps[\"regressor\"]\n    transformer = model.named_steps[\"transformer\"]\n    transformed = transformer.transform(X=inputs)\n    explainer = shap.TreeExplainer(model=regressor)\n    shap_values = schemas.SHAPValues(\n        data=explainer.shap_values(X=transformed),\n        columns=transformer.get_feature_names_out(),\n    )\n    return shap_values\n</code></pre> </li> </ol> <p></p> <ol> <li> <p><code>ExplanationsJob</code>: This job, defined in <code>bikes.jobs.explanations</code>, orchestrates the process of generating explanations. It loads a registered model from MLflow, reads a sample of data, computes both model-level and sample-level explanations, and then writes these explanations to Parquet files.</p> <pre><code>class ExplanationsJob(base.Job):\n    # ... (other attributes)\n\n    def run(self):\n        # ... (logic for loading model, reading data, and generating explanations)\n        # - models\n        logger.info(\"Explain model: {}\", model)\n        models_explanations = model.explain_model()\n        logger.debug(\"- Models explanations shape: {}\", models_explanations.shape)\n        # - samples\n        logger.info(\"Explain samples: {}\", len(inputs_samples))\n        samples_explanations = model.explain_samples(inputs=inputs_samples)\n        logger.debug(\"- Samples explanations shape: {}\", samples_explanations.shape)\n        # ... (write explanations to files)\n</code></pre> </li> </ol>"},{"location":"7.%20Observability/5.%20Explainability.html#integrating-explainability-into-your-workflow","title":"Integrating Explainability into Your Workflow","text":"<p>You can incorporate explainability into your MLOps workflow by executing the <code>ExplanationsJob</code> after model training or deployment. The resulting explanation files can then be used for various purposes:</p> <ul> <li>Model Understanding: Analyze feature importances to understand the key drivers of the model's predictions.</li> <li>Bias Detection: Identify features that disproportionately influence predictions for specific groups or categories.</li> <li>Debugging and Improvement: Gain insights into individual predictions that may be erroneous or unexpected, guiding model refinements.</li> <li>Communication with Stakeholders: Present explanations to business users or regulators to build trust and transparency in model decisions.</li> </ul>"},{"location":"7.%20Observability/5.%20Explainability.html#which-sectors-are-impacted-by-explainable-ai","title":"Which sectors are impacted by Explainable AI?","text":"<p>Explainable AI is especially relevant in sectors where the consequences of model decisions are significant or where regulatory compliance mandates transparency and accountability:</p> <ul> <li>Banking and Finance: For applications such as loan approvals, credit scoring, or fraud detection, explainability helps ensure fairness and avoid discriminatory practices.</li> <li>Insurance: Explainable AI is crucial for setting premiums, assessing risk, and handling claims in a way that is transparent and justifiable.</li> <li>Healthcare: For medical diagnosis, treatment recommendations, or patient risk assessment, explainable AI promotes trust and understanding of model-driven decisions.</li> <li>Law Enforcement: In scenarios where AI is used for predictive policing or criminal justice applications, explainability is crucial for ethical and legal considerations.</li> </ul>"},{"location":"7.%20Observability/5.%20Explainability.html#explainability-additional-resources","title":"Explainability Additional Resources","text":"<ul> <li>Explainability example from the MLOps Python Package</li> <li>SHAP (SHapley Additive exPlanations)</li> <li>LIME (Local Interpretable Model-agnostic Explanations)</li> <li>Interpretable Machine Learning: A Guide for Making Black Box Models Explainable</li> <li>Explainable AI (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI</li> </ul>"},{"location":"7.%20Observability/6.%20Infrastructure.html","title":"7.6. Infrastructure","text":""},{"location":"7.%20Observability/6.%20Infrastructure.html#what-is-infrastructure-monitoring-in-mlops","title":"What is Infrastructure Monitoring in MLOps?","text":"<p>Infrastructure Monitoring in MLOps involves the continuous tracking and analysis of hardware resources and their performance metrics during the execution of machine learning workloads. This encompasses a range of aspects including CPU utilization, memory consumption, network bandwidth usage, disk I/O, and GPU performance (if applicable). The data collected from infrastructure monitoring provides valuable insights into the efficiency and stability of your MLOps pipeline.</p>"},{"location":"7.%20Observability/6.%20Infrastructure.html#why-do-you-need-infrastructure-monitoring-for-aiml","title":"Why do you need Infrastructure Monitoring for AI/ML?","text":"<p>Monitoring the performance and utilization of your hardware infrastructure is crucial for several reasons:</p> <ul> <li>Resource Optimization: Identify bottlenecks and areas of over-utilization or under-utilization to optimize resource allocation, maximizing efficiency and minimizing costs.</li> <li>Performance Tuning: Pinpoint performance issues related to hardware limitations or resource contention to enhance model training and inference speeds.</li> <li>Capacity Planning: Forecast future infrastructure needs based on trends in resource consumption, allowing for proactive scaling to accommodate growing workloads.</li> <li>Cost Management: Track resource usage to accurately estimate and manage costs associated with running AI/ML workloads on cloud platforms or on-premise infrastructure.</li> </ul> <p>Effective infrastructure monitoring can lead to significant cost savings, improved model training and deployment times, and a more stable and predictable operational environment for your AI/ML solutions.</p>"},{"location":"7.%20Observability/6.%20Infrastructure.html#how-can-you-implement-infrastructure-monitoring-with-mlflow","title":"How can you implement Infrastructure Monitoring with MLflow?","text":"<p>The MLOps Python Package leverages the MLflow system metrics module to capture and log hardware performance metrics during the execution of jobs. These metrics provide insights into the resource utilization of the system during critical operations such as model training or inference, which is crucial for understanding the resource demands of different models and tasks.</p> <p>Here's how you can integrate infrastructure monitoring into your project using MLflow:</p> <ol> <li> <p>Enable System Metrics Logging: Configure the <code>MlflowService</code> within your job to enable system metrics logging. This is done by setting <code>log_system_metrics=True</code> in the <code>RunConfig</code>.</p> <pre><code>from bikes.io import services\n\nrun_config = services.MlflowService.RunConfig(\n    name=\"Training\", log_system_metrics=True\n)\n\ntraining_job = jobs.TrainingJob(\n    ...,\n    run_config=run_config,\n)\n</code></pre> </li> <li> <p>Execute Your Job: Run your job as usual, and MLflow will automatically capture system metrics during execution.</p> </li> <li> <p>View Metrics in the MLflow UI: Access the MLflow UI to see the logged system metrics. These metrics provide information about CPU utilization, memory usage, and other resource-related details, allowing you to assess the hardware demands of your training or inference tasks.</p> </li> </ol> <p></p>"},{"location":"7.%20Observability/6.%20Infrastructure.html#customizing-infrastructure-monitoring-with-mlflow","title":"Customizing Infrastructure Monitoring with MLflow","text":"<p>MLflow's system metrics module offers customization options, allowing you to:</p> <ul> <li>Adjust the Collection Interval: Control the frequency at which metrics are sampled.</li> <li>Select Specific Metrics: Choose which metrics to track, depending on your analysis needs.</li> <li>Integrate with Other Tools: Export the logged metrics to external monitoring systems for more comprehensive analysis and visualization.</li> </ul>"},{"location":"7.%20Observability/6.%20Infrastructure.html#what-are-alternative-solutions-for-infrastructure-monitoring","title":"What are Alternative Solutions for Infrastructure Monitoring?","text":"<p>Beyond MLflow, specialized monitoring platforms offer comprehensive solutions tailored for production infrastructure setups:</p> <ul> <li>Datadog: A popular cloud-based monitoring platform that offers a wide range of integrations and visualization tools, including those for AI/ML applications.</li> <li>Prometheus: An open-source monitoring system that excels at collecting and storing time series data, ideal for tracking resource utilization metrics over time.</li> <li>Grafana: A visualization and dashboarding tool that can integrate with multiple data sources, including Prometheus, to create informative dashboards for monitoring your MLOps infrastructure.</li> </ul>"},{"location":"7.%20Observability/6.%20Infrastructure.html#infrastructure-additional-resources","title":"Infrastructure additional resources","text":"<ul> <li>Example from the MLOps Python Package</li> <li>MLflow System Metrics</li> <li>Datadog</li> <li>Prometheus</li> <li>Grafana</li> </ul>"}]}